{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Baseline model Classification Densenet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPK6myUjXe2c4z/6ogIxsFl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyoY9nCAkqgo",
        "outputId": "98776c66-948d-4c3e-af02-5a2bda62c270"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M84SeDz1kwZw",
        "outputId": "8efcfc5c-96d9-4302-b578-f90eb0c602e2"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eon5HN5UkzR-",
        "outputId": "a0ca63ac-9859-4e12-c339-959e957da999"
      },
      "source": [
        "cd /content/gdrive/My Drive/ZFDataset/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/ZFDataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TETUqP6nlK5O"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import pathlib\n",
        "from io import BytesIO\n",
        "import base64\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "from matplotlib import cm\n",
        "\n",
        "from scipy import ndimage as ndi\n",
        "from skimage import io, filters, measure, morphology, img_as_ubyte\n",
        "import pandas as pd\n",
        "from sklearn import decomposition, manifold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzJsUW-elDe_"
      },
      "source": [
        "# syllable_df_Nest_Total = pd.read_pickle('/content/gdrive/My Drive/ZFDataset/Nest1_Densenet121.pkl')\n",
        "# syllable_df_Nest_Total1 = pd.read_pickle('/content/gdrive/My Drive/ZFDataset/Nest4_Densenet121.pkl')\n",
        "# syllable_df_Nest_Total2 = pd.read_pickle('/content/gdrive/My Drive/ZFDataset/Nest5_Densenet121.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEHthnpSGRGA"
      },
      "source": [
        "# syllable_df_Nest_Total1 = pd.read_pickle('/content/gdrive/My Drive/ZFDataset/Nest1_Densenet121.pkl')\n",
        "# syllable_df_Nest_Total1['Nest'] = \"Nest1\"\n",
        "syllable_df_Nest_Total2 = pd.read_pickle('/content/gdrive/My Drive/ZFDataset/Nest2_Densenet121.pkl')\n",
        "syllable_df_Nest_Total2['Nest'] = \"Nest2\"\n",
        "syllable_df_Nest_Total3 = pd.read_pickle('/content/gdrive/My Drive/ZFDataset/Nest3_Densenet121.pkl')\n",
        "syllable_df_Nest_Total3['Nest'] = \"Nest3\"\n",
        "syllable_df_Nest_Total4 = pd.read_pickle('/content/gdrive/My Drive/ZFDataset/Nest4_Densenet121.pkl')\n",
        "syllable_df_Nest_Total4['Nest'] = \"Nest4\"\n",
        "syllable_df_Nest_Total5 = pd.read_pickle('/content/gdrive/My Drive/ZFDataset/Nest5_Densenet121.pkl')\n",
        "syllable_df_Nest_Total5['Nest'] = \"Nest5\"\n",
        "syllable_df_Nest_Total6 = pd.read_pickle('/content/gdrive/My Drive/ZFDataset/Nest6_Densenet121.pkl')\n",
        "syllable_df_Nest_Total6['Nest'] = \"Nest6\"\n",
        "syllable_df_Nest_Total7 = pd.read_pickle('/content/gdrive/My Drive/ZFDataset/Nest7_Densenet121.pkl')\n",
        "syllable_df_Nest_Total7['Nest'] = \"Nest7\"\n",
        "syllable_df_Nest_Total8 = pd.read_pickle('/content/gdrive/My Drive/ZFDataset/Nest8_Densenet121.pkl')\n",
        "syllable_df_Nest_Total8['Nest'] = \"Nest8\"\n",
        "syllable_df_Nest_Total9 = pd.read_pickle('/content/gdrive/My Drive/ZFDataset/Nest9_Densenet121.pkl')\n",
        "syllable_df_Nest_Total9['Nest'] = \"Nest9\"\n",
        "syllable_df_Nest_Total10 = pd.read_pickle('/content/gdrive/My Drive/ZFDataset/Nest10_Densenet121.pkl')\n",
        "syllable_df_Nest_Total10['Nest'] = \"Nest10\"\n",
        "syllable_df_Nest_Total11 = pd.read_pickle('/content/gdrive/My Drive/ZFDataset/Nest11_Densenet121.pkl')\n",
        "syllable_df_Nest_Total11['Nest'] = \"Nest11\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E0vlxcPbPq1"
      },
      "source": [
        "syllable_df_Nest_Total = pd.concat([syllable_df_Nest_Total2, syllable_df_Nest_Total3, syllable_df_Nest_Total4, syllable_df_Nest_Total5,\n",
        "                                    syllable_df_Nest_Total6, syllable_df_Nest_Total7, syllable_df_Nest_Total8, syllable_df_Nest_Total9, syllable_df_Nest_Total10, syllable_df_Nest_Total11], ignore_index=True)\n",
        "del syllable_df_Nest_Total11, syllable_df_Nest_Total2, syllable_df_Nest_Total3, syllable_df_Nest_Total4, syllable_df_Nest_Total5\n",
        "del syllable_df_Nest_Total6, syllable_df_Nest_Total7, syllable_df_Nest_Total8, syllable_df_Nest_Total9, syllable_df_Nest_Total10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB5e3WS_lZRr"
      },
      "source": [
        "syllable_df_Nest_Total.drop(columns=['audio', 'spectrogram'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "gj7z6OY0ldFT",
        "outputId": "885a8ef5-1abe-4b4c-ec17-a7810b7d6d59"
      },
      "source": [
        "syllable_df_Nest_Total"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>labels</th>\n",
              "      <th>indv</th>\n",
              "      <th>indvi</th>\n",
              "      <th>key</th>\n",
              "      <th>rate</th>\n",
              "      <th>labels_indv</th>\n",
              "      <th>densenet121_features</th>\n",
              "      <th>Nest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.050414</td>\n",
              "      <td>0.097987</td>\n",
              "      <td>i</td>\n",
              "      <td>cxyc</td>\n",
              "      <td>1</td>\n",
              "      <td>cxyc_0000</td>\n",
              "      <td>44100</td>\n",
              "      <td>cxyc_i</td>\n",
              "      <td>[0.00028999924, 0.0064279474, 0.0022861622, 0....</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.508713</td>\n",
              "      <td>0.570935</td>\n",
              "      <td>i</td>\n",
              "      <td>cxyc</td>\n",
              "      <td>2</td>\n",
              "      <td>cxyc_0000</td>\n",
              "      <td>44100</td>\n",
              "      <td>cxyc_i</td>\n",
              "      <td>[0.0002864063, 0.0052835844, 0.002387906, 0.00...</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.644405</td>\n",
              "      <td>0.703089</td>\n",
              "      <td>i</td>\n",
              "      <td>cxyc</td>\n",
              "      <td>3</td>\n",
              "      <td>cxyc_0000</td>\n",
              "      <td>44100</td>\n",
              "      <td>cxyc_i</td>\n",
              "      <td>[0.0002815476, 0.0063230395, 0.002232295, 0.00...</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.810504</td>\n",
              "      <td>0.870799</td>\n",
              "      <td>i</td>\n",
              "      <td>cxyc</td>\n",
              "      <td>4</td>\n",
              "      <td>cxyc_0000</td>\n",
              "      <td>44100</td>\n",
              "      <td>cxyc_i</td>\n",
              "      <td>[0.00028404495, 0.004314999, 0.0023957393, 0.0...</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.938940</td>\n",
              "      <td>1.000845</td>\n",
              "      <td>i</td>\n",
              "      <td>cxyc</td>\n",
              "      <td>5</td>\n",
              "      <td>cxyc_0000</td>\n",
              "      <td>44100</td>\n",
              "      <td>cxyc_i</td>\n",
              "      <td>[0.00026726056, 0.0051532886, 0.0022074692, 0....</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23050</th>\n",
              "      <td>2.372736</td>\n",
              "      <td>2.450990</td>\n",
              "      <td>f</td>\n",
              "      <td>vstd</td>\n",
              "      <td>16</td>\n",
              "      <td>vstd_0004</td>\n",
              "      <td>44100</td>\n",
              "      <td>vstd_f</td>\n",
              "      <td>[0.00024792607, 0.005248108, 0.0019565802, 0.0...</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23051</th>\n",
              "      <td>2.598813</td>\n",
              "      <td>2.651443</td>\n",
              "      <td>0</td>\n",
              "      <td>vstd</td>\n",
              "      <td>17</td>\n",
              "      <td>vstd_0004</td>\n",
              "      <td>44100</td>\n",
              "      <td>vstd_0</td>\n",
              "      <td>[0.00027455005, 0.0042480123, 0.0022447598, 0....</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23052</th>\n",
              "      <td>3.907656</td>\n",
              "      <td>3.982781</td>\n",
              "      <td>0</td>\n",
              "      <td>vstd</td>\n",
              "      <td>18</td>\n",
              "      <td>vstd_0004</td>\n",
              "      <td>44100</td>\n",
              "      <td>vstd_0</td>\n",
              "      <td>[0.0002736851, 0.0052517042, 0.001988696, 0.00...</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23053</th>\n",
              "      <td>4.090015</td>\n",
              "      <td>4.216341</td>\n",
              "      <td>0</td>\n",
              "      <td>vstd</td>\n",
              "      <td>19</td>\n",
              "      <td>vstd_0004</td>\n",
              "      <td>44100</td>\n",
              "      <td>vstd_0</td>\n",
              "      <td>[0.00030207614, 0.0031468559, 0.0016397715, 0....</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23054</th>\n",
              "      <td>5.081715</td>\n",
              "      <td>5.187112</td>\n",
              "      <td>0</td>\n",
              "      <td>vstd</td>\n",
              "      <td>20</td>\n",
              "      <td>vstd_0004</td>\n",
              "      <td>44100</td>\n",
              "      <td>vstd_0</td>\n",
              "      <td>[0.00032343273, 0.003422297, 0.0019693254, 0.0...</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23055 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       start_time  ...    Nest\n",
              "0        0.050414  ...   Nest2\n",
              "1        0.508713  ...   Nest2\n",
              "2        0.644405  ...   Nest2\n",
              "3        0.810504  ...   Nest2\n",
              "4        0.938940  ...   Nest2\n",
              "...           ...  ...     ...\n",
              "23050    2.372736  ...  Nest11\n",
              "23051    2.598813  ...  Nest11\n",
              "23052    3.907656  ...  Nest11\n",
              "23053    4.090015  ...  Nest11\n",
              "23054    5.081715  ...  Nest11\n",
              "\n",
              "[23055 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pP_Ayg3bUIc"
      },
      "source": [
        "Tutor_dataset = syllable_df_Nest_Total[syllable_df_Nest_Total['indv'].isin([\"ivoj\", \"nzen\", \"xsup\", \"gthh\", \"ttog\", \"isab\", \"ixea\", \"ihza\", \"zegf\", \"sjew\", \"cgby\"])].reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUUpJIqObXEk"
      },
      "source": [
        "Pupil_dataset = syllable_df_Nest_Total[syllable_df_Nest_Total['indv'].isin([\"hphi\", \"cyea\", \"phpd\", \"cxyc\", \"qfod\", \"nsrn\", \"khxv\", \"oogw\", \"kcos\", \"tbfk\", \"kccr\", \"bbyj\", \"onsu\", \"vusu\", \"kfgj\", \"inji\", \"hsew\", \"sdhp\", \"vstd\"])].reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "p4EdpblylHOI",
        "outputId": "93a888e8-e013-4d11-cfd8-364caa4c5f4f"
      },
      "source": [
        "Tutor_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>labels</th>\n",
              "      <th>indv</th>\n",
              "      <th>indvi</th>\n",
              "      <th>key</th>\n",
              "      <th>rate</th>\n",
              "      <th>labels_indv</th>\n",
              "      <th>densenet121_features</th>\n",
              "      <th>Nest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.141369</td>\n",
              "      <td>0.163840</td>\n",
              "      <td>i</td>\n",
              "      <td>nzen</td>\n",
              "      <td>1</td>\n",
              "      <td>nzen_0000</td>\n",
              "      <td>44100</td>\n",
              "      <td>nzen_i</td>\n",
              "      <td>[0.000281047, 0.006896307, 0.0021862306, 0.003...</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.226426</td>\n",
              "      <td>0.290575</td>\n",
              "      <td>i</td>\n",
              "      <td>nzen</td>\n",
              "      <td>2</td>\n",
              "      <td>nzen_0000</td>\n",
              "      <td>44100</td>\n",
              "      <td>nzen_i</td>\n",
              "      <td>[0.0003058371, 0.005542181, 0.0023643032, 0.00...</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.388081</td>\n",
              "      <td>0.453342</td>\n",
              "      <td>i</td>\n",
              "      <td>nzen</td>\n",
              "      <td>3</td>\n",
              "      <td>nzen_0000</td>\n",
              "      <td>44100</td>\n",
              "      <td>nzen_i</td>\n",
              "      <td>[0.0002870175, 0.0060381475, 0.0023333135, 0.0...</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.487174</td>\n",
              "      <td>0.550757</td>\n",
              "      <td>j</td>\n",
              "      <td>nzen</td>\n",
              "      <td>4</td>\n",
              "      <td>nzen_0000</td>\n",
              "      <td>44100</td>\n",
              "      <td>nzen_j</td>\n",
              "      <td>[0.0003228713, 0.0052634007, 0.0023634112, 0.0...</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.579305</td>\n",
              "      <td>0.642185</td>\n",
              "      <td>a</td>\n",
              "      <td>nzen</td>\n",
              "      <td>5</td>\n",
              "      <td>nzen_0000</td>\n",
              "      <td>44100</td>\n",
              "      <td>nzen_a</td>\n",
              "      <td>[0.00028934886, 0.0054234248, 0.00228799, 0.00...</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7176</th>\n",
              "      <td>10.182711</td>\n",
              "      <td>10.273732</td>\n",
              "      <td>0</td>\n",
              "      <td>cgby</td>\n",
              "      <td>39</td>\n",
              "      <td>cgby_0032</td>\n",
              "      <td>44100</td>\n",
              "      <td>cgby_0</td>\n",
              "      <td>[0.0003101198, 0.0023758058, 0.00217167, 0.003...</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7177</th>\n",
              "      <td>10.322530</td>\n",
              "      <td>10.421079</td>\n",
              "      <td>0</td>\n",
              "      <td>cgby</td>\n",
              "      <td>40</td>\n",
              "      <td>cgby_0032</td>\n",
              "      <td>44100</td>\n",
              "      <td>cgby_0</td>\n",
              "      <td>[0.00037552853, 0.004450283, 0.0019465615, 0.0...</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7178</th>\n",
              "      <td>10.451782</td>\n",
              "      <td>10.581827</td>\n",
              "      <td>0</td>\n",
              "      <td>cgby</td>\n",
              "      <td>41</td>\n",
              "      <td>cgby_0032</td>\n",
              "      <td>44100</td>\n",
              "      <td>cgby_0</td>\n",
              "      <td>[0.00045851385, 0.0021333802, 0.0020485146, 0....</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7179</th>\n",
              "      <td>10.640036</td>\n",
              "      <td>10.822507</td>\n",
              "      <td>0</td>\n",
              "      <td>cgby</td>\n",
              "      <td>42</td>\n",
              "      <td>cgby_0032</td>\n",
              "      <td>44100</td>\n",
              "      <td>cgby_0</td>\n",
              "      <td>[0.00029902745, 0.0032967748, 0.0025172085, 0....</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7180</th>\n",
              "      <td>10.846702</td>\n",
              "      <td>10.935183</td>\n",
              "      <td>0</td>\n",
              "      <td>cgby</td>\n",
              "      <td>43</td>\n",
              "      <td>cgby_0032</td>\n",
              "      <td>44100</td>\n",
              "      <td>cgby_0</td>\n",
              "      <td>[0.0002637265, 0.0070170346, 0.0019420997, 0.0...</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7181 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      start_time  ...    Nest\n",
              "0       0.141369  ...   Nest2\n",
              "1       0.226426  ...   Nest2\n",
              "2       0.388081  ...   Nest2\n",
              "3       0.487174  ...   Nest2\n",
              "4       0.579305  ...   Nest2\n",
              "...          ...  ...     ...\n",
              "7176   10.182711  ...  Nest11\n",
              "7177   10.322530  ...  Nest11\n",
              "7178   10.451782  ...  Nest11\n",
              "7179   10.640036  ...  Nest11\n",
              "7180   10.846702  ...  Nest11\n",
              "\n",
              "[7181 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "pzDmWSTsluPy",
        "outputId": "e7c148b3-8951-4f6d-979b-13b4ce47d3d0"
      },
      "source": [
        "Pupil_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>labels</th>\n",
              "      <th>indv</th>\n",
              "      <th>indvi</th>\n",
              "      <th>key</th>\n",
              "      <th>rate</th>\n",
              "      <th>labels_indv</th>\n",
              "      <th>densenet121_features</th>\n",
              "      <th>Nest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.050414</td>\n",
              "      <td>0.097987</td>\n",
              "      <td>i</td>\n",
              "      <td>cxyc</td>\n",
              "      <td>1</td>\n",
              "      <td>cxyc_0000</td>\n",
              "      <td>44100</td>\n",
              "      <td>cxyc_i</td>\n",
              "      <td>[0.00028999924, 0.0064279474, 0.0022861622, 0....</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.508713</td>\n",
              "      <td>0.570935</td>\n",
              "      <td>i</td>\n",
              "      <td>cxyc</td>\n",
              "      <td>2</td>\n",
              "      <td>cxyc_0000</td>\n",
              "      <td>44100</td>\n",
              "      <td>cxyc_i</td>\n",
              "      <td>[0.0002864063, 0.0052835844, 0.002387906, 0.00...</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.644405</td>\n",
              "      <td>0.703089</td>\n",
              "      <td>i</td>\n",
              "      <td>cxyc</td>\n",
              "      <td>3</td>\n",
              "      <td>cxyc_0000</td>\n",
              "      <td>44100</td>\n",
              "      <td>cxyc_i</td>\n",
              "      <td>[0.0002815476, 0.0063230395, 0.002232295, 0.00...</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.810504</td>\n",
              "      <td>0.870799</td>\n",
              "      <td>i</td>\n",
              "      <td>cxyc</td>\n",
              "      <td>4</td>\n",
              "      <td>cxyc_0000</td>\n",
              "      <td>44100</td>\n",
              "      <td>cxyc_i</td>\n",
              "      <td>[0.00028404495, 0.004314999, 0.0023957393, 0.0...</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.938940</td>\n",
              "      <td>1.000845</td>\n",
              "      <td>i</td>\n",
              "      <td>cxyc</td>\n",
              "      <td>5</td>\n",
              "      <td>cxyc_0000</td>\n",
              "      <td>44100</td>\n",
              "      <td>cxyc_i</td>\n",
              "      <td>[0.00026726056, 0.0051532886, 0.0022074692, 0....</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15869</th>\n",
              "      <td>2.372736</td>\n",
              "      <td>2.450990</td>\n",
              "      <td>f</td>\n",
              "      <td>vstd</td>\n",
              "      <td>16</td>\n",
              "      <td>vstd_0004</td>\n",
              "      <td>44100</td>\n",
              "      <td>vstd_f</td>\n",
              "      <td>[0.00024792607, 0.005248108, 0.0019565802, 0.0...</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15870</th>\n",
              "      <td>2.598813</td>\n",
              "      <td>2.651443</td>\n",
              "      <td>0</td>\n",
              "      <td>vstd</td>\n",
              "      <td>17</td>\n",
              "      <td>vstd_0004</td>\n",
              "      <td>44100</td>\n",
              "      <td>vstd_0</td>\n",
              "      <td>[0.00027455005, 0.0042480123, 0.0022447598, 0....</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15871</th>\n",
              "      <td>3.907656</td>\n",
              "      <td>3.982781</td>\n",
              "      <td>0</td>\n",
              "      <td>vstd</td>\n",
              "      <td>18</td>\n",
              "      <td>vstd_0004</td>\n",
              "      <td>44100</td>\n",
              "      <td>vstd_0</td>\n",
              "      <td>[0.0002736851, 0.0052517042, 0.001988696, 0.00...</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15872</th>\n",
              "      <td>4.090015</td>\n",
              "      <td>4.216341</td>\n",
              "      <td>0</td>\n",
              "      <td>vstd</td>\n",
              "      <td>19</td>\n",
              "      <td>vstd_0004</td>\n",
              "      <td>44100</td>\n",
              "      <td>vstd_0</td>\n",
              "      <td>[0.00030207614, 0.0031468559, 0.0016397715, 0....</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15873</th>\n",
              "      <td>5.081715</td>\n",
              "      <td>5.187112</td>\n",
              "      <td>0</td>\n",
              "      <td>vstd</td>\n",
              "      <td>20</td>\n",
              "      <td>vstd_0004</td>\n",
              "      <td>44100</td>\n",
              "      <td>vstd_0</td>\n",
              "      <td>[0.00032343273, 0.003422297, 0.0019693254, 0.0...</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15874 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       start_time  ...    Nest\n",
              "0        0.050414  ...   Nest2\n",
              "1        0.508713  ...   Nest2\n",
              "2        0.644405  ...   Nest2\n",
              "3        0.810504  ...   Nest2\n",
              "4        0.938940  ...   Nest2\n",
              "...           ...  ...     ...\n",
              "15869    2.372736  ...  Nest11\n",
              "15870    2.598813  ...  Nest11\n",
              "15871    3.907656  ...  Nest11\n",
              "15872    4.090015  ...  Nest11\n",
              "15873    5.081715  ...  Nest11\n",
              "\n",
              "[15874 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI52AoQelNfO"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import string\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJa9mNfNXXWE"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import column_or_1d\n",
        "from sklearn.model_selection import KFold\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSlDdQXIB5K6"
      },
      "source": [
        "def data_splitting_based_on_sample_size(syllable_df_Nest_Total):\n",
        "  Label_Total = []\n",
        "  Nest_Total = []\n",
        "\n",
        "  for key in syllable_df_Nest_Total.key.unique():\n",
        "    Label_Total.append(syllable_df_Nest_Total.loc[syllable_df_Nest_Total['key']==key]['indv'].values[0])\n",
        "    Nest_Total.append(syllable_df_Nest_Total.loc[syllable_df_Nest_Total['key']==key]['Nest'].values[0])\n",
        "\n",
        "  le = LabelEncoder()\n",
        "  le.fit(Nest_Total)\n",
        "\n",
        "  targets = le.transform(Nest_Total)\n",
        "\n",
        "  encoded_targets = pd.DataFrame({'targets' : targets,\n",
        "        'indvi' : Label_Total,\n",
        "        'key' : syllable_df_Nest_Total.key.unique(),\n",
        "        'Nest' : Nest_Total})\n",
        "\n",
        "  # train_indices, test_indices = train_test_split(np.arange(targets.shape[0]), train_size=train_test_split_ratio, stratify=targets)\n",
        "\n",
        "  # train_keys = encoded_targets.loc[train_indices]['key'].to_list()\n",
        "\n",
        "  # test_keys = encoded_targets.loc[test_indices]['key'].to_list()\n",
        "\n",
        "  return encoded_targets, le"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSnf4eQFCDnw"
      },
      "source": [
        "encoded_targets, le = data_splitting_based_on_sample_size(Tutor_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "KjlJN2edDBbu",
        "outputId": "3815a3d8-8493-4603-d28d-d20fd9d53622"
      },
      "source": [
        "encoded_targets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>targets</th>\n",
              "      <th>indvi</th>\n",
              "      <th>key</th>\n",
              "      <th>Nest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>nzen</td>\n",
              "      <td>nzen_0000</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>nzen</td>\n",
              "      <td>nzen_0001</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>nzen</td>\n",
              "      <td>nzen_0002</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>nzen</td>\n",
              "      <td>nzen_0003</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>nzen</td>\n",
              "      <td>nzen_0004</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320</th>\n",
              "      <td>1</td>\n",
              "      <td>cgby</td>\n",
              "      <td>cgby_0026</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>1</td>\n",
              "      <td>cgby</td>\n",
              "      <td>cgby_0048</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>1</td>\n",
              "      <td>cgby</td>\n",
              "      <td>cgby_0022</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323</th>\n",
              "      <td>1</td>\n",
              "      <td>cgby</td>\n",
              "      <td>cgby_0012</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>324</th>\n",
              "      <td>1</td>\n",
              "      <td>cgby</td>\n",
              "      <td>cgby_0032</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>325 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     targets indvi        key    Nest\n",
              "0          2  nzen  nzen_0000   Nest2\n",
              "1          2  nzen  nzen_0001   Nest2\n",
              "2          2  nzen  nzen_0002   Nest2\n",
              "3          2  nzen  nzen_0003   Nest2\n",
              "4          2  nzen  nzen_0004   Nest2\n",
              "..       ...   ...        ...     ...\n",
              "320        1  cgby  cgby_0026  Nest11\n",
              "321        1  cgby  cgby_0048  Nest11\n",
              "322        1  cgby  cgby_0022  Nest11\n",
              "323        1  cgby  cgby_0012  Nest11\n",
              "324        1  cgby  cgby_0032  Nest11\n",
              "\n",
              "[325 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVMF0z1mEa1a"
      },
      "source": [
        "encoded_targets = encoded_targets.sample(frac=1, random_state=2021).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "ybkRwAowEpI1",
        "outputId": "e849b718-f8b4-4dd5-dafe-16fd995400bb"
      },
      "source": [
        "encoded_targets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>targets</th>\n",
              "      <th>indvi</th>\n",
              "      <th>key</th>\n",
              "      <th>Nest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>ixea</td>\n",
              "      <td>ixea_0001</td>\n",
              "      <td>Nest7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "      <td>isab</td>\n",
              "      <td>isab_0028</td>\n",
              "      <td>Nest6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>gthh</td>\n",
              "      <td>gthh_0030</td>\n",
              "      <td>Nest4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>isab</td>\n",
              "      <td>isab_0004</td>\n",
              "      <td>Nest6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>sjew</td>\n",
              "      <td>sjew_0007</td>\n",
              "      <td>Nest10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320</th>\n",
              "      <td>3</td>\n",
              "      <td>xsup</td>\n",
              "      <td>xsup_0027</td>\n",
              "      <td>Nest3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>5</td>\n",
              "      <td>ttog</td>\n",
              "      <td>ttog_0023</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>6</td>\n",
              "      <td>isab</td>\n",
              "      <td>isab_0006</td>\n",
              "      <td>Nest6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323</th>\n",
              "      <td>4</td>\n",
              "      <td>gthh</td>\n",
              "      <td>gthh_0033</td>\n",
              "      <td>Nest4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>324</th>\n",
              "      <td>5</td>\n",
              "      <td>ttog</td>\n",
              "      <td>ttog_0031</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>325 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     targets indvi        key    Nest\n",
              "0          7  ixea  ixea_0001   Nest7\n",
              "1          6  isab  isab_0028   Nest6\n",
              "2          4  gthh  gthh_0030   Nest4\n",
              "3          6  isab  isab_0004   Nest6\n",
              "4          0  sjew  sjew_0007  Nest10\n",
              "..       ...   ...        ...     ...\n",
              "320        3  xsup  xsup_0027   Nest3\n",
              "321        5  ttog  ttog_0023   Nest5\n",
              "322        6  isab  isab_0006   Nest6\n",
              "323        4  gthh  gthh_0033   Nest4\n",
              "324        5  ttog  ttog_0031   Nest5\n",
              "\n",
              "[325 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxE4d95tFBaa"
      },
      "source": [
        "# sequence_length = current_songfile['indvi'].values[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b7-lG1eE6kN"
      },
      "source": [
        "# for i in range(0, sequence_length):\n",
        "#   print(torch.tensor(current_songfile['densenet121_features'].values[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeVCSj0lZ5Io",
        "outputId": "3e8e9cc4-d3ba-44af-fac3-3d790d553ad6"
      },
      "source": [
        "# X_train = []; X_test =[];\n",
        "# y_train = []; y_test =[];\n",
        "X_total = []; y_total = [];\n",
        "\n",
        "for key in encoded_targets['key'].unique():# [:10]:\n",
        "    print(key)\n",
        "    current_songfile = syllable_df_Nest_Total.loc[syllable_df_Nest_Total['key']==key]\n",
        "    sequence_length = current_songfile['indvi'].values[-1]\n",
        "    for i in range(0, sequence_length):\n",
        "      X_total.append(torch.tensor(current_songfile['densenet121_features'].values[i]))\n",
        "      y_total.append(current_songfile['Nest'].values[i])\n",
        "    # if key in train_keys:\n",
        "    #   for i in range(0, sequence_length):\n",
        "    #     X_train.append(torch.tensor(current_songfile['densenet121_features'].values[i]))\n",
        "    #     y_train.append(current_songfile['indv'].values[i])\n",
        "    # elif key in test_keys:\n",
        "    #   for i in range(0, sequence_length):\n",
        "    #     X_test.append(torch.tensor(current_songfile['densenet121_features'].values[i]))\n",
        "    #     y_test.append(current_songfile['indv'].values[i])\n",
        "    # else:\n",
        "    #   print(key, \"Not Found\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ixea_0001\n",
            "isab_0028\n",
            "gthh_0030\n",
            "isab_0004\n",
            "sjew_0007\n",
            "ixea_0008\n",
            "gthh_0024\n",
            "ttog_0016\n",
            "zegf_0000\n",
            "cgby_0021\n",
            "nzen_0001\n",
            "ixea_0025\n",
            "sjew_0000\n",
            "cgby_0002\n",
            "isab_0021\n",
            "gthh_0010\n",
            "nzen_0006\n",
            "ixea_0018\n",
            "gthh_0022\n",
            "xsup_0019\n",
            "xsup_0011\n",
            "xsup_0029\n",
            "ttog_0001\n",
            "ttog_0032\n",
            "isab_0020\n",
            "ttog_0027\n",
            "ihza_0021\n",
            "cgby_0027\n",
            "ttog_0033\n",
            "cgby_0009\n",
            "isab_0018\n",
            "xsup_0031\n",
            "sjew_0019\n",
            "nzen_0009\n",
            "nzen_0002\n",
            "isab_0013\n",
            "isab_0012\n",
            "ttog_0030\n",
            "ttog_0026\n",
            "cgby_0033\n",
            "isab_0011\n",
            "ttog_0000\n",
            "ihza_0006\n",
            "ixea_0024\n",
            "xsup_0005\n",
            "cgby_0014\n",
            "xsup_0024\n",
            "nzen_0015\n",
            "cgby_0031\n",
            "cgby_0025\n",
            "xsup_0025\n",
            "ihza_0005\n",
            "cgby_0007\n",
            "ixea_0034\n",
            "ixea_0002\n",
            "xsup_0020\n",
            "cgby_0019\n",
            "zegf_0005\n",
            "gthh_0007\n",
            "zegf_0007\n",
            "ttog_0010\n",
            "nzen_0008\n",
            "nzen_0014\n",
            "cgby_0010\n",
            "ihza_0017\n",
            "gthh_0028\n",
            "gthh_0032\n",
            "gthh_0015\n",
            "ttog_0005\n",
            "isab_0008\n",
            "ixea_0031\n",
            "xsup_0009\n",
            "ihza_0026\n",
            "gthh_0013\n",
            "sjew_0023\n",
            "cgby_0046\n",
            "ixea_0011\n",
            "zegf_0015\n",
            "isab_0003\n",
            "ihza_0013\n",
            "gthh_0002\n",
            "ttog_0036\n",
            "ixea_0014\n",
            "zegf_0024\n",
            "ixea_0000\n",
            "gthh_0006\n",
            "isab_0009\n",
            "ttog_0011\n",
            "nzen_0013\n",
            "cgby_0012\n",
            "ihza_0016\n",
            "cgby_0015\n",
            "ttog_0002\n",
            "ixea_0028\n",
            "isab_0019\n",
            "gthh_0016\n",
            "zegf_0006\n",
            "gthh_0008\n",
            "nzen_0003\n",
            "isab_0017\n",
            "cgby_0026\n",
            "cgby_0003\n",
            "xsup_0033\n",
            "cgby_0022\n",
            "gthh_0003\n",
            "zegf_0027\n",
            "xsup_0018\n",
            "xsup_0021\n",
            "ttog_0009\n",
            "ixea_0029\n",
            "nzen_0000\n",
            "isab_0024\n",
            "zegf_0021\n",
            "cgby_0041\n",
            "zegf_0017\n",
            "ttog_0018\n",
            "zegf_0022\n",
            "xsup_0015\n",
            "gthh_0005\n",
            "xsup_0001\n",
            "cgby_0008\n",
            "ttog_0021\n",
            "gthh_0012\n",
            "gthh_0031\n",
            "cgby_0028\n",
            "ixea_0030\n",
            "xsup_0007\n",
            "ttog_0019\n",
            "zegf_0010\n",
            "nzen_0010\n",
            "sjew_0021\n",
            "ixea_0007\n",
            "cgby_0011\n",
            "ihza_0018\n",
            "ixea_0003\n",
            "cgby_0047\n",
            "xsup_0026\n",
            "gthh_0011\n",
            "xsup_0006\n",
            "gthh_0026\n",
            "ihza_0014\n",
            "cgby_0048\n",
            "zegf_0030\n",
            "sjew_0022\n",
            "ttog_0029\n",
            "cgby_0038\n",
            "ttog_0012\n",
            "ihza_0022\n",
            "ttog_0034\n",
            "xsup_0034\n",
            "sjew_0011\n",
            "sjew_0013\n",
            "sjew_0024\n",
            "ihza_0011\n",
            "cgby_0001\n",
            "sjew_0012\n",
            "xsup_0036\n",
            "nzen_0016\n",
            "cgby_0013\n",
            "xsup_0017\n",
            "ttog_0006\n",
            "ttog_0015\n",
            "nzen_0004\n",
            "cgby_0040\n",
            "ixea_0021\n",
            "gthh_0014\n",
            "xsup_0035\n",
            "ihza_0025\n",
            "ihza_0009\n",
            "xsup_0028\n",
            "isab_0023\n",
            "zegf_0004\n",
            "isab_0001\n",
            "ixea_0012\n",
            "ihza_0023\n",
            "sjew_0006\n",
            "ihza_0001\n",
            "cgby_0044\n",
            "cgby_0036\n",
            "ixea_0015\n",
            "isab_0016\n",
            "xsup_0038\n",
            "xsup_0014\n",
            "zegf_0032\n",
            "zegf_0025\n",
            "cgby_0000\n",
            "ihza_0007\n",
            "xsup_0037\n",
            "sjew_0005\n",
            "sjew_0015\n",
            "zegf_0019\n",
            "cgby_0039\n",
            "gthh_0023\n",
            "ttog_0014\n",
            "ixea_0019\n",
            "ttog_0024\n",
            "ihza_0029\n",
            "gthh_0021\n",
            "ixea_0033\n",
            "zegf_0001\n",
            "isab_0025\n",
            "isab_0015\n",
            "ihza_0000\n",
            "xsup_0013\n",
            "ihza_0027\n",
            "cgby_0030\n",
            "ihza_0020\n",
            "ihza_0024\n",
            "ttog_0020\n",
            "gthh_0004\n",
            "zegf_0031\n",
            "ihza_0002\n",
            "nzen_0012\n",
            "ixea_0013\n",
            "zegf_0029\n",
            "gthh_0029\n",
            "sjew_0003\n",
            "cgby_0005\n",
            "ttog_0025\n",
            "sjew_0002\n",
            "gthh_0001\n",
            "ixea_0006\n",
            "ixea_0020\n",
            "isab_0026\n",
            "ihza_0010\n",
            "cgby_0037\n",
            "zegf_0028\n",
            "sjew_0026\n",
            "isab_0007\n",
            "isab_0000\n",
            "sjew_0004\n",
            "sjew_0001\n",
            "zegf_0008\n",
            "gthh_0020\n",
            "cgby_0024\n",
            "cgby_0006\n",
            "ihza_0028\n",
            "ixea_0027\n",
            "zegf_0026\n",
            "cgby_0043\n",
            "isab_0027\n",
            "cgby_0042\n",
            "gthh_0009\n",
            "xsup_0022\n",
            "sjew_0010\n",
            "xsup_0012\n",
            "xsup_0030\n",
            "cgby_0032\n",
            "ttog_0003\n",
            "ihza_0015\n",
            "ihza_0003\n",
            "sjew_0014\n",
            "ixea_0022\n",
            "zegf_0018\n",
            "ixea_0017\n",
            "ihza_0004\n",
            "cgby_0023\n",
            "isab_0005\n",
            "zegf_0003\n",
            "cgby_0034\n",
            "ixea_0005\n",
            "cgby_0045\n",
            "cgby_0018\n",
            "sjew_0018\n",
            "xsup_0003\n",
            "ttog_0028\n",
            "ttog_0008\n",
            "zegf_0020\n",
            "gthh_0027\n",
            "cgby_0029\n",
            "isab_0022\n",
            "zegf_0012\n",
            "ixea_0023\n",
            "sjew_0009\n",
            "sjew_0008\n",
            "ixea_0026\n",
            "gthh_0017\n",
            "cgby_0049\n",
            "ttog_0022\n",
            "sjew_0016\n",
            "ttog_0013\n",
            "zegf_0016\n",
            "xsup_0010\n",
            "zegf_0014\n",
            "nzen_0007\n",
            "cgby_0035\n",
            "xsup_0000\n",
            "nzen_0011\n",
            "xsup_0023\n",
            "zegf_0023\n",
            "gthh_0025\n",
            "ixea_0010\n",
            "xsup_0002\n",
            "ixea_0009\n",
            "ttog_0007\n",
            "xsup_0008\n",
            "zegf_0002\n",
            "cgby_0017\n",
            "ixea_0016\n",
            "cgby_0020\n",
            "zegf_0013\n",
            "nzen_0005\n",
            "ttog_0017\n",
            "ixea_0004\n",
            "isab_0002\n",
            "xsup_0032\n",
            "gthh_0000\n",
            "ihza_0012\n",
            "zegf_0009\n",
            "sjew_0025\n",
            "sjew_0020\n",
            "ttog_0004\n",
            "xsup_0016\n",
            "gthh_0019\n",
            "ttog_0035\n",
            "ihza_0019\n",
            "isab_0014\n",
            "xsup_0004\n",
            "ixea_0035\n",
            "cgby_0004\n",
            "xsup_0027\n",
            "ttog_0023\n",
            "isab_0006\n",
            "gthh_0033\n",
            "ttog_0031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge1u-0Srpl1W",
        "outputId": "18931cb3-6c6c-4da8-de8e-14b655d920d4"
      },
      "source": [
        "len(X_total)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7181"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65DWkDhRXqNf"
      },
      "source": [
        "targets = le.transform(y_total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9jXgzCE4j5c",
        "outputId": "2b57c94a-0ae8-43d5-cb48-3e7d432d0526"
      },
      "source": [
        "targets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 7, 7, ..., 5, 5, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPblW9bYgNO1",
        "outputId": "135bca8e-9361-4ee2-f0c2-6f18f8438a5b"
      },
      "source": [
        "len(np.unique(targets))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLoYnMB_YcWR"
      },
      "source": [
        "class LinearClassifier2layer(torch.nn.Module):\n",
        "  def __init__(self, input_dim=1024, hidden_dim=512, output_dim=10):\n",
        "    super(LinearClassifier2layer, self).__init__()\n",
        "    self.linear1 = torch.nn.Linear(input_dim, hidden_dim, bias=True)\n",
        "    self.linear2 = torch.nn.Linear(hidden_dim, output_dim, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.linear1(x))\n",
        "    # x = F.relu(self.linear2(x))\n",
        "    x = self.linear2(x)\n",
        "    # x = self.linear1(x)\n",
        "    return x\n",
        "\n",
        "class LinearClassifier3layer(torch.nn.Module):\n",
        "  def __init__(self, input_dim=1024, hidden_dim=512, output_dim=10):\n",
        "    super(LinearClassifier3layer, self).__init__()\n",
        "    # self.linear1 = torch.nn.Linear(input_dim, output_dim, bias=True)\n",
        "    self.linear1 = torch.nn.Linear(input_dim, hidden_dim, bias=True)\n",
        "    self.linear2 = torch.nn.Linear(hidden_dim, int(hidden_dim/2), bias=True)\n",
        "    self.linear3 = torch.nn.Linear(int(hidden_dim/2), output_dim, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = F.relu(self.linear2(x))\n",
        "    x = self.linear3(x)\n",
        "    # x = self.linear1(x)\n",
        "    return x\n",
        "\n",
        "class LinearClassifier4layer(torch.nn.Module):\n",
        "  def __init__(self, input_dim=1024, hidden_dim=512, output_dim=10):\n",
        "    super(LinearClassifier4layer, self).__init__()\n",
        "    # self.linear1 = torch.nn.Linear(input_dim, output_dim, bias=True)\n",
        "    self.linear1 = torch.nn.Linear(input_dim, hidden_dim, bias=True)\n",
        "    self.linear2 = torch.nn.Linear(hidden_dim, int(hidden_dim/2), bias=True)\n",
        "    self.linear3 = torch.nn.Linear(int(hidden_dim/2), int(hidden_dim/4), bias=True)\n",
        "    self.linear4 = torch.nn.Linear(int(hidden_dim/4), output_dim, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = F.relu(self.linear2(x))\n",
        "    x = F.relu(self.linear3(x))\n",
        "    x = self.linear4(x)\n",
        "    # x = self.linear1(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nC9FUZ2guZ0"
      },
      "source": [
        "num_occurences = encoded_targets.groupby('Nest').nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "nswl5fK7a7CL",
        "outputId": "df6080b3-a24b-48f4-ef39-bae15fc6d4ad"
      },
      "source": [
        "num_occurences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>targets</th>\n",
              "      <th>indvi</th>\n",
              "      <th>key</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Nest10</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest11</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest5</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest6</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest7</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest8</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest9</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        targets  indvi  key\n",
              "Nest                       \n",
              "Nest10        1      1   26\n",
              "Nest11        1      1   49\n",
              "Nest2         1      1   17\n",
              "Nest3         1      1   39\n",
              "Nest4         1      1   33\n",
              "Nest5         1      1   37\n",
              "Nest6         1      1   28\n",
              "Nest7         1      1   35\n",
              "Nest8         1      1   29\n",
              "Nest9         1      1   32"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-wO3FI58ECh"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device ='cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqG6eVRygzEc"
      },
      "source": [
        "class_weights = torch.tensor(num_occurences['key'].min()/num_occurences['key'].values, dtype=float).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1h1_2KrWMyO"
      },
      "source": [
        "model = LinearClassifier2layer(hidden_dim=512, output_dim=len(le.classes_))\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weights.float())\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnzmtoUD6wfp",
        "outputId": "b5bace17-4c49-4f54-f7c5-3a216a821737"
      },
      "source": [
        "encoded_targets_test, le_test = data_splitting_based_on_sample_size(Pupil_dataset)\n",
        "encoded_targets_test = encoded_targets_test.sample(frac=1, random_state=2021).reset_index(drop=True)\n",
        "\n",
        "X_Total_Test = []\n",
        "y_Total_Test = []\n",
        "\n",
        "for key in encoded_targets_test['key'].unique():# [:10]:\n",
        "    print(key)\n",
        "    current_songfile = Pupil_dataset.loc[Pupil_dataset['key']==key]\n",
        "    sequence_length = current_songfile['indvi'].values[-1]\n",
        "    for i in range(0, sequence_length):\n",
        "      X_Total_Test.append(torch.tensor(current_songfile['densenet121_features'].values[i]))\n",
        "      y_Total_Test.append(current_songfile['Nest'].values[i])\n",
        "\n",
        "targets_test = torch.as_tensor(le.transform(y_Total_Test), dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kfgj_0012\n",
            "khxv_0046\n",
            "onsu_0007\n",
            "sdhp_0020\n",
            "nsrn_0007\n",
            "tbfk_0011\n",
            "oogw_0000\n",
            "khxv_0032\n",
            "kccr_0047\n",
            "kccr_0019\n",
            "inji_0004\n",
            "vstd_0024\n",
            "vstd_0001\n",
            "bbyj_0026\n",
            "kfgj_0016\n",
            "qfod_0011\n",
            "cxyc_0013\n",
            "cxyc_0024\n",
            "kccr_0039\n",
            "sdhp_0029\n",
            "vusu_0001\n",
            "tbfk_0004\n",
            "tbfk_0016\n",
            "khxv_0020\n",
            "sdhp_0025\n",
            "kfgj_0002\n",
            "kccr_0033\n",
            "vstd_0009\n",
            "khxv_0025\n",
            "khxv_0012\n",
            "khxv_0024\n",
            "kccr_0054\n",
            "kcos_0015\n",
            "sdhp_0015\n",
            "khxv_0039\n",
            "cxyc_0002\n",
            "kcos_0029\n",
            "inji_0003\n",
            "onsu_0022\n",
            "nsrn_0009\n",
            "tbfk_0021\n",
            "bbyj_0012\n",
            "bbyj_0042\n",
            "hsew_0000\n",
            "khxv_0029\n",
            "bbyj_0043\n",
            "khxv_0004\n",
            "bbyj_0017\n",
            "kcos_0024\n",
            "kccr_0026\n",
            "kccr_0031\n",
            "kccr_0009\n",
            "nsrn_0001\n",
            "nsrn_0005\n",
            "sdhp_0013\n",
            "khxv_0022\n",
            "tbfk_0014\n",
            "khxv_0002\n",
            "nsrn_0019\n",
            "hsew_0006\n",
            "kfgj_0017\n",
            "tbfk_0026\n",
            "tbfk_0028\n",
            "vusu_0026\n",
            "kccr_0060\n",
            "kccr_0025\n",
            "qfod_0008\n",
            "vstd_0010\n",
            "bbyj_0019\n",
            "kccr_0051\n",
            "onsu_0016\n",
            "kcos_0004\n",
            "vstd_0008\n",
            "bbyj_0016\n",
            "bbyj_0001\n",
            "tbfk_0009\n",
            "cxyc_0009\n",
            "oogw_0027\n",
            "oogw_0005\n",
            "kfgj_0029\n",
            "nsrn_0010\n",
            "tbfk_0031\n",
            "oogw_0031\n",
            "onsu_0014\n",
            "kccr_0057\n",
            "onsu_0018\n",
            "kfgj_0015\n",
            "oogw_0016\n",
            "bbyj_0009\n",
            "oogw_0023\n",
            "khxv_0018\n",
            "qfod_0013\n",
            "bbyj_0025\n",
            "cxyc_0001\n",
            "bbyj_0000\n",
            "khxv_0003\n",
            "nsrn_0008\n",
            "hsew_0001\n",
            "oogw_0007\n",
            "kccr_0024\n",
            "sdhp_0016\n",
            "onsu_0008\n",
            "tbfk_0020\n",
            "nsrn_0023\n",
            "kfgj_0000\n",
            "nsrn_0025\n",
            "inji_0021\n",
            "nsrn_0037\n",
            "oogw_0033\n",
            "kccr_0027\n",
            "kccr_0041\n",
            "vstd_0015\n",
            "vstd_0018\n",
            "vstd_0021\n",
            "kccr_0005\n",
            "bbyj_0034\n",
            "vstd_0002\n",
            "vstd_0003\n",
            "cxyc_0000\n",
            "qfod_0001\n",
            "kfgj_0004\n",
            "kfgj_0014\n",
            "kcos_0030\n",
            "cxyc_0008\n",
            "oogw_0004\n",
            "khxv_0037\n",
            "vusu_0023\n",
            "tbfk_0035\n",
            "sdhp_0012\n",
            "vstd_0023\n",
            "kccr_0050\n",
            "kfgj_0023\n",
            "kcos_0011\n",
            "oogw_0024\n",
            "nsrn_0014\n",
            "kccr_0028\n",
            "cxyc_0010\n",
            "onsu_0001\n",
            "sdhp_0002\n",
            "kfgj_0021\n",
            "inji_0019\n",
            "vusu_0024\n",
            "vusu_0020\n",
            "sdhp_0021\n",
            "sdhp_0024\n",
            "tbfk_0025\n",
            "nsrn_0031\n",
            "cxyc_0015\n",
            "tbfk_0008\n",
            "bbyj_0030\n",
            "vstd_0000\n",
            "vstd_0016\n",
            "onsu_0021\n",
            "kccr_0042\n",
            "bbyj_0029\n",
            "hsew_0003\n",
            "khxv_0009\n",
            "cxyc_0022\n",
            "kfgj_0003\n",
            "khxv_0043\n",
            "khxv_0047\n",
            "kccr_0023\n",
            "khxv_0038\n",
            "onsu_0012\n",
            "vusu_0027\n",
            "qfod_0012\n",
            "onsu_0026\n",
            "tbfk_0039\n",
            "cxyc_0006\n",
            "nsrn_0021\n",
            "khxv_0017\n",
            "khxv_0027\n",
            "kcos_0005\n",
            "kcos_0026\n",
            "bbyj_0037\n",
            "sdhp_0023\n",
            "cxyc_0014\n",
            "oogw_0022\n",
            "kfgj_0024\n",
            "onsu_0011\n",
            "kccr_0035\n",
            "vusu_0009\n",
            "sdhp_0018\n",
            "qfod_0000\n",
            "oogw_0008\n",
            "kccr_0030\n",
            "inji_0020\n",
            "kccr_0021\n",
            "kcos_0025\n",
            "onsu_0020\n",
            "nsrn_0011\n",
            "tbfk_0036\n",
            "kcos_0019\n",
            "oogw_0006\n",
            "inji_0010\n",
            "kfgj_0007\n",
            "nsrn_0015\n",
            "vstd_0022\n",
            "tbfk_0033\n",
            "kccr_0013\n",
            "kcos_0009\n",
            "oogw_0013\n",
            "nsrn_0027\n",
            "cxyc_0023\n",
            "oogw_0003\n",
            "kccr_0040\n",
            "bbyj_0020\n",
            "tbfk_0023\n",
            "tbfk_0017\n",
            "tbfk_0012\n",
            "kccr_0004\n",
            "nsrn_0033\n",
            "nsrn_0024\n",
            "cxyc_0003\n",
            "kccr_0016\n",
            "kccr_0059\n",
            "onsu_0009\n",
            "inji_0015\n",
            "kccr_0022\n",
            "oogw_0002\n",
            "khxv_0008\n",
            "kcos_0027\n",
            "bbyj_0006\n",
            "kccr_0038\n",
            "nsrn_0034\n",
            "tbfk_0022\n",
            "bbyj_0044\n",
            "kfgj_0011\n",
            "vstd_0005\n",
            "bbyj_0032\n",
            "kcos_0006\n",
            "sdhp_0026\n",
            "kccr_0058\n",
            "kcos_0022\n",
            "bbyj_0015\n",
            "oogw_0015\n",
            "oogw_0034\n",
            "tbfk_0029\n",
            "nsrn_0035\n",
            "kccr_0010\n",
            "inji_0001\n",
            "tbfk_0034\n",
            "vusu_0013\n",
            "kcos_0007\n",
            "qfod_0007\n",
            "tbfk_0000\n",
            "onsu_0004\n",
            "hsew_0002\n",
            "kccr_0056\n",
            "inji_0011\n",
            "vstd_0030\n",
            "qfod_0014\n",
            "bbyj_0003\n",
            "nsrn_0032\n",
            "tbfk_0015\n",
            "oogw_0011\n",
            "qfod_0003\n",
            "cxyc_0026\n",
            "oogw_0028\n",
            "vusu_0010\n",
            "kfgj_0019\n",
            "bbyj_0039\n",
            "tbfk_0038\n",
            "khxv_0007\n",
            "vusu_0007\n",
            "qfod_0010\n",
            "khxv_0041\n",
            "khxv_0023\n",
            "kcos_0001\n",
            "kcos_0016\n",
            "khxv_0045\n",
            "vstd_0013\n",
            "qfod_0015\n",
            "bbyj_0010\n",
            "kccr_0062\n",
            "kfgj_0005\n",
            "kccr_0055\n",
            "sdhp_0030\n",
            "inji_0017\n",
            "tbfk_0007\n",
            "bbyj_0031\n",
            "tbfk_0010\n",
            "oogw_0029\n",
            "sdhp_0017\n",
            "kccr_0018\n",
            "sdhp_0022\n",
            "cxyc_0018\n",
            "kccr_0048\n",
            "khxv_0031\n",
            "bbyj_0023\n",
            "bbyj_0035\n",
            "bbyj_0005\n",
            "inji_0007\n",
            "kccr_0006\n",
            "bbyj_0022\n",
            "kccr_0061\n",
            "qfod_0002\n",
            "kccr_0012\n",
            "onsu_0003\n",
            "sdhp_0027\n",
            "nsrn_0016\n",
            "kfgj_0009\n",
            "nsrn_0029\n",
            "bbyj_0002\n",
            "qfod_0020\n",
            "khxv_0005\n",
            "vusu_0003\n",
            "inji_0002\n",
            "oogw_0017\n",
            "kcos_0018\n",
            "khxv_0011\n",
            "sdhp_0010\n",
            "bbyj_0028\n",
            "sdhp_0003\n",
            "kccr_0011\n",
            "khxv_0036\n",
            "kccr_0052\n",
            "khxv_0000\n",
            "nsrn_0018\n",
            "qfod_0009\n",
            "bbyj_0018\n",
            "vusu_0011\n",
            "nsrn_0026\n",
            "kcos_0002\n",
            "cxyc_0016\n",
            "sdhp_0006\n",
            "kccr_0014\n",
            "khxv_0006\n",
            "cxyc_0004\n",
            "kccr_0063\n",
            "onsu_0000\n",
            "kcos_0028\n",
            "kccr_0029\n",
            "khxv_0019\n",
            "qfod_0006\n",
            "kccr_0043\n",
            "khxv_0044\n",
            "vusu_0002\n",
            "inji_0008\n",
            "onsu_0025\n",
            "vstd_0014\n",
            "bbyj_0038\n",
            "kfgj_0010\n",
            "vusu_0029\n",
            "kfgj_0025\n",
            "kfgj_0028\n",
            "oogw_0014\n",
            "nsrn_0028\n",
            "nsrn_0004\n",
            "kcos_0003\n",
            "sdhp_0014\n",
            "khxv_0040\n",
            "vstd_0012\n",
            "bbyj_0024\n",
            "vstd_0028\n",
            "vusu_0022\n",
            "khxv_0026\n",
            "kccr_0046\n",
            "khxv_0013\n",
            "kcos_0008\n",
            "bbyj_0036\n",
            "kccr_0015\n",
            "onsu_0023\n",
            "kccr_0003\n",
            "kcos_0014\n",
            "vusu_0000\n",
            "nsrn_0003\n",
            "inji_0000\n",
            "tbfk_0030\n",
            "kcos_0017\n",
            "kcos_0031\n",
            "onsu_0002\n",
            "sdhp_0000\n",
            "kfgj_0020\n",
            "tbfk_0003\n",
            "cxyc_0012\n",
            "oogw_0032\n",
            "vusu_0016\n",
            "qfod_0016\n",
            "inji_0012\n",
            "onsu_0027\n",
            "vusu_0014\n",
            "qfod_0004\n",
            "oogw_0026\n",
            "oogw_0020\n",
            "oogw_0019\n",
            "bbyj_0041\n",
            "vusu_0025\n",
            "bbyj_0033\n",
            "kfgj_0026\n",
            "oogw_0009\n",
            "onsu_0015\n",
            "kfgj_0001\n",
            "vusu_0021\n",
            "qfod_0018\n",
            "sdhp_0019\n",
            "tbfk_0032\n",
            "kfgj_0018\n",
            "vstd_0007\n",
            "bbyj_0008\n",
            "kccr_0045\n",
            "nsrn_0012\n",
            "inji_0018\n",
            "nsrn_0002\n",
            "nsrn_0020\n",
            "vstd_0004\n",
            "khxv_0010\n",
            "kcos_0023\n",
            "inji_0016\n",
            "kcos_0012\n",
            "tbfk_0018\n",
            "tbfk_0001\n",
            "vstd_0017\n",
            "oogw_0001\n",
            "kccr_0017\n",
            "vstd_0019\n",
            "tbfk_0005\n",
            "kccr_0044\n",
            "sdhp_0028\n",
            "kcos_0000\n",
            "kccr_0049\n",
            "onsu_0013\n",
            "kccr_0008\n",
            "sdhp_0009\n",
            "vstd_0006\n",
            "onsu_0017\n",
            "sdhp_0001\n",
            "cxyc_0020\n",
            "khxv_0035\n",
            "khxv_0021\n",
            "tbfk_0006\n",
            "vusu_0017\n",
            "khxv_0001\n",
            "vusu_0008\n",
            "kccr_0020\n",
            "sdhp_0011\n",
            "oogw_0012\n",
            "kfgj_0030\n",
            "hsew_0005\n",
            "tbfk_0013\n",
            "kccr_0037\n",
            "oogw_0025\n",
            "bbyj_0021\n",
            "inji_0014\n",
            "bbyj_0014\n",
            "oogw_0021\n",
            "qfod_0019\n",
            "sdhp_0008\n",
            "onsu_0006\n",
            "khxv_0014\n",
            "bbyj_0007\n",
            "inji_0006\n",
            "inji_0005\n",
            "khxv_0033\n",
            "tbfk_0027\n",
            "nsrn_0000\n",
            "bbyj_0013\n",
            "cxyc_0007\n",
            "vstd_0025\n",
            "kccr_0032\n",
            "cxyc_0017\n",
            "kccr_0036\n",
            "kfgj_0013\n",
            "vusu_0019\n",
            "cxyc_0011\n",
            "kccr_0007\n",
            "hsew_0004\n",
            "nsrn_0013\n",
            "vusu_0005\n",
            "tbfk_0024\n",
            "qfod_0017\n",
            "kcos_0021\n",
            "cxyc_0019\n",
            "onsu_0028\n",
            "onsu_0010\n",
            "oogw_0030\n",
            "vusu_0015\n",
            "khxv_0028\n",
            "cxyc_0025\n",
            "bbyj_0040\n",
            "cxyc_0021\n",
            "oogw_0035\n",
            "vstd_0026\n",
            "sdhp_0004\n",
            "kccr_0053\n",
            "vusu_0018\n",
            "tbfk_0037\n",
            "kfgj_0027\n",
            "kccr_0001\n",
            "cxyc_0005\n",
            "khxv_0034\n",
            "vstd_0011\n",
            "inji_0009\n",
            "vusu_0028\n",
            "kccr_0034\n",
            "kcos_0010\n",
            "oogw_0010\n",
            "nsrn_0022\n",
            "nsrn_0036\n",
            "sdhp_0007\n",
            "kfgj_0022\n",
            "kcos_0013\n",
            "kfgj_0006\n",
            "tbfk_0019\n",
            "vusu_0006\n",
            "sdhp_0005\n",
            "onsu_0005\n",
            "bbyj_0004\n",
            "bbyj_0011\n",
            "khxv_0015\n",
            "nsrn_0006\n",
            "qfod_0005\n",
            "khxv_0016\n",
            "kcos_0020\n",
            "vstd_0029\n",
            "vusu_0012\n",
            "oogw_0018\n",
            "onsu_0019\n",
            "vstd_0027\n",
            "nsrn_0017\n",
            "khxv_0042\n",
            "nsrn_0030\n",
            "kccr_0000\n",
            "khxv_0030\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--2lOzjXtjFw",
        "outputId": "e83e8f31-e05a-4b80-ab90-aee2992b6e50"
      },
      "source": [
        "len(X_Total_Test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14445"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bxQpaxw8FKS",
        "outputId": "ade0d9c5-b93b-4591-8aff-88225f9e2dff"
      },
      "source": [
        "class_weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6538, 0.3469, 1.0000, 0.4359, 0.5152, 0.4595, 0.6071, 0.4857, 0.5862,\n",
              "        0.5312], device='cuda:0', dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "O25kajwUkLaK",
        "outputId": "c9a6d01c-6a96-4c91-f0db-04b84daeef14"
      },
      "source": [
        "encoded_targets_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>targets</th>\n",
              "      <th>indvi</th>\n",
              "      <th>key</th>\n",
              "      <th>Nest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>kfgj</td>\n",
              "      <td>kfgj_0012</td>\n",
              "      <td>Nest8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>khxv</td>\n",
              "      <td>khxv_0046</td>\n",
              "      <td>Nest3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>onsu</td>\n",
              "      <td>onsu_0007</td>\n",
              "      <td>Nest7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>sdhp</td>\n",
              "      <td>sdhp_0020</td>\n",
              "      <td>Nest11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>nsrn</td>\n",
              "      <td>nsrn_0007</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>519</th>\n",
              "      <td>2</td>\n",
              "      <td>nsrn</td>\n",
              "      <td>nsrn_0017</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>520</th>\n",
              "      <td>3</td>\n",
              "      <td>khxv</td>\n",
              "      <td>khxv_0042</td>\n",
              "      <td>Nest3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>521</th>\n",
              "      <td>2</td>\n",
              "      <td>nsrn</td>\n",
              "      <td>nsrn_0030</td>\n",
              "      <td>Nest2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>522</th>\n",
              "      <td>6</td>\n",
              "      <td>kccr</td>\n",
              "      <td>kccr_0000</td>\n",
              "      <td>Nest6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>523</th>\n",
              "      <td>3</td>\n",
              "      <td>khxv</td>\n",
              "      <td>khxv_0030</td>\n",
              "      <td>Nest3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>524 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     targets indvi        key    Nest\n",
              "0          8  kfgj  kfgj_0012   Nest8\n",
              "1          3  khxv  khxv_0046   Nest3\n",
              "2          7  onsu  onsu_0007   Nest7\n",
              "3          1  sdhp  sdhp_0020  Nest11\n",
              "4          2  nsrn  nsrn_0007   Nest2\n",
              "..       ...   ...        ...     ...\n",
              "519        2  nsrn  nsrn_0017   Nest2\n",
              "520        3  khxv  khxv_0042   Nest3\n",
              "521        2  nsrn  nsrn_0030   Nest2\n",
              "522        6  kccr  kccr_0000   Nest6\n",
              "523        3  khxv  khxv_0030   Nest3\n",
              "\n",
              "[524 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "320leASI1YIa"
      },
      "source": [
        "kf = KFold(n_splits  = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upSGtry12lk5"
      },
      "source": [
        "random_number_set = [\n",
        " 3324,\n",
        " 1778,\n",
        " 329,\n",
        " 5689,\n",
        " 2746,\n",
        " 8964,\n",
        " 70,\n",
        " 4786,\n",
        " 2816,\n",
        " 2405,\n",
        " 2681,\n",
        " 5769,\n",
        " 9327,\n",
        " 7867,\n",
        " 4469,\n",
        " 4928,\n",
        " 5730,\n",
        " 858,\n",
        " 6775,\n",
        " 5487,\n",
        " 1464,\n",
        " 7365,\n",
        " 4313,\n",
        " 9324,\n",
        " 8958,\n",
        " 7663,\n",
        " 5362,\n",
        " 299,\n",
        " 4931,\n",
        " 4906,\n",
        " 5007,\n",
        " 5848,\n",
        " 5119,\n",
        " 7172,\n",
        " 6862,\n",
        " 3981,\n",
        " 8262,\n",
        " 5441,\n",
        " 4702,\n",
        " 4177,\n",
        " 2611,\n",
        " 6342,\n",
        " 9402,\n",
        " 5609,\n",
        " 3590,\n",
        " 5397];"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "jVi1WNNT2oSp",
        "outputId": "f68c6447-4b4d-4e70-ba8e-e8de77a57893"
      },
      "source": [
        "epochs = 100; lr = 0.005;\n",
        "\n",
        "df_loss_accuracy_crossval = pd.DataFrame(columns=['Train Loss', 'Train Accuracy', 'Val Loss', 'Val Accuracy', 'Test Loss', 'Test Accuracy'])\n",
        "# overall_df_loss_accuracy_crossval = pd.DataFrame(columns=['Train Accuracy', 'Val Accuracy', 'Train Seq Accuracy', 'Test Seq Accuracy'])\n",
        "Total_val_loss = []\n",
        "\n",
        "def test_sequence_eval(model, syllable_df, encoding):\n",
        "  model.eval()\n",
        "  sequence_prediction = []\n",
        "  actual_prediction = []\n",
        "  for key in encoding['key'].unique():# [:10]:\n",
        "    # print(key)\n",
        "    current_songfile = syllable_df.loc[syllable_df['key']==key]\n",
        "    sequence_length = current_songfile['indvi'].values[-1]\n",
        "    current_songfile = current_songfile.sample(frac=1, random_state=2021).reset_index(drop=True)\n",
        "    sequence_individual_segment = []\n",
        "    for i in range(0, sequence_length):\n",
        "      y_pred = model(torch.tensor(current_songfile['densenet121_features'].values[i]).resize_(1, 1024).to(device))\n",
        "      y_label_pred = torch.max(y_pred, 1)[1].to('cpu')\n",
        "      sequence_individual_segment.append(y_label_pred.numpy()[0])\n",
        "    sequence_prediction.append(np.bincount(sequence_individual_segment).argmax())\n",
        "    actual_prediction.append(le.transform(current_songfile['Nest'].values)[-1])\n",
        "    # print(key, le.transform(current_songfile['Nest'].values)[-1])\n",
        "    # print(sequence_individual_segment)\n",
        "  return accuracy_score(actual_prediction, sequence_prediction), confusion_matrix(actual_prediction, sequence_prediction)\n",
        "\n",
        "def validation_metrics(model_test, X_test, test_targets):\n",
        "  model_test.eval()\n",
        "  sum_loss_test = 0.0\n",
        "  total_test = 0.0\n",
        "  correct_test = 0.0\n",
        "  for i in range(len(X_test)):\n",
        "    x = X_test[i].to(device)\n",
        "    x.resize_(1, 1024)\n",
        "    y_pred = model_test(x)\n",
        "    y = torch.tensor(test_targets[i].to(device)).long().resize_((1))\n",
        "    loss = criterion(y_pred, y)\n",
        "    sum_loss_test += loss.item()*y.shape[0]\n",
        "    y_label_pred = torch.max(y_pred, 1)[1]\n",
        "    correct_test += (y_label_pred == y).float().sum()\n",
        "    total_test += y.shape[0]\n",
        "    # print(y, y_label_pred,correct_test, total_test)\n",
        "  return (sum_loss_test/total_test), (correct_test/total_test)\n",
        "\n",
        "for iter, current_random_number in enumerate([5689]):\n",
        "  \n",
        "  print(iter, current_random_number)\n",
        "  overall_df_loss_accuracy_crossval = pd.DataFrame(columns=['Train Accuracy', 'Val Accuracy', 'Train Seq Accuracy', 'Test Seq Accuracy'])\n",
        "  encoded_targets, le = data_splitting_based_on_sample_size(Pupil_dataset)\n",
        "  encoded_targets = encoded_targets.sample(frac=1, random_state=current_random_number).reset_index(drop=True)\n",
        "\n",
        "  for _fold, (train_index, val_index) in enumerate(kf.split(encoded_targets['key'].unique())):\n",
        "    \n",
        "    # if _fold in [0,3,6,7]:\n",
        "    #   print('Fold number ', _fold)\n",
        "      train_keys = encoded_targets['key'][train_index].values\n",
        "      valid_keys = encoded_targets['key'][val_index].values\n",
        "\n",
        "      X_train_subset = []; y_train_subset = [];\n",
        "      X_val_subset = []; y_val_subset = [];\n",
        "      val_loss_epoch = [];\n",
        "\n",
        "      for key in train_keys:\n",
        "        current_songfile = syllable_df_Nest_Total.loc[syllable_df_Nest_Total['key']==key]\n",
        "        sequence_length = current_songfile['indvi'].values[-1]\n",
        "        for i in range(0, sequence_length):\n",
        "          X_train_subset.append(torch.tensor(current_songfile['densenet121_features'].values[i]))\n",
        "          y_train_subset.append(current_songfile['Nest'].values[i])\n",
        "        targets_train_subset = torch.as_tensor(le.transform(y_train_subset), dtype=torch.long)\n",
        "\n",
        "      for key in valid_keys:\n",
        "        current_songfile = syllable_df_Nest_Total.loc[syllable_df_Nest_Total['key']==key]\n",
        "        sequence_length = current_songfile['indvi'].values[-1]\n",
        "        for i in range(0, sequence_length):\n",
        "          X_val_subset.append(torch.tensor(current_songfile['densenet121_features'].values[i]))\n",
        "          y_val_subset.append(current_songfile['Nest'].values[i])\n",
        "        targets_val_subset = torch.as_tensor(le.transform(y_val_subset), dtype=torch.long)\n",
        "        \n",
        "      train_target_occ = pd.DataFrame.from_dict(Counter(targets_train_subset.numpy()), orient='index').reset_index()\n",
        "      train_target_occ = train_target_occ.rename(columns={'index':'Nest_encoding', 0:'count'}).sort_values(by=['Nest_encoding'])\n",
        "\n",
        "      class_weights = torch.tensor(train_target_occ['count'].min()/train_target_occ['count'].values, dtype=float).to(device)\n",
        "\n",
        "      model = LinearClassifier2layer(hidden_dim=512, output_dim=len(np.unique(encoded_targets['Nest']))).to(device)\n",
        "      criterion = torch.nn.CrossEntropyLoss(weight=class_weights.float())\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "      model.train()\n",
        "\n",
        "      epoch_best_loss = 0\n",
        "      epoch_best_acc = 0\n",
        "      epoch_best_val_loss = 0\n",
        "      epoch_best_val_acc = 0\n",
        "      overall_best_model_train_acc = 0\n",
        "\n",
        "      best_model_test_seq_acc = 0;\n",
        "      best_model_train_seq_acc = 0;\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0    \n",
        "        sum_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        for i in range(len(X_train_subset)):\n",
        "          x = X_train_subset[i].to(device)\n",
        "          x.resize_(1, 1024)\n",
        "          y_pred = model(x)\n",
        "          y = torch.tensor(targets_train_subset[i].to(device)).long().resize_((1))\n",
        "          optimizer.zero_grad()\n",
        "          loss = criterion(y_pred, y)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          sum_loss += loss.item()*y.shape[0]\n",
        "          y_label_pred = torch.max(y_pred, 1)[1]\n",
        "          train_correct += (y_label_pred == y).float().sum()\n",
        "          train_total += y.shape[0]\n",
        "        epoch_loss = sum_loss/train_total\n",
        "        epoch_acc = train_correct/train_total\n",
        "        val_loss, val_accuracy = validation_metrics(model, X_val_subset, targets_val_subset)\n",
        "        train_seq_acc, train_conf_mat = test_sequence_eval(model, Tutor_dataset, encoded_targets)\n",
        "        test_seq_acc, test_conf_mat = test_sequence_eval(model, Pupil_dataset, encoded_targets_test)\n",
        "        print(\"Epoch %d, train loss %.3f train accuracy %.3f val loss %.3f val accuracy %.3f train seq accuracy %.3f test seq accuracy %.3f\" %(epoch, epoch_loss, epoch_acc, val_loss, val_accuracy, train_seq_acc, test_seq_acc))\n",
        "        val_loss_epoch.append(val_loss)\n",
        "        # if epoch_best_val_loss < val_loss:\n",
        "        #   epoch_best_val_loss = val_loss\n",
        "\n",
        "        # if epoch_best_loss < epoch_loss:\n",
        "        #   epoch_best_loss = epoch_loss\n",
        "\n",
        "        if epoch_best_acc <= epoch_acc:\n",
        "          if epoch_best_val_acc <= val_accuracy:        \n",
        "            epoch_best_acc = epoch_acc\n",
        "            epoch_best_val_acc = val_accuracy\n",
        "            epoch_best_val_loss = val_loss\n",
        "            epoch_best_loss = epoch_loss\n",
        "            now = datetime.now() \n",
        "            dt_string = now.strftime(\"%d_%m_%Y_%H_%M\")\n",
        "            best_model = model\n",
        "            print('Saving overall best train val model')\n",
        "            if best_model_test_seq_acc <= test_seq_acc:\n",
        "              best_model_test_seq_acc = test_seq_acc\n",
        "              best_model_train_seq_acc = train_seq_acc\n",
        "              overall_best_model = model\n",
        "              overall_best_model_stats = [ epoch_best_acc.to('cpu').numpy(), epoch_best_val_acc.to('cpu').numpy(), best_model_train_seq_acc, best_model_test_seq_acc]\n",
        "              print('Saving overall best test model')\n",
        "            elif overall_best_model_train_acc < epoch_acc:\n",
        "                best_model_test_seq_acc = test_seq_acc\n",
        "                best_model_train_acc = train_seq_acc\n",
        "                overall_best_model = model\n",
        "                overall_best_model_train_acc = epoch_acc\n",
        "                overall_best_model_stats = [ epoch_best_acc.to('cpu').numpy(), epoch_best_val_acc.to('cpu').numpy(), best_model_test_seq_acc, best_model_train_seq_acc]\n",
        "                overall_best_test_conf_matrix = pd.DataFrame(test_conf_mat, index=[class_label+'_True' for class_label in list(le.classes_)], columns=[class_label+'_Pred' for class_label in list(le.classes_)])\n",
        "                print('Replacing overall best test model')\n",
        "            \n",
        "      # df_loss_accuracy_crossval = df_loss_accuracy_crossval.append({'Train Loss': epoch_best_loss , 'Train Accuracy': epoch_best_acc.numpy(),  'Val Loss':  epoch_best_val_loss, \n",
        "      #                                           'Val Accuracy': epoch_best_val_acc.numpy(), 'Test Loss': best_model_test_loss, 'Test Accuracy': best_model_test_acc.numpy()}, ignore_index=True) \n",
        "      # torch.save(best_model, '/content/gdrive/My Drive/ZFDataset/SavedModels/Baseline/'+'Fold_'+str(_fold)+'_'+dt_string+'TrainSeq_'+str(np.round(best_model_train_seq_acc,4))+'TestSeq_'+str(np.round(best_model_test_seq_acc,4)))\n",
        "      # torch.save(overall_best_model, '/content/gdrive/My Drive/ZFDataset/SavedModels/Baseline/'+'Overall_Fold_'+str(_fold)+'_'+dt_string+'TrainSeq_'+str(np.round(best_model_train_seq_acc,4))+'TestSeq_'+str(np.round(best_model_test_seq_acc,4)))\n",
        "\n",
        "      df_loss_accuracy_crossval = df_loss_accuracy_crossval.append({'Train Loss': epoch_best_loss , 'Train Accuracy': epoch_best_acc.to('cpu').numpy(),  'Val Loss':  epoch_best_val_loss, \n",
        "                                                'Val Accuracy': epoch_best_val_acc.to('cpu').numpy(), 'Train Seq Accuracy': np.round(best_model_train_seq_acc,4), 'Test Seq Accuracy': np.round(best_model_test_seq_acc,4)}, ignore_index=True)\n",
        "\n",
        "      overall_df_loss_accuracy_crossval = overall_df_loss_accuracy_crossval.append(pd.Series(overall_best_model_stats, index=overall_df_loss_accuracy_crossval.columns.values), ignore_index=True)\n",
        "\n",
        "      Total_val_loss.append(val_loss_epoch)\n",
        "\n",
        "      # overall_best_test_conf_matrix.to_csv('/content/gdrive/My Drive/ZFDataset/SavedModels/Final Architecture/Baseline/'+'Conf_Mat_Test_Random_'+str(current_random_number)+'.csv')\n",
        "      # overall_df_loss_accuracy_crossval.to_csv('/content/gdrive/My Drive/ZFDataset/SavedModels/Final Architecture/Baseline/'+'Accuracy_Table_Test_Random_'+str(current_random_number)+'.csv')\n",
        "      \n",
        "      del model\n",
        "      print(\"Model deleted\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 5689\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-8d8ebd91fd77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m           \u001b[0msum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "0aucwOkDRhbC",
        "outputId": "6d694338-d7ac-45db-e938-3ff8cafd5bf5"
      },
      "source": [
        "overall_df_loss_accuracy_crossval = overall_df_loss_accuracy_crossval.append(pd.Series(overall_best_model_stats, index=overall_df_loss_accuracy_crossval.columns.values), ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b3bf7094859f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moverall_df_loss_accuracy_crossval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverall_df_loss_accuracy_crossval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverall_best_model_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverall_df_loss_accuracy_crossval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'overall_df_loss_accuracy_crossval' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "e5paRH0hjMFp",
        "outputId": "6baf3586-545f-4539-e8b2-6e3580a8d4a1"
      },
      "source": [
        "overall_df_loss_accuracy_crossval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-60ee7c61e589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moverall_df_loss_accuracy_crossval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'overall_df_loss_accuracy_crossval' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHpx8A8oGBfQ"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nkKBo7KGPNg",
        "outputId": "f39b68d5-3428-49b8-b02b-c9dbd6f831ec"
      },
      "source": [
        "len(Total_val_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "pMLQrReJF-Nx",
        "outputId": "cd3a7af0-52e5-4426-cfb3-ea8a2044447c"
      },
      "source": [
        "for i in range(0, len(Total_val_loss)):\n",
        "  plt.plot(Total_val_loss[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr8gDu1qEveY"
      },
      "source": [
        "## Training on whole train set and testing on test set ( no cross validation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZjm86Rmwf4F",
        "outputId": "d5e6225b-6949-4bf7-d201-cb08c0294663"
      },
      "source": [
        "Tutor_dataset['key'] == 'kccr_0001'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       False\n",
              "1       False\n",
              "2       False\n",
              "3       False\n",
              "4       False\n",
              "        ...  \n",
              "7176    False\n",
              "7177    False\n",
              "7178    False\n",
              "7179    False\n",
              "7180    False\n",
              "Name: key, Length: 7181, dtype: bool"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vS2JSjCr3z4",
        "outputId": "5908dcbd-79f2-4c34-b1a6-0486962c98f9"
      },
      "source": [
        "epochs = 200; lr = 0.00001;\n",
        "\n",
        "df_loss_accuracy_crossval = pd.DataFrame(columns=['Train Loss', 'Train Accuracy', 'Val Loss', 'Val Accuracy', 'Test Loss', 'Test Accuracy'])\n",
        "overall_df_loss_accuracy_crossval = pd.DataFrame(columns=['Train Accuracy', 'Val Accuracy', 'Train Seq Accuracy', 'Test Seq Accuracy', 'Test Accuracy Equal Weight'])\n",
        "Total_val_loss = []\n",
        "\n",
        "test_accuracy_temp_equal_weight = 0\n",
        "test_accuracy_equal_weight = 0\n",
        "\n",
        "def test_sequence_eval(model, syllable_df, encoding):\n",
        "  model.eval()\n",
        "  sequence_prediction = []\n",
        "  actual_prediction = []\n",
        "  for key in encoding['key'].unique():# [:10]:\n",
        "    # print(key)\n",
        "    current_songfile = syllable_df.loc[syllable_df['key']==key]\n",
        "    sequence_length = current_songfile['indvi'].values[-1]\n",
        "    current_songfile = current_songfile.sample(frac=1, random_state=2021).reset_index(drop=True)\n",
        "    sequence_individual_segment = []\n",
        "    with torch.no_grad():\n",
        "      for i in range(0, sequence_length):\n",
        "        y_pred = model(torch.tensor(current_songfile['densenet121_features'].values[i]).resize_(1, 1024).to(device))\n",
        "        y_label_pred = torch.max(y_pred, 1)[1].to('cpu')\n",
        "        sequence_individual_segment.append(y_label_pred.numpy()[0])\n",
        "      sequence_prediction.append(np.bincount(sequence_individual_segment).argmax())\n",
        "      actual_prediction.append(le.transform(current_songfile['Nest'].values)[-1])\n",
        "      conf_mat = confusion_matrix(actual_prediction, sequence_prediction)\n",
        "    # print(key, le.transform(current_songfile['Nest'].values)[-1])\n",
        "    # print(sequence_individual_segment)\n",
        "  return accuracy_score(actual_prediction, sequence_prediction), conf_mat, Nest_Equal_weight_accuracy(conf_mat)\n",
        "\n",
        "def Nest_Equal_weight_accuracy(confusion_matrix):\n",
        "  Nest_sum = confusion_matrix.sum(axis=1)\n",
        "  Indv_Nest_Acc = [];\n",
        "  for i in range(0, Nest_sum.shape[0]):\n",
        "    Indv_Nest_Acc.append(confusion_matrix[i, i]/Nest_sum[i])\n",
        "    # print(confusion_matrix[i, i]/Nest_sum[i])\n",
        "  return np.mean(Indv_Nest_Acc)\n",
        "\n",
        "def validation_metrics(model_test, X_test, test_targets):\n",
        "  model_test.eval()\n",
        "  sum_loss_test = 0.0\n",
        "  total_test = 0.0\n",
        "  correct_test = 0.0\n",
        "  with torch.no_grad():\n",
        "    for i in range(len(X_test)):\n",
        "      x = X_test[i].to(device)\n",
        "      x.resize_(1, 1024)\n",
        "      y_pred = model_test(x)\n",
        "      y = torch.tensor(test_targets[i].to(device)).long().resize_((1))\n",
        "      loss = criterion(y_pred, y)\n",
        "      sum_loss_test += loss.item()*y.shape[0]\n",
        "      y_label_pred = torch.max(y_pred, 1)[1]\n",
        "      correct_test += (y_label_pred == y).float().sum()\n",
        "      total_test += y.shape[0]\n",
        "      # print(y, y_label_pred,correct_test, total_test)\n",
        "  return (sum_loss_test/total_test), (correct_test/total_test)\n",
        "\n",
        "current_random_number = 2021;\n",
        "\n",
        "def data_shuffle(input_dataset):\n",
        "  encoding, le = data_splitting_based_on_sample_size(input_dataset)    # Set which dataset to train on over here\n",
        "  encoding = encoding.sample(frac=1, random_state=int(np.random.randint(9999, size=1))).reset_index(drop=True)\n",
        "\n",
        "  X_total = []; y_total = [];\n",
        "  for key in encoding['key'].unique():# [:10]:\n",
        "      current_songfile = input_dataset.loc[input_dataset['key']==key]\n",
        "      sequence_length = current_songfile['indvi'].values[-1]\n",
        "      for i in range(0, sequence_length):\n",
        "        X_total.append(torch.tensor(current_songfile['densenet121_features'].values[i]))\n",
        "        y_total.append(current_songfile['Nest'].values[i])\n",
        "\n",
        "  targets_total = torch.as_tensor(le.transform(y_total), dtype=torch.long)\n",
        "\n",
        "  return X_total, y_total, targets_total\n",
        "\n",
        "X_train, y_train, targets_train = data_shuffle(Tutor_dataset)\n",
        "X_test, y_test, targets_test = data_shuffle(Pupil_dataset)\n",
        "\n",
        "targets_train = torch.as_tensor(le.transform(y_train), dtype=torch.long)\n",
        "targets_test = torch.as_tensor(le.transform(y_test), dtype=torch.long)\n",
        "\n",
        "train_target_occ = pd.DataFrame.from_dict(Counter(targets_train.numpy()), orient='index').reset_index()\n",
        "train_target_occ = train_target_occ.rename(columns={'index':'Nest_encoding', 0:'count'}).sort_values(by=['Nest_encoding'])\n",
        "\n",
        "class_weights = torch.tensor(train_target_occ['count'].min()/train_target_occ['count'].values, dtype=float).to(device)\n",
        "# print(class_weights)\n",
        "\n",
        "model = LinearClassifier3layer(hidden_dim=128, output_dim=len(np.unique(encoded_targets['Nest']))).to(device)   ##  SET MODEL HERE\n",
        "print(model.eval())\n",
        "# criterion = torch.nn.CrossEntropyLoss(weight=class_weights.float())\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "# model.train()\n",
        "\n",
        "epoch_best_loss = 0\n",
        "epoch_best_acc = 0\n",
        "epoch_best_val_loss = 0\n",
        "epoch_best_val_acc = 0\n",
        "overall_best_model_train_acc = 0\n",
        "overall_best_model_stats = []\n",
        "valid_loss_epoch = []\n",
        "test_loss_epoch = []\n",
        "\n",
        "best_model_test_seq_acc = 0;\n",
        "best_model_train_seq_acc = 0;\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0    \n",
        "  sum_loss = 0\n",
        "  # loss = 0\n",
        "  train_correct = 0\n",
        "  train_total = 0\n",
        "  X_train, y_train, targets_train = data_shuffle(Tutor_dataset)\n",
        "  # optimizer.zero_grad()\n",
        "\n",
        "  for i in range(len(X_train)):\n",
        "    x = X_train[i].to(device)\n",
        "    x.resize_(1, 1024)\n",
        "    y_pred = model(x)\n",
        "    y = torch.tensor(targets_train[i].to(device)).long().resize_((1))\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(y_pred, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    sum_loss += loss.item()*y.shape[0]\n",
        "    \n",
        "    # loss += criterion(y_pred, y)\n",
        "    \n",
        "    y_label_pred = torch.max(y_pred, 1)[1]\n",
        "    train_correct += (y_label_pred == y).float().sum()\n",
        "    train_total += y.shape[0]\n",
        "  # loss.backward()\n",
        "  # optimizer.step()\n",
        "  # sum_loss = loss.item()*y.shape[0]\n",
        "  epoch_loss = sum_loss/train_total\n",
        "  epoch_acc = train_correct/train_total  \n",
        "  val_loss, val_accuracy = validation_metrics(model, X_train, targets_train)\n",
        "  test_loss, test_accuracy = validation_metrics(model, X_test, targets_test)\n",
        "  train_seq_acc, train_conf_mat, train_acuracy_equal_weight = test_sequence_eval(model, Tutor_dataset, encoded_targets)\n",
        "  test_seq_acc, test_conf_mat, test_accuracy_equal_weight = test_sequence_eval(model, Pupil_dataset, encoded_targets_test)\n",
        "  print(\"Epoch %d, train loss %.3f train accuracy %.3f val loss %.3f val accuracy %.3f train seq accuracy %.3f test seq accuracy %.3f test seq ew accuracy %.3f\" %(epoch, epoch_loss, epoch_acc, val_loss, val_accuracy, train_seq_acc, test_seq_acc, test_accuracy_equal_weight))\n",
        "  valid_loss_epoch.append(val_loss)\n",
        "  test_loss_epoch.append(test_loss)\n",
        "  \n",
        "  if epoch_best_acc <= epoch_acc:\n",
        "    if epoch_best_val_acc <= val_accuracy:        \n",
        "      epoch_best_acc = epoch_acc\n",
        "      epoch_best_val_acc = val_accuracy\n",
        "      epoch_best_val_loss = val_loss\n",
        "      epoch_best_loss = epoch_loss\n",
        "      epoch_best_val_acc_equal_weight = train_acuracy_equal_weight\n",
        "      test_accuracy_temp_equal_weight = test_accuracy_equal_weight\n",
        "      now = datetime.now() \n",
        "      dt_string = now.strftime(\"%d_%m_%Y_%H_%M\")\n",
        "      best_model = model\n",
        "      print('Saving overall best train val model')\n",
        "      if best_model_test_seq_acc <= test_seq_acc:\n",
        "        best_model_test_seq_acc = test_seq_acc\n",
        "        best_model_train_seq_acc = train_seq_acc\n",
        "        best_model_test_acc_equal_weight = test_accuracy_equal_weight\n",
        "        overall_best_model = model\n",
        "        overall_best_model_stats = [ epoch_best_acc.to('cpu').numpy(), epoch_best_val_acc.to('cpu').numpy(), best_model_train_seq_acc, best_model_test_seq_acc, best_model_test_acc_equal_weight]\n",
        "        print('Saving overall best test model')\n",
        "      elif overall_best_model_train_acc < epoch_acc:\n",
        "          best_model_test_seq_acc = test_seq_acc\n",
        "          best_model_train_seq_acc = train_seq_acc\n",
        "          best_model_test_acc_equal_weight = test_accuracy_equal_weight\n",
        "          overall_best_model = model\n",
        "          overall_best_model_train_acc = epoch_acc\n",
        "          overall_best_model_stats = [ epoch_best_acc.to('cpu').numpy(), epoch_best_val_acc.to('cpu').numpy(), best_model_train_seq_acc, best_model_test_seq_acc, best_model_test_acc_equal_weight]\n",
        "          overall_best_test_conf_matrix = pd.DataFrame(test_conf_mat, index=[class_label+'_True' for class_label in list(le.classes_)], columns=[class_label+'_Pred' for class_label in list(le.classes_)])\n",
        "          print('Replacing overall best test model')\n",
        "      \n",
        "# df_loss_accuracy_crossval = df_loss_accuracy_crossval.append({'Train Loss': epoch_best_loss , 'Train Accuracy': epoch_best_acc.numpy(),  'Val Loss':  epoch_best_val_loss, \n",
        "#                                           'Val Accuracy': epoch_best_val_acc.numpy(), 'Test Loss': best_model_test_loss, 'Test Accuracy': best_model_test_acc.numpy()}, ignore_index=True) \n",
        "# torch.save(best_model, '/content/gdrive/My Drive/ZFDataset/FullDataset/SavedModels/Baseline/'+'Total_'+dt_string+'TrainSeq_'+str(np.round(best_model_train_seq_acc,4))+'TestSeq_'+str(np.round(best_model_test_seq_acc,4)))\n",
        "torch.save(overall_best_model, '/content/gdrive/My Drive/ZFDataset/SavedModels/FullDataset/Baseline/'+'Total_'+dt_string+'TrainSeq_'+str(np.round(best_model_train_seq_acc,4))+'TestSeq_'+str(np.round(best_model_test_seq_acc,4))+'_EW_Test_'+str(np.round(best_model_test_acc_equal_weight,4)))\n",
        "df_loss_accuracy_crossval = df_loss_accuracy_crossval.append({'Train Loss': epoch_best_loss , 'Train Accuracy': epoch_best_acc.to('cpu').numpy(),  'Val Loss':  epoch_best_val_loss, \n",
        "                                          'Val Accuracy': epoch_best_val_acc.to('cpu').numpy(), 'Train Seq Accuracy': np.round(best_model_train_seq_acc,4), 'Test Seq Accuracy': np.round(best_model_test_seq_acc,4), \n",
        "                                          'Test Accuracy Equal Weight': test_accuracy_temp_equal_weight}, ignore_index=True)\n",
        "\n",
        "overall_df_loss_accuracy_crossval = overall_df_loss_accuracy_crossval.append(pd.Series(overall_best_model_stats, index=overall_df_loss_accuracy_crossval.columns.values), ignore_index=True)\n",
        "\n",
        "# Total_val_loss.append(val_loss_epoch)\n",
        "# Total_test_loss.append(test_loss_epoch)\n",
        "\n",
        "# overall_best_test_conf_matrix.to_csv('/content/gdrive/My Drive/ZFDataset/SavedModels/Final Architecture/Baseline/'+'Conf_Mat_Test_Random_'+str(current_random_number)+'.csv')\n",
        "# overall_df_loss_accuracy_crossval.to_csv('/content/gdrive/My Drive/ZFDataset/SavedModels/Final Architecture/Baseline/'+'Accuracy_Table_Test_Random_'+str(current_random_number)+'.csv')\n",
        "\n",
        "# del model\n",
        "# print(\"Model deleted\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearClassifier3layer(\n",
            "  (linear1): Linear(in_features=1024, out_features=128, bias=True)\n",
            "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, train loss 2.300 train accuracy 0.088 val loss 2.289 val accuracy 0.116 train seq accuracy 0.120 test seq accuracy 0.109 test seq ew accuracy 0.100\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 1, train loss 2.280 train accuracy 0.153 val loss 2.270 val accuracy 0.184 train seq accuracy 0.163 test seq accuracy 0.061 test seq ew accuracy 0.113\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 2, train loss 2.262 train accuracy 0.175 val loss 2.254 val accuracy 0.172 train seq accuracy 0.157 test seq accuracy 0.191 test seq ew accuracy 0.170\n",
            "Epoch 3, train loss 2.246 train accuracy 0.173 val loss 2.238 val accuracy 0.172 train seq accuracy 0.157 test seq accuracy 0.214 test seq ew accuracy 0.158\n",
            "Epoch 4, train loss 2.230 train accuracy 0.178 val loss 2.222 val accuracy 0.182 train seq accuracy 0.148 test seq accuracy 0.216 test seq ew accuracy 0.140\n",
            "Epoch 5, train loss 2.214 train accuracy 0.193 val loss 2.206 val accuracy 0.242 train seq accuracy 0.246 test seq accuracy 0.219 test seq ew accuracy 0.145\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 6, train loss 2.199 train accuracy 0.263 val loss 2.192 val accuracy 0.276 train seq accuracy 0.231 test seq accuracy 0.135 test seq ew accuracy 0.146\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 7, train loss 2.184 train accuracy 0.279 val loss 2.177 val accuracy 0.237 train seq accuracy 0.151 test seq accuracy 0.116 test seq ew accuracy 0.100\n",
            "Epoch 8, train loss 2.171 train accuracy 0.236 val loss 2.164 val accuracy 0.208 train seq accuracy 0.151 test seq accuracy 0.116 test seq ew accuracy 0.100\n",
            "Epoch 9, train loss 2.158 train accuracy 0.212 val loss 2.152 val accuracy 0.193 train seq accuracy 0.151 test seq accuracy 0.116 test seq ew accuracy 0.100\n",
            "Epoch 10, train loss 2.146 train accuracy 0.190 val loss 2.141 val accuracy 0.196 train seq accuracy 0.151 test seq accuracy 0.116 test seq ew accuracy 0.100\n",
            "Epoch 11, train loss 2.135 train accuracy 0.214 val loss 2.130 val accuracy 0.194 train seq accuracy 0.151 test seq accuracy 0.116 test seq ew accuracy 0.100\n",
            "Epoch 12, train loss 2.125 train accuracy 0.209 val loss 2.120 val accuracy 0.197 train seq accuracy 0.151 test seq accuracy 0.116 test seq ew accuracy 0.100\n",
            "Epoch 13, train loss 2.114 train accuracy 0.204 val loss 2.110 val accuracy 0.206 train seq accuracy 0.151 test seq accuracy 0.116 test seq ew accuracy 0.100\n",
            "Epoch 14, train loss 2.104 train accuracy 0.211 val loss 2.099 val accuracy 0.220 train seq accuracy 0.154 test seq accuracy 0.116 test seq ew accuracy 0.100\n",
            "Epoch 15, train loss 2.093 train accuracy 0.251 val loss 2.089 val accuracy 0.219 train seq accuracy 0.154 test seq accuracy 0.116 test seq ew accuracy 0.100\n",
            "Epoch 16, train loss 2.083 train accuracy 0.240 val loss 2.078 val accuracy 0.223 train seq accuracy 0.157 test seq accuracy 0.116 test seq ew accuracy 0.100\n",
            "Epoch 17, train loss 2.072 train accuracy 0.244 val loss 2.067 val accuracy 0.235 train seq accuracy 0.163 test seq accuracy 0.118 test seq ew accuracy 0.101\n",
            "Epoch 18, train loss 2.061 train accuracy 0.251 val loss 2.056 val accuracy 0.252 train seq accuracy 0.185 test seq accuracy 0.118 test seq ew accuracy 0.101\n",
            "Epoch 19, train loss 2.050 train accuracy 0.261 val loss 2.044 val accuracy 0.273 train seq accuracy 0.215 test seq accuracy 0.120 test seq ew accuracy 0.102\n",
            "Epoch 20, train loss 2.038 train accuracy 0.285 val loss 2.032 val accuracy 0.292 train seq accuracy 0.249 test seq accuracy 0.128 test seq ew accuracy 0.111\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 21, train loss 2.025 train accuracy 0.277 val loss 2.019 val accuracy 0.306 train seq accuracy 0.265 test seq accuracy 0.143 test seq ew accuracy 0.124\n",
            "Epoch 22, train loss 2.012 train accuracy 0.307 val loss 2.006 val accuracy 0.304 train seq accuracy 0.265 test seq accuracy 0.143 test seq ew accuracy 0.117\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 23, train loss 1.998 train accuracy 0.320 val loss 1.993 val accuracy 0.300 train seq accuracy 0.268 test seq accuracy 0.134 test seq ew accuracy 0.111\n",
            "Epoch 24, train loss 1.986 train accuracy 0.316 val loss 1.979 val accuracy 0.313 train seq accuracy 0.268 test seq accuracy 0.155 test seq ew accuracy 0.131\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 25, train loss 1.971 train accuracy 0.318 val loss 1.964 val accuracy 0.336 train seq accuracy 0.274 test seq accuracy 0.179 test seq ew accuracy 0.164\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 26, train loss 1.956 train accuracy 0.345 val loss 1.949 val accuracy 0.347 train seq accuracy 0.274 test seq accuracy 0.197 test seq ew accuracy 0.154\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 27, train loss 1.941 train accuracy 0.354 val loss 1.934 val accuracy 0.343 train seq accuracy 0.274 test seq accuracy 0.195 test seq ew accuracy 0.149\n",
            "Epoch 28, train loss 1.926 train accuracy 0.355 val loss 1.919 val accuracy 0.362 train seq accuracy 0.286 test seq accuracy 0.185 test seq ew accuracy 0.146\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 29, train loss 1.910 train accuracy 0.373 val loss 1.904 val accuracy 0.367 train seq accuracy 0.298 test seq accuracy 0.204 test seq ew accuracy 0.158\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 30, train loss 1.895 train accuracy 0.381 val loss 1.888 val accuracy 0.382 train seq accuracy 0.314 test seq accuracy 0.202 test seq ew accuracy 0.154\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 31, train loss 1.880 train accuracy 0.388 val loss 1.872 val accuracy 0.389 train seq accuracy 0.354 test seq accuracy 0.210 test seq ew accuracy 0.158\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 32, train loss 1.864 train accuracy 0.396 val loss 1.857 val accuracy 0.388 train seq accuracy 0.348 test seq accuracy 0.225 test seq ew accuracy 0.168\n",
            "Epoch 33, train loss 1.849 train accuracy 0.398 val loss 1.842 val accuracy 0.397 train seq accuracy 0.363 test seq accuracy 0.223 test seq ew accuracy 0.166\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 34, train loss 1.834 train accuracy 0.406 val loss 1.826 val accuracy 0.409 train seq accuracy 0.372 test seq accuracy 0.258 test seq ew accuracy 0.187\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 35, train loss 1.818 train accuracy 0.419 val loss 1.812 val accuracy 0.409 train seq accuracy 0.366 test seq accuracy 0.265 test seq ew accuracy 0.191\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 36, train loss 1.804 train accuracy 0.412 val loss 1.797 val accuracy 0.424 train seq accuracy 0.391 test seq accuracy 0.286 test seq ew accuracy 0.202\n",
            "Epoch 37, train loss 1.790 train accuracy 0.424 val loss 1.784 val accuracy 0.428 train seq accuracy 0.397 test seq accuracy 0.326 test seq ew accuracy 0.221\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 38, train loss 1.775 train accuracy 0.427 val loss 1.771 val accuracy 0.442 train seq accuracy 0.452 test seq accuracy 0.382 test seq ew accuracy 0.249\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 39, train loss 1.763 train accuracy 0.438 val loss 1.756 val accuracy 0.437 train seq accuracy 0.434 test seq accuracy 0.342 test seq ew accuracy 0.228\n",
            "Epoch 40, train loss 1.749 train accuracy 0.441 val loss 1.743 val accuracy 0.440 train seq accuracy 0.434 test seq accuracy 0.368 test seq ew accuracy 0.242\n",
            "Epoch 41, train loss 1.736 train accuracy 0.437 val loss 1.730 val accuracy 0.452 train seq accuracy 0.471 test seq accuracy 0.370 test seq ew accuracy 0.241\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 42, train loss 1.723 train accuracy 0.448 val loss 1.718 val accuracy 0.457 train seq accuracy 0.474 test seq accuracy 0.378 test seq ew accuracy 0.246\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 43, train loss 1.710 train accuracy 0.451 val loss 1.707 val accuracy 0.460 train seq accuracy 0.483 test seq accuracy 0.401 test seq ew accuracy 0.259\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 44, train loss 1.699 train accuracy 0.461 val loss 1.694 val accuracy 0.462 train seq accuracy 0.480 test seq accuracy 0.378 test seq ew accuracy 0.246\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 45, train loss 1.687 train accuracy 0.464 val loss 1.682 val accuracy 0.463 train seq accuracy 0.483 test seq accuracy 0.389 test seq ew accuracy 0.251\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 46, train loss 1.676 train accuracy 0.471 val loss 1.672 val accuracy 0.460 train seq accuracy 0.471 test seq accuracy 0.363 test seq ew accuracy 0.237\n",
            "Epoch 47, train loss 1.662 train accuracy 0.462 val loss 1.661 val accuracy 0.477 train seq accuracy 0.517 test seq accuracy 0.420 test seq ew accuracy 0.270\n",
            "Epoch 48, train loss 1.654 train accuracy 0.481 val loss 1.649 val accuracy 0.473 train seq accuracy 0.498 test seq accuracy 0.427 test seq ew accuracy 0.271\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 49, train loss 1.643 train accuracy 0.473 val loss 1.638 val accuracy 0.478 train seq accuracy 0.502 test seq accuracy 0.406 test seq ew accuracy 0.261\n",
            "Epoch 50, train loss 1.630 train accuracy 0.478 val loss 1.629 val accuracy 0.478 train seq accuracy 0.495 test seq accuracy 0.395 test seq ew accuracy 0.256\n",
            "Epoch 51, train loss 1.622 train accuracy 0.480 val loss 1.617 val accuracy 0.484 train seq accuracy 0.511 test seq accuracy 0.406 test seq ew accuracy 0.259\n",
            "Epoch 52, train loss 1.611 train accuracy 0.487 val loss 1.608 val accuracy 0.486 train seq accuracy 0.523 test seq accuracy 0.433 test seq ew accuracy 0.273\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 53, train loss 1.601 train accuracy 0.487 val loss 1.597 val accuracy 0.489 train seq accuracy 0.520 test seq accuracy 0.426 test seq ew accuracy 0.272\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 54, train loss 1.591 train accuracy 0.490 val loss 1.588 val accuracy 0.494 train seq accuracy 0.548 test seq accuracy 0.403 test seq ew accuracy 0.262\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 55, train loss 1.581 train accuracy 0.498 val loss 1.577 val accuracy 0.496 train seq accuracy 0.542 test seq accuracy 0.412 test seq ew accuracy 0.265\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 56, train loss 1.571 train accuracy 0.499 val loss 1.568 val accuracy 0.497 train seq accuracy 0.535 test seq accuracy 0.427 test seq ew accuracy 0.274\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 57, train loss 1.561 train accuracy 0.502 val loss 1.558 val accuracy 0.499 train seq accuracy 0.548 test seq accuracy 0.427 test seq ew accuracy 0.269\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 58, train loss 1.550 train accuracy 0.504 val loss 1.550 val accuracy 0.504 train seq accuracy 0.554 test seq accuracy 0.403 test seq ew accuracy 0.264\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 59, train loss 1.541 train accuracy 0.504 val loss 1.539 val accuracy 0.511 train seq accuracy 0.578 test seq accuracy 0.401 test seq ew accuracy 0.264\n",
            "Epoch 60, train loss 1.533 train accuracy 0.511 val loss 1.531 val accuracy 0.515 train seq accuracy 0.588 test seq accuracy 0.399 test seq ew accuracy 0.261\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 61, train loss 1.523 train accuracy 0.513 val loss 1.521 val accuracy 0.520 train seq accuracy 0.609 test seq accuracy 0.405 test seq ew accuracy 0.267\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 62, train loss 1.514 train accuracy 0.521 val loss 1.512 val accuracy 0.516 train seq accuracy 0.588 test seq accuracy 0.437 test seq ew accuracy 0.279\n",
            "Epoch 63, train loss 1.505 train accuracy 0.525 val loss 1.503 val accuracy 0.517 train seq accuracy 0.585 test seq accuracy 0.437 test seq ew accuracy 0.275\n",
            "Epoch 64, train loss 1.493 train accuracy 0.521 val loss 1.495 val accuracy 0.522 train seq accuracy 0.634 test seq accuracy 0.429 test seq ew accuracy 0.274\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 65, train loss 1.488 train accuracy 0.529 val loss 1.485 val accuracy 0.529 train seq accuracy 0.612 test seq accuracy 0.427 test seq ew accuracy 0.276\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 66, train loss 1.477 train accuracy 0.536 val loss 1.477 val accuracy 0.528 train seq accuracy 0.612 test seq accuracy 0.435 test seq ew accuracy 0.279\n",
            "Epoch 67, train loss 1.467 train accuracy 0.537 val loss 1.467 val accuracy 0.533 train seq accuracy 0.609 test seq accuracy 0.418 test seq ew accuracy 0.270\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 68, train loss 1.459 train accuracy 0.535 val loss 1.459 val accuracy 0.542 train seq accuracy 0.658 test seq accuracy 0.397 test seq ew accuracy 0.267\n",
            "Epoch 69, train loss 1.450 train accuracy 0.540 val loss 1.450 val accuracy 0.549 train seq accuracy 0.662 test seq accuracy 0.414 test seq ew accuracy 0.273\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 70, train loss 1.443 train accuracy 0.546 val loss 1.440 val accuracy 0.551 train seq accuracy 0.658 test seq accuracy 0.418 test seq ew accuracy 0.274\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 71, train loss 1.432 train accuracy 0.552 val loss 1.431 val accuracy 0.546 train seq accuracy 0.649 test seq accuracy 0.429 test seq ew accuracy 0.278\n",
            "Epoch 72, train loss 1.422 train accuracy 0.547 val loss 1.425 val accuracy 0.551 train seq accuracy 0.668 test seq accuracy 0.418 test seq ew accuracy 0.283\n",
            "Epoch 73, train loss 1.415 train accuracy 0.559 val loss 1.417 val accuracy 0.546 train seq accuracy 0.658 test seq accuracy 0.431 test seq ew accuracy 0.276\n",
            "Epoch 74, train loss 1.408 train accuracy 0.555 val loss 1.406 val accuracy 0.560 train seq accuracy 0.652 test seq accuracy 0.410 test seq ew accuracy 0.278\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 75, train loss 1.398 train accuracy 0.558 val loss 1.397 val accuracy 0.559 train seq accuracy 0.658 test seq accuracy 0.406 test seq ew accuracy 0.268\n",
            "Epoch 76, train loss 1.388 train accuracy 0.558 val loss 1.389 val accuracy 0.569 train seq accuracy 0.683 test seq accuracy 0.418 test seq ew accuracy 0.284\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 77, train loss 1.376 train accuracy 0.568 val loss 1.387 val accuracy 0.565 train seq accuracy 0.698 test seq accuracy 0.416 test seq ew accuracy 0.272\n",
            "Epoch 78, train loss 1.370 train accuracy 0.579 val loss 1.379 val accuracy 0.553 train seq accuracy 0.671 test seq accuracy 0.437 test seq ew accuracy 0.279\n",
            "Epoch 79, train loss 1.364 train accuracy 0.568 val loss 1.368 val accuracy 0.560 train seq accuracy 0.683 test seq accuracy 0.435 test seq ew accuracy 0.278\n",
            "Epoch 80, train loss 1.357 train accuracy 0.572 val loss 1.358 val accuracy 0.577 train seq accuracy 0.662 test seq accuracy 0.408 test seq ew accuracy 0.280\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 81, train loss 1.344 train accuracy 0.581 val loss 1.352 val accuracy 0.566 train seq accuracy 0.695 test seq accuracy 0.412 test seq ew accuracy 0.269\n",
            "Epoch 82, train loss 1.343 train accuracy 0.574 val loss 1.341 val accuracy 0.578 train seq accuracy 0.671 test seq accuracy 0.412 test seq ew accuracy 0.279\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 83, train loss 1.330 train accuracy 0.580 val loss 1.338 val accuracy 0.580 train seq accuracy 0.652 test seq accuracy 0.399 test seq ew accuracy 0.268\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 84, train loss 1.323 train accuracy 0.587 val loss 1.326 val accuracy 0.575 train seq accuracy 0.668 test seq accuracy 0.422 test seq ew accuracy 0.270\n",
            "Epoch 85, train loss 1.317 train accuracy 0.586 val loss 1.318 val accuracy 0.582 train seq accuracy 0.649 test seq accuracy 0.424 test seq ew accuracy 0.289\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 86, train loss 1.310 train accuracy 0.594 val loss 1.314 val accuracy 0.579 train seq accuracy 0.665 test seq accuracy 0.433 test seq ew accuracy 0.285\n",
            "Epoch 87, train loss 1.303 train accuracy 0.588 val loss 1.303 val accuracy 0.592 train seq accuracy 0.686 test seq accuracy 0.412 test seq ew accuracy 0.280\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 88, train loss 1.292 train accuracy 0.599 val loss 1.297 val accuracy 0.584 train seq accuracy 0.689 test seq accuracy 0.420 test seq ew accuracy 0.270\n",
            "Epoch 89, train loss 1.283 train accuracy 0.594 val loss 1.295 val accuracy 0.590 train seq accuracy 0.692 test seq accuracy 0.439 test seq ew accuracy 0.296\n",
            "Epoch 90, train loss 1.280 train accuracy 0.601 val loss 1.280 val accuracy 0.605 train seq accuracy 0.698 test seq accuracy 0.414 test seq ew accuracy 0.287\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 91, train loss 1.269 train accuracy 0.604 val loss 1.275 val accuracy 0.600 train seq accuracy 0.695 test seq accuracy 0.422 test seq ew accuracy 0.290\n",
            "Epoch 92, train loss 1.264 train accuracy 0.607 val loss 1.266 val accuracy 0.602 train seq accuracy 0.705 test seq accuracy 0.418 test seq ew accuracy 0.273\n",
            "Epoch 93, train loss 1.257 train accuracy 0.611 val loss 1.257 val accuracy 0.608 train seq accuracy 0.695 test seq accuracy 0.427 test seq ew accuracy 0.289\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 94, train loss 1.245 train accuracy 0.616 val loss 1.256 val accuracy 0.602 train seq accuracy 0.662 test seq accuracy 0.406 test seq ew accuracy 0.287\n",
            "Epoch 95, train loss 1.244 train accuracy 0.609 val loss 1.244 val accuracy 0.623 train seq accuracy 0.720 test seq accuracy 0.418 test seq ew accuracy 0.301\n",
            "Epoch 96, train loss 1.235 train accuracy 0.614 val loss 1.240 val accuracy 0.618 train seq accuracy 0.723 test seq accuracy 0.429 test seq ew accuracy 0.281\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 97, train loss 1.230 train accuracy 0.621 val loss 1.234 val accuracy 0.613 train seq accuracy 0.720 test seq accuracy 0.443 test seq ew accuracy 0.299\n",
            "Epoch 98, train loss 1.222 train accuracy 0.622 val loss 1.225 val accuracy 0.621 train seq accuracy 0.698 test seq accuracy 0.422 test seq ew accuracy 0.301\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 99, train loss 1.214 train accuracy 0.622 val loss 1.218 val accuracy 0.627 train seq accuracy 0.735 test seq accuracy 0.420 test seq ew accuracy 0.275\n",
            "Epoch 100, train loss 1.201 train accuracy 0.642 val loss 1.223 val accuracy 0.602 train seq accuracy 0.628 test seq accuracy 0.448 test seq ew accuracy 0.310\n",
            "Epoch 101, train loss 1.198 train accuracy 0.619 val loss 1.205 val accuracy 0.637 train seq accuracy 0.742 test seq accuracy 0.420 test seq ew accuracy 0.319\n",
            "Epoch 102, train loss 1.195 train accuracy 0.642 val loss 1.200 val accuracy 0.620 train seq accuracy 0.671 test seq accuracy 0.427 test seq ew accuracy 0.298\n",
            "Epoch 103, train loss 1.189 train accuracy 0.630 val loss 1.195 val accuracy 0.625 train seq accuracy 0.695 test seq accuracy 0.429 test seq ew accuracy 0.308\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 104, train loss 1.183 train accuracy 0.633 val loss 1.183 val accuracy 0.640 train seq accuracy 0.726 test seq accuracy 0.426 test seq ew accuracy 0.294\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 105, train loss 1.176 train accuracy 0.641 val loss 1.178 val accuracy 0.645 train seq accuracy 0.738 test seq accuracy 0.414 test seq ew accuracy 0.308\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 106, train loss 1.169 train accuracy 0.650 val loss 1.177 val accuracy 0.628 train seq accuracy 0.738 test seq accuracy 0.418 test seq ew accuracy 0.276\n",
            "Epoch 107, train loss 1.161 train accuracy 0.649 val loss 1.170 val accuracy 0.631 train seq accuracy 0.720 test seq accuracy 0.420 test seq ew accuracy 0.291\n",
            "Epoch 108, train loss 1.155 train accuracy 0.644 val loss 1.163 val accuracy 0.642 train seq accuracy 0.714 test seq accuracy 0.403 test seq ew accuracy 0.308\n",
            "Epoch 109, train loss 1.144 train accuracy 0.645 val loss 1.163 val accuracy 0.648 train seq accuracy 0.742 test seq accuracy 0.376 test seq ew accuracy 0.295\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 110, train loss 1.146 train accuracy 0.653 val loss 1.146 val accuracy 0.661 train seq accuracy 0.769 test seq accuracy 0.420 test seq ew accuracy 0.302\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 111, train loss 1.140 train accuracy 0.660 val loss 1.139 val accuracy 0.655 train seq accuracy 0.757 test seq accuracy 0.412 test seq ew accuracy 0.294\n",
            "Epoch 112, train loss 1.130 train accuracy 0.665 val loss 1.136 val accuracy 0.653 train seq accuracy 0.708 test seq accuracy 0.422 test seq ew accuracy 0.309\n",
            "Epoch 113, train loss 1.125 train accuracy 0.658 val loss 1.132 val accuracy 0.669 train seq accuracy 0.754 test seq accuracy 0.408 test seq ew accuracy 0.317\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 114, train loss 1.118 train accuracy 0.667 val loss 1.125 val accuracy 0.651 train seq accuracy 0.680 test seq accuracy 0.435 test seq ew accuracy 0.322\n",
            "Epoch 115, train loss 1.114 train accuracy 0.665 val loss 1.117 val accuracy 0.666 train seq accuracy 0.735 test seq accuracy 0.416 test seq ew accuracy 0.311\n",
            "Epoch 116, train loss 1.104 train accuracy 0.666 val loss 1.114 val accuracy 0.669 train seq accuracy 0.772 test seq accuracy 0.424 test seq ew accuracy 0.306\n",
            "Epoch 117, train loss 1.102 train accuracy 0.669 val loss 1.105 val accuracy 0.660 train seq accuracy 0.751 test seq accuracy 0.408 test seq ew accuracy 0.285\n",
            "Epoch 118, train loss 1.093 train accuracy 0.669 val loss 1.101 val accuracy 0.680 train seq accuracy 0.778 test seq accuracy 0.412 test seq ew accuracy 0.309\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 119, train loss 1.088 train accuracy 0.680 val loss 1.108 val accuracy 0.672 train seq accuracy 0.742 test seq accuracy 0.414 test seq ew accuracy 0.343\n",
            "Epoch 120, train loss 1.085 train accuracy 0.681 val loss 1.088 val accuracy 0.674 train seq accuracy 0.769 test seq accuracy 0.426 test seq ew accuracy 0.311\n",
            "Epoch 121, train loss 1.079 train accuracy 0.681 val loss 1.083 val accuracy 0.672 train seq accuracy 0.778 test seq accuracy 0.412 test seq ew accuracy 0.279\n",
            "Epoch 122, train loss 1.072 train accuracy 0.681 val loss 1.078 val accuracy 0.674 train seq accuracy 0.800 test seq accuracy 0.424 test seq ew accuracy 0.298\n",
            "Epoch 123, train loss 1.065 train accuracy 0.686 val loss 1.074 val accuracy 0.687 train seq accuracy 0.791 test seq accuracy 0.433 test seq ew accuracy 0.330\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 124, train loss 1.060 train accuracy 0.698 val loss 1.066 val accuracy 0.660 train seq accuracy 0.668 test seq accuracy 0.433 test seq ew accuracy 0.337\n",
            "Epoch 125, train loss 1.056 train accuracy 0.687 val loss 1.067 val accuracy 0.665 train seq accuracy 0.674 test seq accuracy 0.427 test seq ew accuracy 0.349\n",
            "Epoch 126, train loss 1.049 train accuracy 0.681 val loss 1.056 val accuracy 0.703 train seq accuracy 0.812 test seq accuracy 0.408 test seq ew accuracy 0.314\n",
            "Epoch 127, train loss 1.045 train accuracy 0.700 val loss 1.055 val accuracy 0.697 train seq accuracy 0.797 test seq accuracy 0.351 test seq ew accuracy 0.286\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 128, train loss 1.039 train accuracy 0.703 val loss 1.044 val accuracy 0.686 train seq accuracy 0.754 test seq accuracy 0.429 test seq ew accuracy 0.333\n",
            "Epoch 129, train loss 1.031 train accuracy 0.698 val loss 1.040 val accuracy 0.696 train seq accuracy 0.791 test seq accuracy 0.401 test seq ew accuracy 0.323\n",
            "Epoch 130, train loss 1.019 train accuracy 0.703 val loss 1.041 val accuracy 0.688 train seq accuracy 0.803 test seq accuracy 0.418 test seq ew accuracy 0.293\n",
            "Epoch 131, train loss 1.024 train accuracy 0.700 val loss 1.030 val accuracy 0.699 train seq accuracy 0.812 test seq accuracy 0.416 test seq ew accuracy 0.280\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 132, train loss 1.020 train accuracy 0.705 val loss 1.020 val accuracy 0.707 train seq accuracy 0.825 test seq accuracy 0.416 test seq ew accuracy 0.304\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 133, train loss 1.011 train accuracy 0.713 val loss 1.016 val accuracy 0.708 train seq accuracy 0.803 test seq accuracy 0.389 test seq ew accuracy 0.294\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 134, train loss 1.007 train accuracy 0.708 val loss 1.010 val accuracy 0.705 train seq accuracy 0.815 test seq accuracy 0.422 test seq ew accuracy 0.286\n",
            "Epoch 135, train loss 1.000 train accuracy 0.710 val loss 1.009 val accuracy 0.720 train seq accuracy 0.831 test seq accuracy 0.372 test seq ew accuracy 0.289\n",
            "Epoch 136, train loss 0.997 train accuracy 0.722 val loss 0.999 val accuracy 0.711 train seq accuracy 0.818 test seq accuracy 0.422 test seq ew accuracy 0.326\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 137, train loss 0.989 train accuracy 0.713 val loss 0.998 val accuracy 0.718 train seq accuracy 0.834 test seq accuracy 0.376 test seq ew accuracy 0.300\n",
            "Epoch 138, train loss 0.984 train accuracy 0.726 val loss 0.993 val accuracy 0.713 train seq accuracy 0.812 test seq accuracy 0.426 test seq ew accuracy 0.295\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 139, train loss 0.978 train accuracy 0.720 val loss 0.993 val accuracy 0.722 train seq accuracy 0.837 test seq accuracy 0.336 test seq ew accuracy 0.287\n",
            "Epoch 140, train loss 0.976 train accuracy 0.725 val loss 0.980 val accuracy 0.724 train seq accuracy 0.868 test seq accuracy 0.399 test seq ew accuracy 0.296\n",
            "Epoch 141, train loss 0.964 train accuracy 0.726 val loss 0.978 val accuracy 0.722 train seq accuracy 0.852 test seq accuracy 0.385 test seq ew accuracy 0.296\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 142, train loss 0.965 train accuracy 0.726 val loss 0.969 val accuracy 0.730 train seq accuracy 0.855 test seq accuracy 0.408 test seq ew accuracy 0.317\n",
            "Epoch 143, train loss 0.955 train accuracy 0.726 val loss 0.970 val accuracy 0.723 train seq accuracy 0.862 test seq accuracy 0.418 test seq ew accuracy 0.308\n",
            "Epoch 144, train loss 0.950 train accuracy 0.734 val loss 0.967 val accuracy 0.721 train seq accuracy 0.865 test seq accuracy 0.401 test seq ew accuracy 0.261\n",
            "Epoch 145, train loss 0.951 train accuracy 0.733 val loss 0.958 val accuracy 0.735 train seq accuracy 0.874 test seq accuracy 0.406 test seq ew accuracy 0.326\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 146, train loss 0.942 train accuracy 0.735 val loss 0.948 val accuracy 0.730 train seq accuracy 0.862 test seq accuracy 0.420 test seq ew accuracy 0.319\n",
            "Epoch 147, train loss 0.938 train accuracy 0.738 val loss 0.948 val accuracy 0.727 train seq accuracy 0.828 test seq accuracy 0.395 test seq ew accuracy 0.355\n",
            "Epoch 148, train loss 0.929 train accuracy 0.733 val loss 0.945 val accuracy 0.743 train seq accuracy 0.880 test seq accuracy 0.426 test seq ew accuracy 0.324\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 149, train loss 0.924 train accuracy 0.738 val loss 0.945 val accuracy 0.730 train seq accuracy 0.865 test seq accuracy 0.374 test seq ew accuracy 0.286\n",
            "Epoch 150, train loss 0.927 train accuracy 0.742 val loss 0.930 val accuracy 0.742 train seq accuracy 0.895 test seq accuracy 0.418 test seq ew accuracy 0.311\n",
            "Epoch 151, train loss 0.920 train accuracy 0.745 val loss 0.933 val accuracy 0.736 train seq accuracy 0.865 test seq accuracy 0.431 test seq ew accuracy 0.332\n",
            "Epoch 152, train loss 0.913 train accuracy 0.742 val loss 0.927 val accuracy 0.746 train seq accuracy 0.908 test seq accuracy 0.426 test seq ew accuracy 0.293\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 153, train loss 0.910 train accuracy 0.748 val loss 0.918 val accuracy 0.745 train seq accuracy 0.892 test seq accuracy 0.385 test seq ew accuracy 0.291\n",
            "Epoch 154, train loss 0.899 train accuracy 0.749 val loss 0.920 val accuracy 0.742 train seq accuracy 0.902 test seq accuracy 0.437 test seq ew accuracy 0.308\n",
            "Epoch 155, train loss 0.901 train accuracy 0.750 val loss 0.906 val accuracy 0.751 train seq accuracy 0.908 test seq accuracy 0.410 test seq ew accuracy 0.323\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 156, train loss 0.896 train accuracy 0.753 val loss 0.907 val accuracy 0.754 train seq accuracy 0.886 test seq accuracy 0.372 test seq ew accuracy 0.315\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 157, train loss 0.888 train accuracy 0.761 val loss 0.908 val accuracy 0.738 train seq accuracy 0.886 test seq accuracy 0.403 test seq ew accuracy 0.276\n",
            "Epoch 158, train loss 0.888 train accuracy 0.754 val loss 0.899 val accuracy 0.748 train seq accuracy 0.886 test seq accuracy 0.412 test seq ew accuracy 0.379\n",
            "Epoch 159, train loss 0.883 train accuracy 0.757 val loss 0.893 val accuracy 0.744 train seq accuracy 0.914 test seq accuracy 0.410 test seq ew accuracy 0.291\n",
            "Epoch 160, train loss 0.875 train accuracy 0.753 val loss 0.888 val accuracy 0.760 train seq accuracy 0.926 test seq accuracy 0.416 test seq ew accuracy 0.304\n",
            "Epoch 161, train loss 0.870 train accuracy 0.762 val loss 0.890 val accuracy 0.747 train seq accuracy 0.877 test seq accuracy 0.380 test seq ew accuracy 0.315\n",
            "Epoch 162, train loss 0.864 train accuracy 0.758 val loss 0.882 val accuracy 0.752 train seq accuracy 0.895 test seq accuracy 0.429 test seq ew accuracy 0.321\n",
            "Epoch 163, train loss 0.860 train accuracy 0.762 val loss 0.881 val accuracy 0.748 train seq accuracy 0.929 test seq accuracy 0.445 test seq ew accuracy 0.348\n",
            "Epoch 164, train loss 0.863 train accuracy 0.761 val loss 0.864 val accuracy 0.765 train seq accuracy 0.905 test seq accuracy 0.416 test seq ew accuracy 0.319\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 165, train loss 0.848 train accuracy 0.767 val loss 0.884 val accuracy 0.739 train seq accuracy 0.902 test seq accuracy 0.441 test seq ew accuracy 0.305\n",
            "Epoch 166, train loss 0.848 train accuracy 0.763 val loss 0.865 val accuracy 0.768 train seq accuracy 0.914 test seq accuracy 0.376 test seq ew accuracy 0.324\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 167, train loss 0.847 train accuracy 0.765 val loss 0.854 val accuracy 0.764 train seq accuracy 0.917 test seq accuracy 0.401 test seq ew accuracy 0.313\n",
            "Epoch 168, train loss 0.839 train accuracy 0.767 val loss 0.853 val accuracy 0.758 train seq accuracy 0.908 test seq accuracy 0.410 test seq ew accuracy 0.305\n",
            "Epoch 169, train loss 0.834 train accuracy 0.769 val loss 0.854 val accuracy 0.761 train seq accuracy 0.914 test seq accuracy 0.378 test seq ew accuracy 0.323\n",
            "Epoch 170, train loss 0.834 train accuracy 0.766 val loss 0.842 val accuracy 0.770 train seq accuracy 0.920 test seq accuracy 0.399 test seq ew accuracy 0.300\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 171, train loss 0.826 train accuracy 0.771 val loss 0.838 val accuracy 0.769 train seq accuracy 0.920 test seq accuracy 0.384 test seq ew accuracy 0.327\n",
            "Epoch 172, train loss 0.823 train accuracy 0.771 val loss 0.836 val accuracy 0.776 train seq accuracy 0.926 test seq accuracy 0.418 test seq ew accuracy 0.340\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 173, train loss 0.819 train accuracy 0.775 val loss 0.830 val accuracy 0.770 train seq accuracy 0.923 test seq accuracy 0.399 test seq ew accuracy 0.306\n",
            "Epoch 174, train loss 0.816 train accuracy 0.777 val loss 0.824 val accuracy 0.771 train seq accuracy 0.914 test seq accuracy 0.416 test seq ew accuracy 0.315\n",
            "Epoch 175, train loss 0.810 train accuracy 0.780 val loss 0.818 val accuracy 0.774 train seq accuracy 0.923 test seq accuracy 0.422 test seq ew accuracy 0.357\n",
            "Epoch 176, train loss 0.806 train accuracy 0.776 val loss 0.816 val accuracy 0.779 train seq accuracy 0.926 test seq accuracy 0.410 test seq ew accuracy 0.333\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 177, train loss 0.800 train accuracy 0.782 val loss 0.815 val accuracy 0.774 train seq accuracy 0.911 test seq accuracy 0.385 test seq ew accuracy 0.321\n",
            "Epoch 178, train loss 0.801 train accuracy 0.778 val loss 0.810 val accuracy 0.776 train seq accuracy 0.923 test seq accuracy 0.405 test seq ew accuracy 0.297\n",
            "Epoch 179, train loss 0.796 train accuracy 0.779 val loss 0.802 val accuracy 0.781 train seq accuracy 0.923 test seq accuracy 0.424 test seq ew accuracy 0.344\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 180, train loss 0.791 train accuracy 0.785 val loss 0.799 val accuracy 0.778 train seq accuracy 0.914 test seq accuracy 0.416 test seq ew accuracy 0.325\n",
            "Epoch 181, train loss 0.788 train accuracy 0.782 val loss 0.793 val accuracy 0.783 train seq accuracy 0.929 test seq accuracy 0.437 test seq ew accuracy 0.344\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 182, train loss 0.786 train accuracy 0.782 val loss 0.789 val accuracy 0.783 train seq accuracy 0.932 test seq accuracy 0.437 test seq ew accuracy 0.333\n",
            "Epoch 183, train loss 0.783 train accuracy 0.786 val loss 0.784 val accuracy 0.785 train seq accuracy 0.932 test seq accuracy 0.426 test seq ew accuracy 0.319\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 184, train loss 0.776 train accuracy 0.785 val loss 0.790 val accuracy 0.778 train seq accuracy 0.917 test seq accuracy 0.389 test seq ew accuracy 0.322\n",
            "Epoch 185, train loss 0.774 train accuracy 0.788 val loss 0.781 val accuracy 0.781 train seq accuracy 0.923 test seq accuracy 0.405 test seq ew accuracy 0.320\n",
            "Epoch 186, train loss 0.762 train accuracy 0.792 val loss 0.776 val accuracy 0.785 train seq accuracy 0.935 test seq accuracy 0.441 test seq ew accuracy 0.326\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 187, train loss 0.761 train accuracy 0.789 val loss 0.777 val accuracy 0.782 train seq accuracy 0.917 test seq accuracy 0.393 test seq ew accuracy 0.327\n",
            "Epoch 188, train loss 0.763 train accuracy 0.787 val loss 0.768 val accuracy 0.789 train seq accuracy 0.920 test seq accuracy 0.431 test seq ew accuracy 0.356\n",
            "Epoch 189, train loss 0.752 train accuracy 0.791 val loss 0.778 val accuracy 0.783 train seq accuracy 0.920 test seq accuracy 0.427 test seq ew accuracy 0.372\n",
            "Epoch 190, train loss 0.753 train accuracy 0.786 val loss 0.763 val accuracy 0.785 train seq accuracy 0.929 test seq accuracy 0.422 test seq ew accuracy 0.330\n",
            "Epoch 191, train loss 0.744 train accuracy 0.793 val loss 0.767 val accuracy 0.790 train seq accuracy 0.920 test seq accuracy 0.397 test seq ew accuracy 0.378\n",
            "Saving overall best train val model\n",
            "Replacing overall best test model\n",
            "Epoch 192, train loss 0.747 train accuracy 0.797 val loss 0.754 val accuracy 0.791 train seq accuracy 0.923 test seq accuracy 0.414 test seq ew accuracy 0.348\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 193, train loss 0.744 train accuracy 0.795 val loss 0.751 val accuracy 0.788 train seq accuracy 0.932 test seq accuracy 0.435 test seq ew accuracy 0.331\n",
            "Epoch 194, train loss 0.732 train accuracy 0.796 val loss 0.774 val accuracy 0.776 train seq accuracy 0.935 test seq accuracy 0.437 test seq ew accuracy 0.280\n",
            "Epoch 195, train loss 0.739 train accuracy 0.794 val loss 0.741 val accuracy 0.794 train seq accuracy 0.932 test seq accuracy 0.437 test seq ew accuracy 0.334\n",
            "Epoch 196, train loss 0.727 train accuracy 0.798 val loss 0.753 val accuracy 0.788 train seq accuracy 0.917 test seq accuracy 0.395 test seq ew accuracy 0.340\n",
            "Epoch 197, train loss 0.727 train accuracy 0.802 val loss 0.738 val accuracy 0.792 train seq accuracy 0.920 test seq accuracy 0.427 test seq ew accuracy 0.348\n",
            "Saving overall best train val model\n",
            "Saving overall best test model\n",
            "Epoch 198, train loss 0.724 train accuracy 0.799 val loss 0.733 val accuracy 0.800 train seq accuracy 0.929 test seq accuracy 0.406 test seq ew accuracy 0.348\n",
            "Epoch 199, train loss 0.717 train accuracy 0.801 val loss 0.748 val accuracy 0.782 train seq accuracy 0.914 test seq accuracy 0.376 test seq ew accuracy 0.324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDyeVuSLZ-mE"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "NTdeCAs3aDHc",
        "outputId": "f27bbf5a-0687-46fd-d7bd-a02617d8b5a5"
      },
      "source": [
        "plt.plot(valid_loss_epoch)\n",
        "plt.plot(test_loss_epoch)\n",
        "# plt.plot(Total_test_loss[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f86bc3b0810>]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "BLTz3jQbaOmM",
        "outputId": "b062bef4-7ffc-4f10-9bd9-632b2821f23b"
      },
      "source": [
        "df_loss_accuracy_crossval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train Loss</th>\n",
              "      <th>Train Accuracy</th>\n",
              "      <th>Val Loss</th>\n",
              "      <th>Val Accuracy</th>\n",
              "      <th>Test Loss</th>\n",
              "      <th>Test Accuracy</th>\n",
              "      <th>Test Accuracy Equal Weight</th>\n",
              "      <th>Test Seq Accuracy</th>\n",
              "      <th>Train Seq Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.37178</td>\n",
              "      <td>0.89346886</td>\n",
              "      <td>0.437981</td>\n",
              "      <td>0.86506057</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.471949</td>\n",
              "      <td>0.5687</td>\n",
              "      <td>0.9908</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Train Loss Train Accuracy  ...  Test Seq Accuracy Train Seq Accuracy\n",
              "0     0.37178     0.89346886  ...             0.5687             0.9908\n",
              "\n",
              "[1 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "lgPXaMUYaSW7",
        "outputId": "a68e6bc5-6dad-4506-e792-1ff906858852"
      },
      "source": [
        "overall_df_loss_accuracy_crossval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-60ee7c61e589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moverall_df_loss_accuracy_crossval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'overall_df_loss_accuracy_crossval' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeFqtQV6gL6V",
        "outputId": "7dcd8292-b4a5-4655-c11b-06acfe74fb92"
      },
      "source": [
        "Nest_Equal_weight_accuracy(test_conf_mat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3977957750144352"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vimbSdkFgUZi",
        "outputId": "0a74bb26-0e02-456e-b6dc-c7a02f2ab4ad"
      },
      "source": [
        "test_conf_mat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   5,   2,   0,   0,   0,   0],\n",
              "       [  0,  16,   0,   1,   0,   0,  43,   0,   0,   1],\n",
              "       [  0,   0,  13,  64,   1,   3,   4,   0,   0,   1],\n",
              "       [  0,   0,   0,  83,   1,   0,   0,   0,   0,   0],\n",
              "       [  0,   0,   0,   2,   2,   8,  20,   0,   0,   0],\n",
              "       [  0,   0,   0,   1,   0,  35,   3,   0,   0,   0],\n",
              "       [  0,   0,   0,   0,   1,   0, 106,   0,   0,   0],\n",
              "       [  0,   0,   0,   1,   2,   1,  21,   5,   0,  27],\n",
              "       [  1,   0,   3,   3,   2,   3,   9,   0,   9,   0],\n",
              "       [  0,   0,   0,  16,   0,   0,   0,   0,   0,   5]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7-10QZW-D9W"
      },
      "source": [
        "df_loss_accuracy_crossval = df_loss_accuracy_crossval.append({'Train Loss': epoch_best_loss , 'Train Accuracy': epoch_best_acc.to('cpu').numpy(),  'Val Loss':  epoch_best_val_loss, \n",
        "                                          'Val Accuracy': epoch_best_val_acc.to('cpu').numpy(), 'Train Seq Accuracy': np.round(best_model_train_seq_acc,4), 'Test Seq Accuracy': np.round(best_model_test_seq_acc,4)}, ignore_index=True)\n",
        "\n",
        "overall_df_loss_accuracy_crossval = overall_df_loss_accuracy_crossval.append(pd.Series(overall_best_model_stats, index=overall_df_loss_accuracy_crossval.columns.values), ignore_index=True)\n",
        "\n",
        "Total_val_loss.append(val_loss_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_yAp63Ilf3B"
      },
      "source": [
        "base_path = '/content/gdrive/My Drive/ZFDataset/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJplWr04lQgr",
        "outputId": "11c78da4-5a86-4b48-fc77-3956c8b6ac44"
      },
      "source": [
        "model_name = 'Total_22_07_2021_01_41TrainSeq_1.0TestSeq_0.5324'\n",
        "model = torch.load(base_path+'SavedModels/FullDataset/Baseline/'+model_name)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearClassifier2layer(\n",
              "  (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (linear3): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPil-ItT_CTV",
        "outputId": "76870de3-0592-45ca-89ca-a2be71bcfa16"
      },
      "source": [
        "print(model.eval())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearClassifier2layer(\n",
            "  (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (linear3): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pS4ZyKzCnOGp",
        "outputId": "31c76138-4dba-4e9f-f01e-79fc19b58c05"
      },
      "source": [
        "le.inverse_transform([np.bincount(sequence_individual_segment).argmax()]).tolist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Nest3']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFpFF2bx_FfF"
      },
      "source": [
        "model.eval()\n",
        "sequence_prediction = []\n",
        "sequence_prediction_label = []\n",
        "actual_label = []\n",
        "\n",
        "for key in encoded_targets_test['key'].unique():# [:10]:\n",
        "  # print(key)\n",
        "  current_songfile = Pupil_dataset.loc[Pupil_dataset['key']==key]\n",
        "  sequence_length = current_songfile['indvi'].values[-1]\n",
        "  # current_songfile = current_songfile.sample(frac=1, random_state=2021).reset_index(drop=True)\n",
        "  sequence_individual_segment = []\n",
        "  for i in range(0, sequence_length):\n",
        "    y_pred = model(torch.tensor(current_songfile['densenet121_features'].values[i]).resize_(1, 1024).to(device))\n",
        "    y_label_pred = torch.max(y_pred, 1)[1].to('cpu')\n",
        "    sequence_individual_segment.append(y_label_pred.numpy()[0])\n",
        "  sequence_prediction.append(np.bincount(sequence_individual_segment).argmax())\n",
        "  actual_label.append(le.transform(current_songfile['Nest'].values)[-1])\n",
        "  sequence_prediction_label.extend(le.inverse_transform([np.bincount(sequence_individual_segment).argmax()]).tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfjMTLYHnpKB"
      },
      "source": [
        "encoded_targets_test['Nest Pred'] = sequence_prediction_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "rtkXO1B9my1K",
        "outputId": "c9945bb1-3193-4f79-8eef-51548db03a46"
      },
      "source": [
        "encoded_targets_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>targets</th>\n",
              "      <th>indvi</th>\n",
              "      <th>key</th>\n",
              "      <th>Nest</th>\n",
              "      <th>Nest Pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>kfgj</td>\n",
              "      <td>kfgj_0012</td>\n",
              "      <td>Nest8</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>khxv</td>\n",
              "      <td>khxv_0046</td>\n",
              "      <td>Nest3</td>\n",
              "      <td>Nest3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>onsu</td>\n",
              "      <td>onsu_0007</td>\n",
              "      <td>Nest7</td>\n",
              "      <td>Nest9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>sdhp</td>\n",
              "      <td>sdhp_0020</td>\n",
              "      <td>Nest11</td>\n",
              "      <td>Nest6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>nsrn</td>\n",
              "      <td>nsrn_0007</td>\n",
              "      <td>Nest2</td>\n",
              "      <td>Nest3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>519</th>\n",
              "      <td>2</td>\n",
              "      <td>nsrn</td>\n",
              "      <td>nsrn_0017</td>\n",
              "      <td>Nest2</td>\n",
              "      <td>Nest3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>520</th>\n",
              "      <td>3</td>\n",
              "      <td>khxv</td>\n",
              "      <td>khxv_0042</td>\n",
              "      <td>Nest3</td>\n",
              "      <td>Nest3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>521</th>\n",
              "      <td>2</td>\n",
              "      <td>nsrn</td>\n",
              "      <td>nsrn_0030</td>\n",
              "      <td>Nest2</td>\n",
              "      <td>Nest3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>522</th>\n",
              "      <td>6</td>\n",
              "      <td>kccr</td>\n",
              "      <td>kccr_0000</td>\n",
              "      <td>Nest6</td>\n",
              "      <td>Nest6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>523</th>\n",
              "      <td>3</td>\n",
              "      <td>khxv</td>\n",
              "      <td>khxv_0030</td>\n",
              "      <td>Nest3</td>\n",
              "      <td>Nest3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>524 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     targets indvi        key    Nest Nest Pred\n",
              "0          8  kfgj  kfgj_0012   Nest8     Nest5\n",
              "1          3  khxv  khxv_0046   Nest3     Nest3\n",
              "2          7  onsu  onsu_0007   Nest7     Nest9\n",
              "3          1  sdhp  sdhp_0020  Nest11     Nest6\n",
              "4          2  nsrn  nsrn_0007   Nest2     Nest3\n",
              "..       ...   ...        ...     ...       ...\n",
              "519        2  nsrn  nsrn_0017   Nest2     Nest3\n",
              "520        3  khxv  khxv_0042   Nest3     Nest3\n",
              "521        2  nsrn  nsrn_0030   Nest2     Nest3\n",
              "522        6  kccr  kccr_0000   Nest6     Nest6\n",
              "523        3  khxv  khxv_0030   Nest3     Nest3\n",
              "\n",
              "[524 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01d9MSrUrFMC"
      },
      "source": [
        "encoded_targets_test.to_csv('/content/gdrive/My Drive/ZFDataset/SavedModels/FullDataset/Baseline/'+'IndividualID_Nest'+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "CmvkyrY-EBG_",
        "outputId": "38070f10-94f5-491f-90f0-15c17d960da6"
      },
      "source": [
        "overall_best_test_conf_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Nest10_Pred</th>\n",
              "      <th>Nest11_Pred</th>\n",
              "      <th>Nest2_Pred</th>\n",
              "      <th>Nest3_Pred</th>\n",
              "      <th>Nest4_Pred</th>\n",
              "      <th>Nest5_Pred</th>\n",
              "      <th>Nest6_Pred</th>\n",
              "      <th>Nest7_Pred</th>\n",
              "      <th>Nest8_Pred</th>\n",
              "      <th>Nest9_Pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Nest10_True</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest11_True</th>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>55</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest2_True</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest3_True</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>81</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest4_True</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest5_True</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest6_True</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>107</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest7_True</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest8_True</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest9_True</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Nest10_Pred  Nest11_Pred  ...  Nest8_Pred  Nest9_Pred\n",
              "Nest10_True            0            0  ...           0           0\n",
              "Nest11_True            0            6  ...           0           0\n",
              "Nest2_True             0            0  ...           0           0\n",
              "Nest3_True             0            0  ...           0           0\n",
              "Nest4_True             0            0  ...           0           0\n",
              "Nest5_True             0            0  ...           0           0\n",
              "Nest6_True             0            0  ...           0           0\n",
              "Nest7_True             0            2  ...           0          15\n",
              "Nest8_True             0            0  ...           1           0\n",
              "Nest9_True             0            0  ...           0           6\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJs-p47h_Hnc"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "JgIucgto_NkM",
        "outputId": "9365eed0-80db-427b-bcb4-f195c7f07094"
      },
      "source": [
        "plt.plot(Total_val_loss[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-92d103291321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTotal_val_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Total_val_loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6MUkmegophQ",
        "outputId": "02354ae0-c0f7-47f1-c0b9-c91ba2fb327f"
      },
      "source": [
        "PupilID_Nest = np.zeros(shape=(len(encoded_targets_test['indvi'].unique()), len(encoded_targets_test['Nest'].unique())), dtype=int )\n",
        "\n",
        "counter = 0;\n",
        "PupilID_Nest_Names = []\n",
        "for indvi in encoded_targets_test['indvi'].unique():\n",
        "  current_Bird = encoded_targets_test.loc[encoded_targets_test['indvi']==indvi]\n",
        "  PupilID_Nest_Names.extend([str(indvi)+'_'+str(current_Bird['Nest'].values[-1])])\n",
        "  print(counter, [str(indvi)+'_'+str(current_Bird['Nest'].values[-1])], len(current_Bird['key']))\n",
        "  for key in current_Bird['key'].unique():\n",
        "    # print(key, counter, int(le.transform(current_Bird['Nest Pred'].loc[current_Bird['key']==key].values)) , PupilID_Nest[counter][int(le.transform(current_Bird['Nest Pred'].loc[current_Bird['key']==key].values))])\n",
        "    PupilID_Nest[counter][int(le.transform(current_Bird['Nest Pred'].loc[current_Bird['key']==key].values))] += 1\n",
        "  counter += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 ['kfgj_Nest8'] 30\n",
            "1 ['khxv_Nest3'] 48\n",
            "2 ['onsu_Nest7'] 28\n",
            "3 ['sdhp_Nest11'] 31\n",
            "4 ['nsrn_Nest2'] 38\n",
            "5 ['tbfk_Nest5'] 39\n",
            "6 ['oogw_Nest3'] 36\n",
            "7 ['kccr_Nest6'] 63\n",
            "8 ['inji_Nest9'] 21\n",
            "9 ['vstd_Nest11'] 30\n",
            "10 ['bbyj_Nest6'] 44\n",
            "11 ['qfod_Nest2'] 21\n",
            "12 ['cxyc_Nest2'] 27\n",
            "13 ['vusu_Nest7'] 29\n",
            "14 ['kcos_Nest4'] 32\n",
            "15 ['hsew_Nest10'] 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nONovcEdovU5"
      },
      "source": [
        "PupilID_Nest_df = pd.DataFrame(PupilID_Nest, index=PupilID_Nest_Names, columns=[class_label+'_Pred' for class_label in list(le.classes_)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "hteTWJONowVg",
        "outputId": "bdf9f784-a615-4e39-f380-c3245d36ae5f"
      },
      "source": [
        "PupilID_Nest_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Nest10_Pred</th>\n",
              "      <th>Nest11_Pred</th>\n",
              "      <th>Nest2_Pred</th>\n",
              "      <th>Nest3_Pred</th>\n",
              "      <th>Nest4_Pred</th>\n",
              "      <th>Nest5_Pred</th>\n",
              "      <th>Nest6_Pred</th>\n",
              "      <th>Nest7_Pred</th>\n",
              "      <th>Nest8_Pred</th>\n",
              "      <th>Nest9_Pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>kfgj_Nest8</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>khxv_Nest3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>onsu_Nest7</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sdhp_Nest11</th>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nsrn_Nest2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tbfk_Nest5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oogw_Nest3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kccr_Nest6</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>inji_Nest9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>vstd_Nest11</th>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bbyj_Nest6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>qfod_Nest2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cxyc_Nest2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>vusu_Nest7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kcos_Nest4</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hsew_Nest10</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Nest10_Pred  Nest11_Pred  ...  Nest8_Pred  Nest9_Pred\n",
              "kfgj_Nest8             3            0  ...           8           0\n",
              "khxv_Nest3             0            0  ...           0           0\n",
              "onsu_Nest7             0            1  ...           0          14\n",
              "sdhp_Nest11            0           11  ...           0           0\n",
              "nsrn_Nest2             0            0  ...           0           0\n",
              "tbfk_Nest5             0            0  ...           0           0\n",
              "oogw_Nest3             0            0  ...           0           0\n",
              "kccr_Nest6             0            1  ...           0           0\n",
              "inji_Nest9             0            0  ...           0           5\n",
              "vstd_Nest11            0           23  ...           0           0\n",
              "bbyj_Nest6             0            0  ...           0           0\n",
              "qfod_Nest2             0            0  ...           0           0\n",
              "cxyc_Nest2             0            0  ...           0           0\n",
              "vusu_Nest7             0            0  ...           0           0\n",
              "kcos_Nest4             0            2  ...           0           0\n",
              "hsew_Nest10            3            0  ...           0           0\n",
              "\n",
              "[16 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec1lfSuho80H"
      },
      "source": [
        "PupilID_Nest_df.to_csv('/content/gdrive/My Drive/ZFDataset/SavedModels/FullDataset/Baseline/'+'PupilID_Nest'+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R68j3zqCm_r5",
        "outputId": "75053eb0-c79d-4d99-fe79-e47cd79aa182"
      },
      "source": [
        "for _fold, (train_index, val_index) in enumerate(kf.split(range(len(X_total)))):\n",
        "  print(_fold)\n",
        "  best_model_test_loss = 0\n",
        "  best_model_test_acc = 0\n",
        "  X_train_subset = []; y_train_subset = [];\n",
        "  X_val_subset = []; y_val_subset = [];\n",
        "\n",
        "  for indx in train_index:\n",
        "    X_train_subset.append(X_total[indx])\n",
        "    y_train_subset.append(y_total[indx])\n",
        "  targets_train_subset = torch.as_tensor(le.transform(y_train_subset), dtype=torch.long)\n",
        "  print(Counter(targets_train_subset.numpy()))\n",
        "\n",
        "  for indx in val_index:\n",
        "    X_val_subset.append(X_total[indx])\n",
        "    y_val_subset.append(y_total[indx])\n",
        "  targets_val_subset = torch.as_tensor(le.transform(y_val_subset), dtype=torch.long)\n",
        "  print(Counter(targets_val_subset.numpy()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Counter({1: 2929, 0: 2570, 2: 1243, 4: 671, 3: 595})\n",
            "Counter({1: 411, 0: 256, 2: 148, 3: 58, 4: 17})\n",
            "1\n",
            "Counter({1: 3088, 0: 2464, 2: 1227, 4: 634, 3: 595})\n",
            "Counter({0: 362, 1: 252, 2: 164, 3: 58, 4: 54})\n",
            "2\n",
            "Counter({1: 2813, 0: 2668, 2: 1292, 4: 630, 3: 605})\n",
            "Counter({1: 527, 0: 158, 2: 99, 4: 58, 3: 48})\n",
            "3\n",
            "Counter({1: 3106, 0: 2420, 2: 1255, 4: 617, 3: 610})\n",
            "Counter({0: 406, 1: 234, 2: 136, 4: 71, 3: 43})\n",
            "4\n",
            "Counter({1: 2914, 0: 2602, 2: 1282, 4: 631, 3: 579})\n",
            "Counter({1: 426, 0: 224, 2: 109, 3: 74, 4: 57})\n",
            "5\n",
            "Counter({1: 2944, 0: 2598, 2: 1281, 4: 596, 3: 589})\n",
            "Counter({1: 396, 0: 228, 2: 110, 4: 92, 3: 64})\n",
            "6\n",
            "Counter({1: 3127, 0: 2481, 2: 1193, 4: 652, 3: 555})\n",
            "Counter({0: 345, 1: 213, 2: 198, 3: 98, 4: 36})\n",
            "7\n",
            "Counter({1: 2954, 0: 2681, 2: 1209, 3: 597, 4: 567})\n",
            "Counter({1: 386, 2: 182, 0: 145, 4: 121, 3: 56})\n",
            "8\n",
            "Counter({1: 3117, 0: 2461, 2: 1256, 4: 607, 3: 568})\n",
            "Counter({0: 365, 1: 223, 2: 135, 3: 85, 4: 81})\n",
            "9\n",
            "Counter({1: 3068, 0: 2489, 2: 1281, 4: 587, 3: 584})\n",
            "Counter({0: 337, 1: 272, 2: 110, 4: 101, 3: 69})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phz3dzUls-5-"
      },
      "source": [
        "train_target_occ = pd.DataFrame.from_dict(Counter(targets_train_subset.numpy()), orient='index').reset_index()\n",
        "train_target_occ = train_target_occ.rename(columns={'index':'Nest_encoding', 0:'count'}).sort_values(by=['Nest_encoding'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "qqliK3aOtf_a",
        "outputId": "1e910066-0470-41cf-a3f2-afc1f527b70f"
      },
      "source": [
        "train_target_occ"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Nest_encoding</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>407</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Nest_encoding  count\n",
              "0              0     95\n",
              "4              1    207\n",
              "2              2    757\n",
              "3              3    449\n",
              "1              4    407"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0Rz3IAH4uAd"
      },
      "source": [
        "class_weights = torch.tensor(train_target_occ['count'].min()/train_target_occ['count'].values, dtype=float).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVFAOIut44Ug",
        "outputId": "16e08931-486e-4c78-e03a-07439cdad8a0"
      },
      "source": [
        "class_weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000, 0.2334, 0.1255, 0.2116, 0.4589], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzMbMvulsc6H"
      },
      "source": [
        "x = Counter(targets_train_subset.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6bQ2MShsnWG",
        "outputId": "07f509b5-2f58-4814-bf34-6e069323debb"
      },
      "source": [
        "type(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "collections.Counter"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhecEqmsnC1m",
        "outputId": "80430dea-5859-4e32-f830-835c230cf780"
      },
      "source": [
        "targets_val_subset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
              "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
              "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
              "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "        3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3,\n",
              "        3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4,\n",
              "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3,\n",
              "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYhXpIBrN5lq"
      },
      "source": [
        "model = LinearClassifier(hidden_dim=512, output_dim=len(np.unique(targets)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-4NEbfuQJYK",
        "outputId": "a6207899-a067-4a1e-eccd-1b69fd758f97"
      },
      "source": [
        "model(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2675,  0.0148, -0.2978, -0.0174,  0.1020]],\n",
              "       grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIVY9daLWTL6"
      },
      "source": [
        "df_loss_accuracy_crossval = df_loss_accuracy_crossval.append({'Train Loss': epoch_best_loss , 'Train Accuracy': epoch_best_acc.to('cpu').numpy(),  'Val Loss':  epoch_best_val_loss, \n",
        "                                            'Val Accuracy': epoch_best_val_acc.to('cpu').numpy(),\n",
        "                                            'Train Seq Accuracy' : seq_train_accuracy, 'Test Seq Accuracy' : seq_test_accuracy,}, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "k5SRsJFqDU3w",
        "outputId": "4860a1d0-7a19-410c-d2bb-bee5d465ec97"
      },
      "source": [
        "df_loss_accuracy_crossval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train Loss</th>\n",
              "      <th>Train Accuracy</th>\n",
              "      <th>Val Loss</th>\n",
              "      <th>Val Accuracy</th>\n",
              "      <th>Test Loss</th>\n",
              "      <th>Test Accuracy</th>\n",
              "      <th>Test Seq Accuracy</th>\n",
              "      <th>Train Seq Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.009796</td>\n",
              "      <td>0.9963427</td>\n",
              "      <td>0.223050</td>\n",
              "      <td>0.9498956</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.798450</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.026011</td>\n",
              "      <td>0.992163</td>\n",
              "      <td>0.120890</td>\n",
              "      <td>0.9749478</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.850129</td>\n",
              "      <td>0.992481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.015224</td>\n",
              "      <td>0.9958202</td>\n",
              "      <td>0.327838</td>\n",
              "      <td>0.92693114</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.969925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.015741</td>\n",
              "      <td>0.99530023</td>\n",
              "      <td>0.381085</td>\n",
              "      <td>0.90794975</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.741602</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.008939</td>\n",
              "      <td>0.9963446</td>\n",
              "      <td>0.099384</td>\n",
              "      <td>0.9769874</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.640827</td>\n",
              "      <td>0.992481</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Train Loss Train Accuracy  ...  Test Seq Accuracy Train Seq Accuracy\n",
              "0    0.009796      0.9963427  ...           0.798450           1.000000\n",
              "1    0.026011       0.992163  ...           0.850129           0.992481\n",
              "2    0.015224      0.9958202  ...           0.777778           0.969925\n",
              "3    0.015741     0.99530023  ...           0.741602           1.000000\n",
              "4    0.008939      0.9963446  ...           0.640827           0.992481\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dw4Zbq4tE7R5",
        "outputId": "129633c0-c76d-4dfb-9ca4-7589c48be6b6"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/ZFDataset'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spmkeyKCE4WT"
      },
      "source": [
        "model = torch.load('/content/gdrive/My Drive/ZFDataset/SavedModels/Baseline_Fold0_12_05_2021_16_58TrainSeq_0.2932TestSeq_0.2171').to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXyYyXhGCFy6"
      },
      "source": [
        "def test_sequence_eval2(model, syllable_df, encoding):\n",
        "  sequence_prediction = []\n",
        "  actual_prediction = []\n",
        "  for key in encoding['key'].unique():# [:10]:\n",
        "    # print(key)\n",
        "    current_songfile = syllable_df.loc[syllable_df['key']==key]\n",
        "    sequence_length = current_songfile['indvi'].values[-1]\n",
        "    current_songfile = current_songfile.sample(frac=1, random_state=2021).reset_index(drop=True)\n",
        "    sequence_individual_segment = []\n",
        "    for i in range(0, sequence_length):\n",
        "      y_pred = model(torch.tensor(current_songfile['densenet121_features'].values[i]).resize_(1, 1024).to(device))\n",
        "      y_label_pred = torch.max(y_pred, 1)[1].to('cpu')\n",
        "      sequence_individual_segment.append(y_label_pred.numpy()[0])\n",
        "    sequence_prediction.append(np.bincount(sequence_individual_segment).argmax())\n",
        "    actual_prediction.append(le.transform(current_songfile['Nest'].values)[-1])\n",
        "    print(key, le.transform(current_songfile['Nest'].values)[-1])\n",
        "    print(sequence_individual_segment)\n",
        "  return accuracy_score(sequence_prediction, actual_prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUi2O8Luah4i",
        "outputId": "03949a79-1791-4cdd-d986-57222bab0d83"
      },
      "source": [
        "le.classes_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Nest1', 'Nest2', 'Nest3', 'Nest4', 'Nest5'], dtype='<U5')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "3BJfummUmpAX",
        "outputId": "0efb034c-6cdf-4ebc-92cc-4454003ccac2"
      },
      "source": [
        "num_occurences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>targets</th>\n",
              "      <th>indvi</th>\n",
              "      <th>key</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Nest1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nest5</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       targets  indvi  key\n",
              "Nest                      \n",
              "Nest1        1      1    7\n",
              "Nest2        1      1   17\n",
              "Nest3        1      1   39\n",
              "Nest4        1      1   34\n",
              "Nest5        1      1   36"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9hGgGPZml1w",
        "outputId": "d3e504bc-4226-4108-ed9e-03eaf174f801"
      },
      "source": [
        "class_weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000, 0.4118, 0.1795, 0.2059, 0.1944], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1_DjAooZ6g0",
        "outputId": "17477560-0adf-44f6-ea0a-4405a753465e"
      },
      "source": [
        "seq_train_accuracy = test_sequence_eval2(model, Tutor_dataset, encoded_targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ivoj_0004 0\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0021 4\n",
            "[2, 2, 2, 2, 2, 2]\n",
            "ivoj_0002 0\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0006 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0011 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0017 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0015 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0010 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0033 3\n",
            "[2, 2, 2, 2, 2, 2, 2]\n",
            "ivoj_0000 0\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0029 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "nzen_0006 1\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "nzen_0001 1\n",
            "[2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0025 3\n",
            "[2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0023 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0004 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0022 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0018 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "nzen_0015 1\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0014 4\n",
            "[2, 2, 2, 2, 2, 2]\n",
            "gthh_0001 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0005 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0014 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0032 4\n",
            "[2, 2, 2, 2, 2, 2]\n",
            "xsup_0021 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0026 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0011 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0001 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0008 4\n",
            "[2, 2, 2, 2, 2, 2]\n",
            "gthh_0006 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0028 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0011 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0013 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0031 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0020 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2]\n",
            "xsup_0036 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0032 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0026 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0009 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0035 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0032 3\n",
            "[2, 2, 2, 2, 2, 2]\n",
            "ttog_0025 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0018 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0010 4\n",
            "[2, 2, 2, 2, 2, 2]\n",
            "xsup_0037 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ivoj_0006 0\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0030 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0034 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0007 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2]\n",
            "xsup_0019 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0012 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ivoj_0003 0\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "nzen_0016 1\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0027 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "nzen_0013 1\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0008 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0027 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0015 3\n",
            "[2, 2, 2, 2, 2, 2]\n",
            "nzen_0011 1\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0002 3\n",
            "[2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0004 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0022 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0010 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0007 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0016 3\n",
            "[2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0009 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0029 3\n",
            "[2, 2, 2, 2, 2, 2]\n",
            "gthh_0024 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "nzen_0002 1\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0027 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0006 4\n",
            "[2, 2, 2, 2, 2, 2]\n",
            "xsup_0003 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0015 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0003 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2]\n",
            "nzen_0010 1\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0017 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0021 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "nzen_0004 1\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0029 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0013 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0016 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "nzen_0008 1\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0019 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0033 4\n",
            "[2, 2, 2, 2, 2]\n",
            "nzen_0012 1\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0012 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "nzen_0003 1\n",
            "[2, 2, 2, 2, 2, 2]\n",
            "xsup_0001 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0024 4\n",
            "[2, 2, 2, 2, 2, 2]\n",
            "xsup_0017 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0018 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0034 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0008 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "nzen_0007 1\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0014 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0035 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ivoj_0005 0\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0023 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0028 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0030 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0026 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "nzen_0009 1\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0025 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0000 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0002 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0024 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0003 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0002 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0016 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0000 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2]\n",
            "ttog_0013 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ivoj_0001 0\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "nzen_0000 1\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0004 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0009 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0020 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0005 4\n",
            "[2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0007 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "nzen_0005 1\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0000 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0030 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "nzen_0014 1\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0005 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0028 3\n",
            "[2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0038 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0020 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0023 3\n",
            "[2, 2, 2, 2, 2, 2]\n",
            "gthh_0031 3\n",
            "[2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0012 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0031 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "xsup_0033 2\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "gthh_0022 3\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "ttog_0019 4\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o_zP9dNm2Fn"
      },
      "source": [
        "train_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f6ZC_0vZ-QS",
        "outputId": "afaecc0f-d93c-49f2-d075-21372eba1d83"
      },
      "source": [
        "seq_train_accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.046511627906976744"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZ8X8vbrev-a",
        "outputId": "9ea8c6ca-fdac-437f-f1a0-fa663c63a326"
      },
      "source": [
        "seq_test_accuracy = test_sequence_eval(model, Pupil_dataset, encoded_targets_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cyea_0000\n",
            "cyea_0000 0\n",
            "[36, 126, 3, 36, 96, 3, 96, 36]\n",
            "cyea_0001\n",
            "cyea_0001 0\n",
            "[36, 36, 36, 36, 36, 3, 96, 36]\n",
            "cyea_0002\n",
            "cyea_0002 0\n",
            "[36, 36, 3, 36, 36, 3, 96, 36]\n",
            "cyea_0003\n",
            "cyea_0003 0\n",
            "[36, 96, 3, 3, 36, 96, 36, 36, 36]\n",
            "cyea_0004\n",
            "cyea_0004 0\n",
            "[4, 36, 3, 36, 96, 36, 36, 36, 96, 96, 36, 110, 36]\n",
            "cyea_0005\n",
            "cyea_0005 0\n",
            "[36, 36, 36, 36, 36, 53, 96, 3]\n",
            "cyea_0006\n",
            "cyea_0006 0\n",
            "[36, 3, 36, 36, 36, 3, 96, 117]\n",
            "cyea_0007\n",
            "cyea_0007 0\n",
            "[36, 36, 36, 36, 4, 3, 96, 36]\n",
            "cyea_0008\n",
            "cyea_0008 0\n",
            "[36, 36, 36, 36, 4, 53, 96, 36]\n",
            "cyea_0009\n",
            "cyea_0009 0\n",
            "[36, 96, 3, 3, 36, 96, 62, 36, 36]\n",
            "cyea_0010\n",
            "cyea_0010 0\n",
            "[36, 36, 3, 3, 62, 3, 96, 36]\n",
            "cyea_0011\n",
            "cyea_0011 0\n",
            "[36, 36, 3, 36, 36, 3, 96, 36]\n",
            "cyea_0012\n",
            "cyea_0012 0\n",
            "[3, 3, 36, 36, 4, 36]\n",
            "cyea_0013\n",
            "cyea_0013 0\n",
            "[36, 36, 36, 36, 4, 3, 96, 110]\n",
            "cyea_0014\n",
            "cyea_0014 0\n",
            "[36, 3, 36, 4, 53, 96, 110]\n",
            "cyea_0015\n",
            "cyea_0015 0\n",
            "[3, 36, 36, 36, 36]\n",
            "cyea_0016\n",
            "cyea_0016 0\n",
            "[36, 36, 36, 36, 62, 3, 96, 36]\n",
            "cyea_0017\n",
            "cyea_0017 0\n",
            "[36, 96, 3, 36, 36, 96, 36, 110, 36]\n",
            "cyea_0018\n",
            "cyea_0018 0\n",
            "[3, 96, 3, 36, 36, 36, 36, 110, 36]\n",
            "cyea_0019\n",
            "cyea_0019 0\n",
            "[53, 36, 36, 36, 4, 3, 96, 36]\n",
            "cyea_0020\n",
            "cyea_0020 0\n",
            "[36, 36, 3, 3, 36, 3, 96, 110]\n",
            "cyea_0021\n",
            "cyea_0021 0\n",
            "[36, 36, 3, 3, 4, 3, 96, 36]\n",
            "cyea_0022\n",
            "cyea_0022 0\n",
            "[3, 36, 36, 36, 96, 3, 96, 36]\n",
            "cyea_0023\n",
            "cyea_0023 0\n",
            "[36, 36, 36, 3, 62, 36, 96, 117]\n",
            "cyea_0024\n",
            "cyea_0024 0\n",
            "[3, 4, 36, 3, 36]\n",
            "cyea_0025\n",
            "cyea_0025 0\n",
            "[36, 3, 36, 36, 36, 36, 3, 96, 110]\n",
            "cyea_0026\n",
            "cyea_0026 0\n",
            "[36, 96, 36, 36, 36, 4, 96, 3, 36]\n",
            "cyea_0027\n",
            "cyea_0027 0\n",
            "[36, 36, 3, 3, 36, 3, 96, 36]\n",
            "cyea_0028\n",
            "cyea_0028 0\n",
            "[3, 96, 3, 36, 36, 96, 4, 36, 36]\n",
            "cyea_0029\n",
            "cyea_0029 0\n",
            "[36, 36, 36, 3, 4, 3, 96, 36]\n",
            "cyea_0030\n",
            "cyea_0030 0\n",
            "[36, 96, 3, 3, 36, 36, 4, 36, 36]\n",
            "cyea_0031\n",
            "cyea_0031 0\n",
            "[3, 96, 3, 36, 36, 36, 36, 110, 36]\n",
            "cyea_0032\n",
            "cyea_0032 0\n",
            "[3, 96, 3, 36, 36, 36, 36, 36, 36]\n",
            "cyea_0033\n",
            "cyea_0033 0\n",
            "[36, 36, 3, 36, 4, 53, 96, 110]\n",
            "cyea_0034\n",
            "cyea_0034 0\n",
            "[96, 36, 36, 3, 96, 3, 36, 36, 36, 36, 117, 36, 36]\n",
            "cyea_0035\n",
            "cyea_0035 0\n",
            "[96, 36, 3, 36, 36, 36, 36, 36, 36, 36]\n",
            "cyea_0036\n",
            "cyea_0036 0\n",
            "[3, 36, 36, 3, 36, 3, 96, 36]\n",
            "cyea_0037\n",
            "cyea_0037 0\n",
            "[96, 3, 53, 36, 36, 36, 4, 96, 36, 36]\n",
            "cyea_0038\n",
            "cyea_0038 0\n",
            "[36, 36, 3, 3, 96, 36, 36, 36, 96, 36, 36, 110, 36]\n",
            "cyea_0039\n",
            "cyea_0039 0\n",
            "[3, 36, 36, 36, 36, 36, 3, 96, 36]\n",
            "cyea_0040\n",
            "cyea_0040 0\n",
            "[96, 36, 3, 36, 36, 36, 36, 36, 36, 36]\n",
            "hphi_0000\n",
            "hphi_0000 0\n",
            "[36, 3, 36, 96, 36, 53, 4, 36, 3, 36, 36, 3, 3, 3, 36, 36, 53]\n",
            "hphi_0001\n",
            "hphi_0001 0\n",
            "[36, 36, 36, 36, 96, 36, 4, 36, 3, 96, 3, 36, 112, 36, 36, 36, 3, 36, 36, 53, 36]\n",
            "hphi_0002\n",
            "hphi_0002 0\n",
            "[36, 96, 96, 53, 36, 36, 3, 36, 53, 36, 3, 36]\n",
            "hphi_0003\n",
            "hphi_0003 0\n",
            "[36, 36, 36, 36, 36, 36, 3, 36, 53, 36, 36, 36]\n",
            "hphi_0004\n",
            "hphi_0004 0\n",
            "[36, 36, 36, 3, 36, 36, 3, 96, 3, 36, 36, 96, 3, 53, 36, 3, 3, 36, 96, 36, 53, 3]\n",
            "hphi_0005\n",
            "hphi_0005 0\n",
            "[4, 96, 36, 36, 36, 53, 36, 4, 3, 36, 36, 36, 36, 3, 36, 36]\n",
            "hphi_0006\n",
            "hphi_0006 0\n",
            "[36, 36, 36, 3, 112, 3, 36, 3, 36, 36, 36, 53, 4, 36, 36, 36, 96, 96, 36, 96, 36]\n",
            "hphi_0007\n",
            "hphi_0007 0\n",
            "[36, 3, 53, 4, 36, 96, 53, 96, 36, 53, 36, 36, 36, 36, 36, 36, 53, 36, 4, 36, 112, 3, 36, 3, 3, 53, 36, 36, 36]\n",
            "hphi_0008\n",
            "hphi_0008 0\n",
            "[36, 96, 36, 96, 3, 53, 96, 36, 36, 3, 36, 3, 53, 36, 36, 36]\n",
            "hphi_0009\n",
            "hphi_0009 0\n",
            "[36, 36, 36, 96, 112, 36, 3, 36, 36, 36, 36]\n",
            "hphi_0010\n",
            "hphi_0010 0\n",
            "[36, 36, 36, 36, 4, 96, 3, 96, 112, 53, 3, 3, 96, 112, 36, 36, 3, 3, 53, 36, 4, 36, 36, 36, 36, 36, 3, 36, 53, 36, 36, 36]\n",
            "hphi_0011\n",
            "hphi_0011 0\n",
            "[36, 96, 36, 36, 3, 36, 36, 36, 36, 4, 36, 36, 36, 36, 96, 36, 53, 36, 36, 3, 96, 3, 36, 3, 36, 36, 36, 23, 36]\n",
            "hphi_0012\n",
            "hphi_0012 0\n",
            "[36, 3, 36, 112, 3, 3, 36, 96, 36, 3, 112, 36, 36, 36, 36, 36, 36, 36, 36, 53, 36, 96, 36, 36, 36, 36, 53, 3, 36, 36, 36, 36, 53, 36]\n",
            "hphi_0013\n",
            "hphi_0013 0\n",
            "[36, 3, 36, 36, 36, 36, 36, 3, 36, 4, 36, 3, 36, 36, 4, 36, 110]\n",
            "hphi_0014\n",
            "hphi_0014 0\n",
            "[36, 3, 4, 53, 36, 36, 36, 4, 36, 36, 36, 36, 36, 36, 36, 53, 53]\n",
            "hphi_0015\n",
            "hphi_0015 0\n",
            "[36, 36, 36, 4, 36, 3, 36, 96, 36, 36, 36, 36, 36, 3, 36, 3, 36, 96, 36, 96, 36, 3, 96, 36, 36, 36, 36, 36, 36, 53, 36, 3, 3, 36, 36, 36, 36, 36, 3, 3, 36, 53, 3]\n",
            "hphi_0016\n",
            "hphi_0016 0\n",
            "[36, 96, 36, 96, 36, 96, 36, 36, 3, 3, 4, 53, 36, 3, 36]\n",
            "hphi_0017\n",
            "hphi_0017 0\n",
            "[96, 3, 36, 53, 36, 53, 36, 36, 36, 36, 36, 3, 3, 3, 36, 96, 53]\n",
            "hphi_0018\n",
            "hphi_0018 0\n",
            "[3, 36, 36, 53, 36, 36, 3, 4, 3, 53, 36, 36, 36, 36, 36, 4, 53, 36, 3, 36, 36, 112, 36, 3, 53, 96, 36, 96]\n",
            "hphi_0019\n",
            "hphi_0019 0\n",
            "[36, 36, 36, 36, 36, 96, 3, 4, 36, 3, 36, 36, 36, 53, 36, 3, 36, 3, 112, 36, 53, 3, 3]\n",
            "hphi_0020\n",
            "hphi_0020 0\n",
            "[36, 53, 4, 36, 36, 36, 3, 36, 96, 36, 53, 3, 3, 36, 36, 36, 36, 36, 3, 96, 96, 36, 36]\n",
            "hphi_0021\n",
            "hphi_0021 0\n",
            "[36, 36, 3, 36, 36, 4, 36, 36, 36, 36, 36, 3, 36, 36, 112, 112, 36, 53, 36, 53, 3, 3, 3, 36, 36, 36, 53, 112, 3, 36]\n",
            "hphi_0022\n",
            "hphi_0022 0\n",
            "[36, 36, 36, 36, 96, 36, 36, 53, 3, 36, 36, 36, 36, 53, 36, 36, 3, 36, 3, 112, 53, 3]\n",
            "hphi_0023\n",
            "hphi_0023 0\n",
            "[36, 96, 36, 36, 3, 53, 36, 3, 36, 53, 36, 36, 3, 36, 36, 3, 36, 53]\n",
            "hphi_0024\n",
            "hphi_0024 0\n",
            "[112, 4, 36, 53, 4, 36, 36, 3, 36, 36, 36, 36]\n",
            "hphi_0025\n",
            "hphi_0025 0\n",
            "[36, 36, 96, 36, 36, 36, 36, 53, 36, 36, 96, 3, 53, 36, 3, 36, 36, 36, 3, 53, 36, 4, 3, 53, 36, 36, 36, 96, 3, 96, 53, 36, 36]\n",
            "hphi_0026\n",
            "hphi_0026 0\n",
            "[36, 96, 3, 4, 36, 4, 36, 36, 53, 3, 36, 112, 3, 36, 53, 3]\n",
            "hphi_0027\n",
            "hphi_0027 0\n",
            "[53, 4, 36, 36, 96, 36, 36, 53, 36, 36, 36, 3, 36, 96, 96, 36, 36, 3, 3, 96, 96, 96, 53]\n",
            "hphi_0028\n",
            "hphi_0028 0\n",
            "[36, 36, 36, 36, 36, 3, 3, 96, 36, 36, 36, 36, 3, 96, 36, 3, 3, 112, 96, 36, 53, 3]\n",
            "hphi_0029\n",
            "hphi_0029 0\n",
            "[53, 36, 36, 4, 36, 96, 36, 4, 36, 53, 36, 36, 36, 36, 36, 53, 36, 96, 36, 96, 36, 96, 53, 36, 36]\n",
            "hphi_0030\n",
            "hphi_0030 0\n",
            "[36, 112, 36, 36, 112, 36, 36, 36, 53, 36, 4, 4, 3, 112, 36, 53, 36, 96, 36, 112, 53, 36]\n",
            "hphi_0031\n",
            "hphi_0031 0\n",
            "[3, 36, 36, 112, 3, 36, 36, 36, 36, 4, 3, 36, 96, 3, 96, 3, 53, 112, 36, 3, 36, 36, 36, 53, 36, 3]\n",
            "hphi_0032\n",
            "hphi_0032 0\n",
            "[36, 36, 4, 3, 36, 36, 3, 36, 36, 36, 96, 3, 36, 112, 36, 36, 112, 53, 36, 36, 36, 36, 53, 53, 112, 36, 96, 96, 3, 53, 36, 36, 36, 36, 36, 53, 4]\n",
            "hphi_0033\n",
            "hphi_0033 0\n",
            "[36, 4, 36, 36, 110, 3, 96, 3, 36, 36, 36, 4, 36, 112, 3, 36, 3, 36, 36, 36, 36, 3, 4, 96]\n",
            "hphi_0034\n",
            "hphi_0034 0\n",
            "[36, 3, 112, 36, 36, 36, 4, 36, 36, 36, 36, 3, 36, 3, 36, 112, 112, 36, 36, 4, 36, 112, 53, 117, 36, 4, 36, 36, 3, 36, 36, 96, 3, 3, 53, 36]\n",
            "hphi_0035\n",
            "hphi_0035 0\n",
            "[36, 36, 36, 36, 36, 36, 3, 96, 36, 36, 36, 36, 3, 36, 36, 3, 36, 36, 36, 36, 3, 36]\n",
            "hphi_0036\n",
            "hphi_0036 0\n",
            "[36, 3, 53, 96, 36, 53, 36, 36, 36, 53, 36, 36, 3, 96, 3, 112, 36, 53]\n",
            "hphi_0037\n",
            "hphi_0037 0\n",
            "[112, 36, 112, 36, 112, 3, 36, 36, 53, 36, 36]\n",
            "hphi_0038\n",
            "hphi_0038 0\n",
            "[36, 3, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 3, 53, 36, 36, 36, 36, 96, 3, 4, 53, 36, 36, 36]\n",
            "hphi_0039\n",
            "hphi_0039 0\n",
            "[36, 36, 4, 53, 36, 36, 36, 36, 96, 36, 36, 36]\n",
            "hphi_0040\n",
            "hphi_0040 0\n",
            "[112, 36, 36, 96, 36, 53, 96, 4, 36, 36, 36, 36, 96, 36, 36, 36]\n",
            "hphi_0041\n",
            "hphi_0041 0\n",
            "[53, 36, 36, 4, 36, 4, 36, 53, 3, 53, 36, 36, 53]\n",
            "hphi_0042\n",
            "hphi_0042 0\n",
            "[4, 36, 112, 96, 36, 36, 36, 36, 3, 36, 36, 36, 36, 36, 36, 36, 4, 36]\n",
            "hphi_0043\n",
            "hphi_0043 0\n",
            "[36, 53, 3, 96, 3, 36, 36, 36, 3, 36, 36, 36, 36, 36, 36, 36, 36, 53]\n",
            "phpd_0000\n",
            "phpd_0000 0\n",
            "[53, 36, 36, 36, 36, 36, 3]\n",
            "phpd_0001\n",
            "phpd_0001 0\n",
            "[4, 3, 3, 36, 36, 36]\n",
            "phpd_0002\n",
            "phpd_0002 0\n",
            "[36, 112, 36, 36, 112, 112, 36, 4, 3, 36, 110, 36, 4, 36, 36]\n",
            "phpd_0003\n",
            "phpd_0003 0\n",
            "[36, 36, 36, 36, 36, 36, 3, 36, 36, 36, 36, 36]\n",
            "phpd_0004\n",
            "phpd_0004 0\n",
            "[36, 36, 36, 36, 36, 36, 36, 3, 36, 36, 4, 3, 3, 36, 36, 36, 36]\n",
            "phpd_0005\n",
            "phpd_0005 0\n",
            "[36, 36, 36, 4, 112, 36, 3, 36, 4, 36, 36, 36, 36, 36, 36, 36, 36]\n",
            "phpd_0006\n",
            "phpd_0006 0\n",
            "[36, 4, 36, 36, 36, 36, 3, 36, 36, 36, 3, 36, 36, 36, 36, 3, 36, 96, 36, 36]\n",
            "phpd_0007\n",
            "phpd_0007 0\n",
            "[36, 36, 36, 36, 96, 36, 36, 53, 36, 36, 36, 36, 36, 36, 36, 36, 3, 36, 36, 36, 36]\n",
            "phpd_0008\n",
            "phpd_0008 0\n",
            "[36, 36, 36, 3, 36, 36, 36, 36, 36, 36, 36, 36, 112, 36, 4, 36, 36, 3, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 112, 36, 36, 36, 112, 36, 36, 36, 3, 36, 36, 3, 36, 36, 112, 36, 112, 36, 36, 36, 36, 36, 36, 36, 36, 53, 36]\n",
            "phpd_0009\n",
            "phpd_0009 0\n",
            "[36, 36, 36, 36, 36, 4, 3, 112, 110, 36, 36, 53, 36, 36, 36, 3, 112, 36, 36, 36, 36, 36, 36, 36, 36, 36, 53, 36, 36, 36, 36, 36]\n",
            "phpd_0010\n",
            "phpd_0010 0\n",
            "[36, 110, 36, 36, 36, 36, 3, 36, 3, 36, 36, 36, 36, 3, 36, 36, 36, 36]\n",
            "phpd_0011\n",
            "phpd_0011 0\n",
            "[36, 36, 36, 4, 36, 4, 36, 36, 3, 3, 36, 3, 36, 36, 36, 36]\n",
            "phpd_0012\n",
            "phpd_0012 0\n",
            "[36, 36, 36, 36, 36, 36, 36, 4, 36, 3, 36, 36]\n",
            "phpd_0013\n",
            "phpd_0013 0\n",
            "[112, 36, 3, 36, 4, 36, 36, 3, 36, 36, 36, 112, 36, 36, 36, 36, 36, 36, 36, 36, 36, 3, 36, 36, 112, 36, 36, 36, 36, 36, 36]\n",
            "phpd_0014\n",
            "phpd_0014 0\n",
            "[3, 36, 36, 4, 4, 36, 36, 3, 3, 112, 53, 3, 36, 4, 36, 4, 36, 112, 36, 36, 3, 36, 36, 36, 36, 36, 36, 36, 36, 36]\n",
            "phpd_0015\n",
            "phpd_0015 0\n",
            "[36, 36, 36, 3, 36, 36, 36, 36, 36, 4, 36, 36, 36, 36, 36, 36, 36, 4, 112, 36, 36, 36, 4, 36, 36, 36, 36, 36, 36, 36, 3, 112, 36, 36, 36, 36, 36, 4, 36, 36, 3, 36, 36, 36, 112, 36, 3, 36, 36, 112, 3, 3, 36, 36, 36, 36, 36, 36, 36]\n",
            "phpd_0016\n",
            "phpd_0016 0\n",
            "[112, 3, 4, 4, 4, 36, 4, 3, 36, 36, 36, 36, 36, 4, 36, 36, 4, 4, 3, 112]\n",
            "phpd_0017\n",
            "phpd_0017 0\n",
            "[4, 36, 4, 36, 36, 36, 4, 4, 3, 36, 53, 3, 4, 3, 112, 112, 112, 4, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 3, 36, 3, 36, 36, 36, 36, 3, 36, 3, 3, 36, 4, 36, 36, 36, 36, 36, 36, 36, 36, 4, 4, 112, 4, 36, 36, 36, 36, 112, 36, 36, 36, 36, 3, 36, 3, 4, 3, 36, 36, 36, 36, 36, 36, 36]\n",
            "phpd_0018\n",
            "phpd_0018 0\n",
            "[4, 4, 3, 36, 36, 36, 36, 4, 112, 3, 36, 36]\n",
            "phpd_0019\n",
            "phpd_0019 0\n",
            "[3, 36, 3, 4, 36, 36, 36, 36, 36, 112, 36, 3, 36, 36, 4, 3, 4, 36, 36, 36, 36, 36, 36, 36, 112, 112, 4, 36, 36, 36, 112, 36, 36, 3, 3, 36, 36, 36, 36, 3, 36, 4, 36, 36, 36, 36, 36]\n",
            "phpd_0020\n",
            "phpd_0020 0\n",
            "[4, 36, 112, 3, 36, 36, 4, 36, 36, 3, 112, 36, 112, 36, 3, 112, 36, 36, 36, 36, 36, 36, 4, 36, 53, 36, 36, 36, 36, 4, 36, 53, 112, 36, 36, 36]\n",
            "phpd_0021\n",
            "phpd_0021 0\n",
            "[4, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 4, 36, 112, 36, 36, 3, 112, 112, 36, 36, 36]\n",
            "phpd_0022\n",
            "phpd_0022 0\n",
            "[4, 4, 36, 36, 4, 36, 36, 36, 112, 36, 36, 112, 4, 36, 36, 4, 36, 36, 112, 4, 36, 4, 112, 112, 36, 112, 4, 4, 3, 112, 36, 36, 96, 36, 36, 36, 36, 36, 36, 3, 36, 36, 36, 3, 4, 3, 36, 36, 36, 36, 112, 69, 36, 3, 3, 112, 3, 3, 36, 3, 36, 4, 3, 36, 36, 3, 36, 36, 36, 36, 36, 36, 36, 112, 36, 3, 36, 36, 69, 36, 36, 36, 3, 36, 36, 36, 36, 36, 36, 4, 36, 3, 3, 36, 36, 3, 4, 36, 36, 36, 36, 3, 4, 36, 36, 4, 36, 36, 4, 36, 36, 36, 3, 36, 36, 36, 36, 4, 36, 36, 36, 3, 36, 36, 36, 36, 36, 4, 36, 36, 36, 3, 112, 36, 3, 36, 4, 3, 36, 112, 36, 96, 36, 36, 36, 4, 3, 36, 36, 36, 36, 3, 36]\n",
            "phpd_0023\n",
            "phpd_0023 0\n",
            "[4, 112, 3, 36, 36, 4, 3, 112, 112, 4, 36, 36, 3, 36, 3, 3, 36, 36, 112, 36, 36, 36, 36, 3, 4, 112, 36, 36, 3, 4]\n",
            "phpd_0024\n",
            "phpd_0024 0\n",
            "[36, 36, 36, 36, 36, 36, 3, 112, 36, 4, 36, 36, 36, 36, 4, 36, 36, 4, 36, 112, 36, 36]\n",
            "phpd_0025\n",
            "phpd_0025 0\n",
            "[3, 36, 36, 36, 4, 36, 36, 36, 36, 36, 4, 36, 53, 36, 4, 96, 3, 3, 36, 36, 36, 36, 36, 36, 4, 36, 112, 36, 3, 36, 36, 36, 3, 3, 4, 36, 36]\n",
            "phpd_0026\n",
            "phpd_0026 0\n",
            "[36, 36, 4, 3, 36, 112, 36, 36, 53, 36, 3, 3, 36, 36, 96, 36, 112, 53, 36, 36, 4, 36, 36, 36, 36, 36, 36, 112, 36, 36, 112, 36, 4, 3, 36, 3, 36, 36]\n",
            "phpd_0027\n",
            "phpd_0027 0\n",
            "[112, 36, 36, 4, 36, 4, 36]\n",
            "phpd_0028\n",
            "phpd_0028 0\n",
            "[36, 36, 36, 36, 36, 36, 53, 112, 36, 36, 36, 3, 36, 36, 36, 36, 36, 3, 36, 53, 96, 36]\n",
            "phpd_0029\n",
            "phpd_0029 0\n",
            "[4, 112, 3, 36, 36, 3, 36, 36, 36, 112, 3, 3, 36, 112, 112, 3, 112, 112, 36, 3, 36, 112, 3, 36, 36, 36]\n",
            "phpd_0030\n",
            "phpd_0030 0\n",
            "[4, 112, 4, 36, 36, 36, 112, 36, 36, 36, 36, 36, 3, 3, 36, 36, 3, 36, 36, 112, 112, 53, 36, 36, 36, 3, 36, 36, 36, 3, 36, 36, 36, 36, 36, 36, 36, 3, 36, 36, 36, 53]\n",
            "phpd_0031\n",
            "phpd_0031 0\n",
            "[53, 36, 110, 36, 36, 36, 36, 4, 36, 3, 4, 3]\n",
            "phpd_0032\n",
            "phpd_0032 0\n",
            "[4, 36, 110, 36, 36, 53, 36, 36, 36, 3, 36, 3]\n",
            "phpd_0033\n",
            "phpd_0033 0\n",
            "[36, 36, 53, 4, 36, 3, 36, 4, 36, 53, 36]\n",
            "phpd_0034\n",
            "phpd_0034 0\n",
            "[112, 36, 3, 36, 36, 36, 36, 4, 36, 3, 4, 36]\n",
            "phpd_0035\n",
            "phpd_0035 0\n",
            "[36, 36, 36, 36, 36, 96, 36, 36, 110, 36, 4, 3, 36, 36, 36, 36]\n",
            "phpd_0036\n",
            "phpd_0036 0\n",
            "[112, 36, 36, 36, 4, 36, 36, 96, 36, 36]\n",
            "phpd_0037\n",
            "phpd_0037 0\n",
            "[36, 96, 96, 36, 36, 3, 3, 4, 36, 36, 4]\n",
            "phpd_0038\n",
            "phpd_0038 0\n",
            "[36, 36, 36, 36, 36, 36, 36, 3, 96, 36, 36, 36, 36, 3, 112, 36, 62, 3, 3, 96, 36, 36, 53, 36]\n",
            "phpd_0039\n",
            "phpd_0039 0\n",
            "[36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 110, 36, 36, 36, 36, 36, 36, 3, 36, 36, 3, 36, 4, 36, 36, 96, 4, 112, 36, 96, 96]\n",
            "phpd_0040\n",
            "phpd_0040 0\n",
            "[36, 36, 36, 36, 36, 36, 3]\n",
            "phpd_0041\n",
            "phpd_0041 0\n",
            "[112, 36, 36, 36, 36, 4, 3]\n",
            "phpd_0042\n",
            "phpd_0042 0\n",
            "[36, 36, 3, 36, 4, 53, 4]\n",
            "phpd_0043\n",
            "phpd_0043 0\n",
            "[36, 3, 4, 36, 36, 36, 3, 36, 4, 53, 3, 36, 112, 36, 36, 112, 4, 3, 53, 36, 36, 36, 36, 36, 4]\n",
            "phpd_0044\n",
            "phpd_0044 0\n",
            "[36, 4, 3, 36, 4, 36, 4, 3, 112, 36, 36, 53, 3, 36, 36, 36, 36, 4, 36, 3, 3, 96, 36, 36, 3, 36]\n",
            "phpd_0045\n",
            "phpd_0045 0\n",
            "[4, 4, 36, 62, 36, 36, 53, 36, 36, 36, 4, 36]\n",
            "phpd_0046\n",
            "phpd_0046 0\n",
            "[62, 4, 36, 36, 3, 96, 3, 3, 4, 36, 36, 36, 96]\n",
            "phpd_0047\n",
            "phpd_0047 0\n",
            "[112, 53, 36, 110, 4, 96, 53, 36, 36, 112, 3, 4, 36]\n",
            "phpd_0048\n",
            "phpd_0048 0\n",
            "[36, 36, 36, 4, 4, 36, 3, 4]\n",
            "phpd_0049\n",
            "phpd_0049 0\n",
            "[96, 36, 4, 96, 36, 96, 36, 36, 3, 36, 3, 36, 36, 36, 4, 36]\n",
            "phpd_0050\n",
            "phpd_0050 0\n",
            "[36, 36, 36, 36, 36, 36, 36, 3, 3, 4, 36, 53, 3, 4, 53, 36, 96, 4, 3, 36, 36]\n",
            "phpd_0051\n",
            "phpd_0051 0\n",
            "[112, 4, 36, 3, 53, 36, 3, 3, 96, 112, 3, 4, 3, 36, 36, 36, 36, 53, 36, 3, 96, 36, 36, 36, 36, 4, 4, 3, 96, 3]\n",
            "phpd_0052\n",
            "phpd_0052 0\n",
            "[3, 36, 36, 3, 36, 4, 3, 36, 4, 112, 4, 3, 36, 36, 4, 3, 4, 36, 4, 36, 3, 112, 3, 96, 96, 4, 36, 112, 3, 36, 3, 36, 3, 3, 36, 36, 36, 112, 3, 53, 112, 36, 36, 4, 36, 4]\n",
            "phpd_0053\n",
            "phpd_0053 0\n",
            "[96, 36, 36, 3, 96, 3, 36, 4, 3, 36, 36, 36, 4, 36, 36, 36, 36, 96, 36, 4]\n",
            "phpd_0054\n",
            "phpd_0054 0\n",
            "[3, 36, 36, 36, 36, 53, 36, 3, 36, 3, 96, 3, 36, 36, 36, 36, 112, 36, 36, 3, 36, 110, 36, 36, 36, 36, 3, 3, 36, 36, 36, 36, 36, 4, 112, 36, 4, 36]\n",
            "phpd_0055\n",
            "phpd_0055 0\n",
            "[36, 36, 4, 36, 3, 36, 36, 36, 112, 36, 36, 53, 3, 96, 3, 3, 36, 36, 36, 36]\n",
            "phpd_0056\n",
            "phpd_0056 0\n",
            "[36, 4, 36, 112, 96, 4, 36, 3, 96, 36, 36, 96, 36, 4, 36, 3, 36, 96, 36, 36, 36]\n",
            "phpd_0057\n",
            "phpd_0057 0\n",
            "[36, 36, 36, 3, 3, 36, 4, 96, 36, 4, 36, 4, 3, 53, 3, 96, 36, 36, 36, 36, 3, 36, 96, 96, 36, 4]\n",
            "phpd_0058\n",
            "phpd_0058 0\n",
            "[53, 3, 36, 36, 4, 3, 36, 36, 36, 110, 36, 36, 36, 36, 36, 36, 36, 36, 96, 36]\n",
            "phpd_0059\n",
            "phpd_0059 0\n",
            "[36, 36, 3, 36, 96, 4, 36, 36, 36, 4, 36]\n",
            "phpd_0060\n",
            "phpd_0060 0\n",
            "[96, 36, 36, 4, 4, 36, 36, 36, 36, 36, 3, 36, 36, 36, 4, 36, 36, 36]\n",
            "phpd_0061\n",
            "phpd_0061 0\n",
            "[36, 36, 36, 53, 36, 36, 3, 36, 36, 4, 36, 36, 36]\n",
            "phpd_0062\n",
            "phpd_0062 0\n",
            "[36, 36, 36, 96, 4]\n",
            "cxyc_0000\n",
            "cxyc_0000 1\n",
            "[112, 36, 36, 36, 112, 112, 3, 0, 36, 112, 112, 112, 36, 36, 36, 112, 36, 36, 36, 112]\n",
            "cxyc_0001\n",
            "cxyc_0001 1\n",
            "[112, 0, 69, 112, 112, 69, 112, 112, 36, 3, 36, 112, 112, 112, 36, 112, 112, 69, 36, 36, 36, 112]\n",
            "cxyc_0002\n",
            "cxyc_0002 1\n",
            "[112, 69, 112, 112, 0, 112, 69, 112, 23, 69, 36, 112, 0, 3, 112, 112, 112, 112]\n",
            "cxyc_0003\n",
            "cxyc_0003 1\n",
            "[3, 69, 36, 112, 36, 3, 36, 112, 36, 112, 0, 36, 36, 112, 112, 112, 0, 99, 0, 112, 36, 112, 112, 112, 112, 112, 112, 36]\n",
            "cxyc_0004\n",
            "cxyc_0004 1\n",
            "[0, 36, 36, 112, 36, 36, 0, 69, 99]\n",
            "cxyc_0005\n",
            "cxyc_0005 1\n",
            "[112, 36, 112, 112, 69, 112, 112, 36, 112, 69, 69, 112, 36, 36, 36, 69, 84, 3]\n",
            "cxyc_0006\n",
            "cxyc_0006 1\n",
            "[36, 112, 36, 36, 112, 0, 36, 112, 36, 36, 112, 112, 0, 36, 0, 112, 112, 112, 69, 112]\n",
            "cxyc_0007\n",
            "cxyc_0007 1\n",
            "[36, 112, 36, 112, 112, 36, 69, 112, 112, 112, 3, 36, 36, 3, 36, 36, 112, 0, 69, 36, 112, 112, 3, 112, 36, 36]\n",
            "cxyc_0008\n",
            "cxyc_0008 1\n",
            "[112, 23, 0, 102, 0, 112, 23, 36, 3, 112, 112, 112, 112, 23, 36, 23, 112, 112]\n",
            "cxyc_0009\n",
            "cxyc_0009 1\n",
            "[112, 0, 112, 36, 36, 112, 0, 36, 0, 112, 36, 0, 36]\n",
            "cxyc_0010\n",
            "cxyc_0010 1\n",
            "[36, 0, 0, 36, 36, 36, 36, 69, 36, 0]\n",
            "cxyc_0011\n",
            "cxyc_0011 1\n",
            "[112, 112, 36, 112, 36, 36, 112, 0, 36, 36, 69, 36, 112, 36, 112, 112, 112, 36, 36, 36, 102, 112, 36, 36]\n",
            "cxyc_0012\n",
            "cxyc_0012 1\n",
            "[0, 69, 36, 112, 112, 112, 36, 112, 0, 0, 112, 36, 112, 36, 112, 36]\n",
            "cxyc_0013\n",
            "cxyc_0013 1\n",
            "[112, 69, 69, 36, 112, 112, 23, 0, 112, 112, 112, 112, 112, 112, 36, 69, 112]\n",
            "cxyc_0014\n",
            "cxyc_0014 1\n",
            "[0, 36, 36, 36, 112, 69, 69, 112, 36, 112, 99, 112, 112, 69]\n",
            "cxyc_0015\n",
            "cxyc_0015 1\n",
            "[112, 36, 36, 112, 69, 36, 112, 112, 112, 112]\n",
            "cxyc_0016\n",
            "cxyc_0016 1\n",
            "[112, 36, 112, 69, 112, 36, 36, 112, 112, 0, 112, 112, 36, 69]\n",
            "cxyc_0017\n",
            "cxyc_0017 1\n",
            "[112, 36, 112, 112, 69, 112, 36, 99, 0, 36, 112, 112, 0, 112, 112, 36, 112]\n",
            "cxyc_0018\n",
            "cxyc_0018 1\n",
            "[36, 36, 36, 112, 0, 112, 112, 112, 36, 36, 36, 36, 112, 36, 36, 36]\n",
            "cxyc_0019\n",
            "cxyc_0019 1\n",
            "[0, 36, 36, 112, 36, 112, 112, 112, 112, 69]\n",
            "cxyc_0020\n",
            "cxyc_0020 1\n",
            "[112, 36, 3, 112, 36, 112, 36, 0, 112, 36, 36, 112, 112, 112, 36, 69, 112]\n",
            "cxyc_0021\n",
            "cxyc_0021 1\n",
            "[112, 69, 0, 112, 36, 112, 36, 112, 0, 36, 36, 36, 112, 69, 112, 69, 112]\n",
            "cxyc_0022\n",
            "cxyc_0022 1\n",
            "[69, 112, 0, 112, 112, 36, 36, 112, 112, 36, 36, 36]\n",
            "cxyc_0023\n",
            "cxyc_0023 1\n",
            "[112, 36, 36, 112, 69, 36, 36, 0, 36, 36, 112, 36, 36, 112, 36, 36, 69]\n",
            "cxyc_0024\n",
            "cxyc_0024 1\n",
            "[36, 112, 36, 112, 0, 36, 36, 3, 112, 112, 36, 69]\n",
            "cxyc_0025\n",
            "cxyc_0025 1\n",
            "[112, 36, 112, 36, 112, 36, 69, 112, 69, 112, 69, 0, 36, 36]\n",
            "cxyc_0026\n",
            "cxyc_0026 1\n",
            "[112, 112, 112, 0, 112, 112, 112, 112, 69, 0, 0, 112, 36, 36, 36, 36, 36, 36, 0, 112]\n",
            "nsrn_0000\n",
            "nsrn_0000 1\n",
            "[112, 36, 112, 69, 112, 112, 112, 112, 36, 69, 36, 112, 36, 36, 36, 112, 112, 112, 112, 36, 36, 36, 10, 10, 36, 112, 112, 69, 36, 112, 36]\n",
            "nsrn_0001\n",
            "nsrn_0001 1\n",
            "[36, 112, 112, 112, 112, 36, 112, 36, 36, 112, 112, 36, 36, 112, 112, 36, 36, 36, 36, 112, 112, 36, 3, 112, 112, 10, 112, 36, 36, 36, 36, 36, 36, 112, 112, 36, 112, 112, 36, 36, 36, 112, 36, 112, 112, 36, 112, 36, 36, 112, 112, 112, 112, 112, 112, 112, 112, 36, 112, 36, 112, 112, 36, 112, 112, 112, 69, 112, 36, 112, 112, 112, 3, 112, 112, 112, 112, 112, 112, 36, 112, 36, 36, 112, 112, 36, 36, 36, 112, 36, 112, 36, 36, 36, 36, 112, 112, 36, 112, 112]\n",
            "nsrn_0002\n",
            "nsrn_0002 1\n",
            "[112, 10, 69, 112, 112, 36, 112, 36, 36, 112, 112, 36, 36, 36, 112, 36, 112, 112, 36, 112, 36, 36, 36, 112, 112, 36, 112, 112, 112, 69, 36, 36, 69, 36, 36, 112, 36, 36, 112, 112, 112, 36, 112, 112, 112, 36, 112, 112, 112, 36, 36, 36, 112, 99, 112, 112, 36, 69, 69, 36, 36]\n",
            "nsrn_0003\n",
            "nsrn_0003 1\n",
            "[112, 112, 36, 112, 112, 112, 112, 36, 36, 112, 36, 69, 112, 36, 36, 112, 112]\n",
            "nsrn_0004\n",
            "nsrn_0004 1\n",
            "[36, 112, 112, 112, 112, 36, 112, 69, 112, 112, 112, 36, 36, 36, 112, 36, 112, 112, 112, 36, 112, 69, 112, 36, 3, 36, 112, 112, 36, 69, 36, 36, 112, 36, 36, 112, 112, 112, 112, 36, 112, 112, 112, 112, 36, 112, 112, 112, 36, 36, 112, 112, 99, 112, 112, 36, 112, 36, 36, 112, 112, 36, 36, 112, 36, 69, 36, 36, 36, 36, 112, 36]\n",
            "nsrn_0005\n",
            "nsrn_0005 1\n",
            "[112, 36, 112, 112, 112, 112, 112, 36, 69, 112, 112, 112, 112, 36, 36, 112, 36, 112, 112, 112, 112, 36, 112, 112, 112, 112, 36, 112, 3, 112, 36, 69, 36, 112, 112, 112, 36, 112, 36, 36, 36, 112, 36, 69, 112, 36, 112, 36, 69, 112, 112, 36, 36, 36, 112, 36, 36, 112, 112, 112, 36, 112, 36, 36, 3, 36, 36, 112, 36, 112, 112, 112, 36, 112, 112, 112, 112, 3, 36, 36, 36, 112, 112, 36, 36, 36, 112, 112, 69, 36, 112, 112, 112, 36, 36, 112, 112, 112, 36, 36, 3, 112, 99, 112, 112, 36, 36, 112, 112, 36, 36, 69, 112, 36, 36, 112, 112, 36, 112, 36, 36, 36, 112, 36, 112, 36, 36, 36, 112, 112, 36, 36, 112, 69]\n",
            "nsrn_0006\n",
            "nsrn_0006 1\n",
            "[112, 36, 112, 36, 36, 36, 69, 36, 112, 112, 112, 112, 36, 36, 112, 36, 69, 112, 36, 112, 36, 112, 36, 112, 36, 36, 112, 36, 112, 36, 36, 112, 112, 36, 112, 69, 36, 36, 112, 112, 36, 112, 112, 112, 36, 36, 36, 69, 112, 36, 36, 112, 112, 36, 36, 36, 69, 36, 36, 112]\n",
            "nsrn_0007\n",
            "nsrn_0007 1\n",
            "[112, 112, 69, 36, 112, 112, 36, 36, 112, 112, 36, 36, 36, 36, 112, 112, 36, 112, 112, 112, 112, 36, 36, 36, 112, 36, 112, 36, 36, 112, 112, 112, 36, 112, 112, 112, 112, 36, 112, 36, 36, 112, 36, 36, 3, 36, 112, 36, 112, 36, 36, 36, 112, 36, 36, 112, 36, 36, 36, 112, 112, 36, 3, 36, 112, 112, 112, 112, 36, 112, 36, 112, 112, 112, 36, 112, 112, 36, 36, 112, 36, 36, 36, 112, 112, 36, 112, 112, 112, 36, 112, 36, 112, 112, 112, 36, 112, 69, 36, 36, 112, 112, 112, 36, 112, 3, 36, 112, 112, 112, 112, 112, 112, 112, 36, 36, 36, 112, 112, 112, 36, 36, 3, 112, 112, 36, 112, 112, 112, 112, 112, 36, 112, 112, 112, 112, 69, 3, 112, 112, 36, 112, 36, 112, 112, 112, 36, 36, 36, 112, 112, 112]\n",
            "nsrn_0008\n",
            "nsrn_0008 1\n",
            "[36, 3, 112, 112, 36, 112, 112, 112, 112, 36, 112, 36, 36, 112, 36, 36, 112, 36, 36, 112, 36, 36, 36, 112, 36, 112, 36, 36, 112, 36, 36, 36, 36, 36]\n",
            "nsrn_0009\n",
            "nsrn_0009 1\n",
            "[112, 112, 36, 112, 36, 112, 36, 112, 36, 36, 36, 112, 112, 112, 112, 36, 112, 102, 36, 69]\n",
            "nsrn_0010\n",
            "nsrn_0010 1\n",
            "[112, 36, 112, 69, 36, 36, 36, 36, 112, 112, 36, 112, 112, 36, 112, 36, 112, 36, 36, 112, 112, 112, 36, 36]\n",
            "nsrn_0011\n",
            "nsrn_0011 1\n",
            "[36, 3, 36, 112, 112, 36, 112, 112, 36, 112, 36, 112, 112, 112, 36, 36, 112, 36, 112, 36, 36, 112, 36, 36, 112, 36]\n",
            "nsrn_0012\n",
            "nsrn_0012 1\n",
            "[112, 112, 112, 112, 112, 112, 112, 3, 112, 3, 69, 112, 36, 69, 36, 112, 112, 112]\n",
            "nsrn_0013\n",
            "nsrn_0013 1\n",
            "[36, 36, 36, 36, 36, 36, 36, 36, 69, 36, 36, 112, 36, 36, 112, 36, 112, 112, 36, 36, 36, 112, 36, 36]\n",
            "nsrn_0014\n",
            "nsrn_0014 1\n",
            "[112, 112, 112, 112, 112, 112, 112, 36, 36, 112, 36, 112, 112, 36, 36, 36, 36]\n",
            "nsrn_0015\n",
            "nsrn_0015 1\n",
            "[112, 112, 36, 112, 36, 69, 36, 112, 0, 36, 36, 112, 112, 112, 36, 36, 112, 69, 36, 112, 36, 36, 112, 112, 112, 36, 69, 36, 112, 36, 36, 112, 112, 36, 112, 36, 112, 36, 36, 36, 36, 69, 36, 112, 36, 36, 36, 36, 112, 112, 112, 36, 36, 112, 112, 36, 112, 36, 112, 36, 36, 69, 112, 36, 69, 36, 112, 36, 112, 112, 112, 36, 112, 36, 112, 36, 112, 36, 36, 36, 36, 36, 36, 112, 36, 36, 36, 36, 112, 112, 36, 112, 36, 36, 36, 112, 112, 112, 36, 36, 36, 36, 112, 112, 69, 112, 36, 36, 112, 36, 112, 112, 112, 112, 36, 112, 112, 112, 10, 112, 36, 112, 36, 112, 112, 112, 112, 112, 36, 36, 112, 112, 36, 102, 112, 112, 112, 112, 102, 36, 36, 36, 36, 36, 112, 0, 112, 112, 36, 3, 36]\n",
            "nsrn_0016\n",
            "nsrn_0016 1\n",
            "[112, 112, 36, 69, 3, 36, 112, 36, 36, 36, 36, 36, 36, 69, 112, 36, 36, 112, 36, 112, 112, 36, 112, 36, 112, 112, 112, 36, 112, 112, 69, 36, 36, 36, 112, 112, 112, 112, 112, 36, 69, 36, 112, 36, 36, 112, 36, 36, 36, 112, 112, 36, 36, 112, 36, 0, 112, 112, 112, 36, 112, 3, 112, 36, 112, 112, 36, 112, 36, 112, 69, 112, 112, 36, 112, 36, 36, 36, 112, 36, 36, 112, 36, 36, 36, 112, 112, 112, 36, 112, 112, 112, 112, 112, 112, 112, 112, 112, 36, 10, 36, 36, 112, 112, 112, 69, 112, 112, 36, 36, 112, 112, 36]\n",
            "nsrn_0017\n",
            "nsrn_0017 1\n",
            "[112, 112, 112, 0, 36, 112, 112, 36, 112, 112, 112, 36, 112, 36, 36, 36, 112, 36, 112, 112, 36, 112, 36, 36, 112, 36, 36, 112, 112, 112, 36, 36, 112, 36, 36, 36, 112, 36, 112, 112, 112, 36]\n",
            "nsrn_0018\n",
            "nsrn_0018 1\n",
            "[36, 36, 3, 36, 112, 36, 112, 112, 112, 112, 112, 112, 36, 112, 36, 36, 36, 36, 112, 10, 36, 112, 112, 3, 36, 36, 36, 112, 112, 36, 36, 112]\n",
            "nsrn_0019\n",
            "nsrn_0019 1\n",
            "[112, 112, 112, 36, 36, 36, 36, 112, 69, 112, 112, 112, 36, 112, 112, 69, 112, 36, 36, 36, 112, 36, 112, 112, 3, 36, 36, 36, 112, 69, 112, 112, 36, 69, 36, 36, 36, 36, 112, 112, 112, 3, 36, 112, 112, 112, 112, 23, 36]\n",
            "nsrn_0020\n",
            "nsrn_0020 1\n",
            "[36, 112, 112, 112, 36, 36, 36, 36, 112, 112, 36, 36, 36, 112, 112, 36, 69, 36, 36, 36, 112, 36, 36, 112, 36, 36, 36, 36, 112, 112, 112, 36, 36, 112, 112, 112, 36, 3, 112, 36, 112, 112, 36, 36, 112, 112, 112, 36, 36, 112, 36, 36, 36, 69, 36, 112, 23, 112, 36, 112, 36, 36, 36, 112, 36, 112, 36, 112, 36, 36, 112, 112, 10, 3, 36, 112, 36, 36, 36, 112, 69, 112, 112, 112, 36, 112, 112, 36, 69, 112, 36, 36, 112, 112, 112, 36, 112, 36, 36, 3, 112, 36]\n",
            "nsrn_0021\n",
            "nsrn_0021 1\n",
            "[36, 112, 112, 36, 112, 36, 112, 112, 112, 36, 112, 36, 112, 112, 69, 36, 112, 36, 69, 112, 36, 36, 112, 112, 112, 112, 36, 36, 36, 36, 112, 112, 112, 36, 36, 36, 69, 112, 112, 10, 112, 36, 36, 112, 36, 36]\n",
            "nsrn_0022\n",
            "nsrn_0022 1\n",
            "[36, 36, 36, 36, 112, 112, 36, 112, 36, 112, 112, 112, 112, 69, 99, 69, 36, 36, 112, 36, 112, 36, 36, 112, 36, 112, 69, 112, 112, 36, 112, 112, 69, 36, 112, 112, 112, 112, 36, 69, 112, 112, 112, 36, 112, 112, 112, 112, 36, 112, 112, 69, 36, 112, 112, 36, 112, 36, 36, 112, 112, 112, 112, 112, 36, 112, 112, 112, 112, 36, 112, 112, 36, 112, 69, 36, 112, 36, 36, 112, 112, 112, 36, 112, 112, 36, 3, 112, 112, 112, 112, 36, 69, 112, 112, 69, 112, 36, 36, 112, 112, 36, 36, 36, 112, 112, 36, 112, 112, 112, 36, 112, 112, 36, 36, 112, 112, 112, 112, 36, 36, 36, 112, 112, 112, 36, 36, 36, 10, 112, 36, 112, 36, 36, 112, 36, 112, 112, 36]\n",
            "nsrn_0023\n",
            "nsrn_0023 1\n",
            "[36, 36, 112, 36, 112, 112, 36, 36, 112, 36, 112, 36, 112, 36, 36, 112, 112, 112, 112, 69, 36, 36, 112, 112, 112, 36, 112, 112, 112, 112, 36, 36, 36, 69, 112, 112, 112, 36, 36, 112, 3, 112, 36, 36, 36, 112, 112, 112, 36, 112, 36, 3, 112, 36, 36, 112, 36, 112, 36, 69, 112, 36, 36, 112, 36, 112, 36, 36, 36, 36, 112, 112, 112, 36, 36, 112, 112, 36, 112, 112, 69, 36, 112, 36, 112, 112, 112, 112, 112, 112, 36, 36, 112, 112, 112, 36, 112, 36, 112, 36, 36, 112, 112, 112, 3, 36, 36, 36, 10, 112]\n",
            "nsrn_0024\n",
            "nsrn_0024 1\n",
            "[112, 36, 112, 36, 112, 36, 112, 36, 112, 112, 112, 36, 112, 36, 36, 36, 112, 112, 112, 112, 69, 3, 112, 112, 36, 112, 36, 36, 112, 112, 112, 36, 69, 112, 112, 112, 112, 36, 36, 36, 36, 112, 36, 69, 112, 112, 112, 112, 112, 112, 112]\n",
            "nsrn_0025\n",
            "nsrn_0025 1\n",
            "[36, 36, 36, 112, 112, 36, 0, 112, 112, 36, 112, 36, 36, 36, 112, 112, 112, 112, 36, 112, 36, 112, 36, 69, 36, 112, 36, 36, 112, 112, 36, 112, 112, 112, 112]\n",
            "nsrn_0026\n",
            "nsrn_0026 1\n",
            "[36, 112, 112, 36, 112, 69, 36, 36, 36, 36, 112, 112, 36, 112, 112, 69, 36, 36, 112, 36, 112, 36, 112, 36, 36, 112, 36, 112, 112, 112, 69, 36, 36, 112, 36, 69, 112, 36, 112, 36, 36, 36, 112, 36, 36, 112, 112, 112, 36, 112, 36, 112, 36, 112, 36, 112, 36, 112, 112, 112, 112, 36, 36, 112, 112, 112, 36, 36, 36, 36, 112, 112, 69, 69, 0, 36, 69, 36, 36, 36, 0, 69, 69, 112, 112, 112, 36, 112, 36, 36, 69, 112, 69, 36, 3, 36, 36, 36, 36, 112, 69, 36, 69, 69, 0]\n",
            "nsrn_0027\n",
            "nsrn_0027 1\n",
            "[112, 36, 112, 36, 36, 36, 36, 36, 99, 3, 36, 112, 112, 112, 112, 112, 36, 112, 36, 112, 112, 36, 36, 112, 36, 36, 36, 36, 36, 112, 112, 112, 112, 112, 36, 112, 36, 36, 112, 112, 36, 36, 36, 36, 112, 36, 36, 3, 112, 36, 36, 36, 36, 112, 112, 36, 36, 112, 112, 36, 36, 36, 36, 36, 36, 112, 69, 3, 36, 36, 112, 112, 112, 3, 36, 36, 36, 112, 112, 36, 112, 36, 36, 36, 112, 112, 36, 36, 0, 112, 112, 36, 112, 36, 112, 36, 36, 112, 112, 36, 112, 112, 36, 69, 112, 112, 36, 3, 112, 112, 112, 112, 112, 112, 112, 112, 36, 112, 10, 69, 36, 112, 112, 36, 112, 69, 36, 99, 69, 112, 36, 36, 36, 112, 112, 112]\n",
            "nsrn_0028\n",
            "nsrn_0028 1\n",
            "[112, 112, 112, 36, 36, 36, 112, 36, 112, 3, 112, 112, 112, 36, 112, 36, 36, 36, 112, 36, 69, 112, 112, 36, 112, 36, 112, 0, 3, 36, 36, 112, 112, 36, 36, 36, 112, 112, 36, 112, 36, 36, 36, 36]\n",
            "nsrn_0029\n",
            "nsrn_0029 1\n",
            "[36, 3, 36, 36, 0, 112, 112, 112, 99, 36, 112, 36, 112, 36, 112, 112, 36, 36, 36, 36, 36, 112, 112, 112, 112, 112, 36, 112, 36, 112, 112, 36, 112, 36, 36, 36, 112, 112, 112, 36, 36, 112, 112, 112, 112, 112, 112, 36, 112, 36, 69, 112, 112, 36, 36, 36, 36, 112, 36, 112, 36, 36, 112, 112, 112, 36, 112, 112, 3, 102, 112, 3, 112]\n",
            "nsrn_0030\n",
            "nsrn_0030 1\n",
            "[36, 36, 112, 36, 112, 112, 112, 36, 69, 36, 3, 112, 36, 112, 112, 36, 112, 112, 36, 99, 112, 112, 112, 112, 36, 36, 112, 112, 36, 36, 36, 36, 112, 36, 112, 112, 36, 112, 36, 112, 0, 112, 112, 36, 36]\n",
            "nsrn_0031\n",
            "nsrn_0031 1\n",
            "[3, 36, 112, 112, 36, 36, 69, 36, 36, 112, 10, 112, 69, 36, 36, 36, 112, 112, 36, 36, 112]\n",
            "nsrn_0032\n",
            "nsrn_0032 1\n",
            "[36, 69, 36, 36, 112, 112, 36, 36, 112, 112, 69, 36, 110, 112, 69, 23, 36, 69, 3, 112, 36, 69, 69, 112, 112, 112, 69, 69, 69, 36, 112, 112, 112, 69, 36, 36, 112, 36, 112, 69, 112, 36, 69, 112, 112, 112, 69, 112, 112, 69, 69, 36, 69, 112, 112, 36, 69, 36, 112, 36, 36, 69, 36, 36, 36, 112, 36, 36, 112, 69, 36]\n",
            "nsrn_0033\n",
            "nsrn_0033 1\n",
            "[36, 69, 69, 69, 112]\n",
            "nsrn_0034\n",
            "nsrn_0034 1\n",
            "[36, 112, 36, 112, 36, 112, 112, 36, 112, 112, 69, 112, 112, 36, 112, 112, 112, 36, 69, 36, 112, 3, 36, 36, 112, 36, 112, 112, 112, 112, 36, 69, 112, 112, 112, 36, 36, 112, 112, 112, 112, 36, 112, 112, 36, 3, 36, 36, 69, 112, 112, 36, 36, 36, 112, 36, 112, 36, 112, 36, 112, 112, 36, 112, 36, 23, 112]\n",
            "nsrn_0035\n",
            "nsrn_0035 1\n",
            "[112, 36, 112, 112, 112, 36, 36, 112, 112, 36, 36, 3, 112, 36, 69, 69, 36, 112, 112, 112, 36, 36, 36, 3, 36, 112, 69, 112, 3]\n",
            "nsrn_0036\n",
            "nsrn_0036 1\n",
            "[112, 112, 3, 36, 112, 112, 112, 36, 36, 36, 36, 36, 112, 36, 36, 36, 112, 36, 36, 112, 36, 112, 3, 112, 36, 112, 112, 36, 36, 112, 112, 112, 112, 36, 36, 36]\n",
            "nsrn_0037\n",
            "nsrn_0037 1\n",
            "[36, 112, 36, 112, 112, 36, 36, 112, 112, 36, 112, 112, 3, 69, 112, 69, 112, 112, 112, 112, 36, 112, 112, 36, 112, 36, 36, 112, 112, 112, 112, 36, 112, 112, 36, 10, 112, 112, 36, 36, 112, 36, 36, 112, 36, 112, 112, 112, 112, 112, 36, 112, 69, 36, 112, 112, 112, 36, 3, 112, 112, 112, 112, 112, 36, 36, 112, 112, 112, 112, 36, 112, 112, 36, 112, 36, 36, 112, 69, 112]\n",
            "qfod_0000\n",
            "qfod_0000 1\n",
            "[112, 36, 112, 36, 112, 112, 36, 36, 112, 36, 36, 36, 36, 36, 36, 112, 112, 69]\n",
            "qfod_0001\n",
            "qfod_0001 1\n",
            "[112, 36, 112, 112, 36, 112, 36, 112, 112, 36, 36, 36, 69, 112]\n",
            "qfod_0002\n",
            "qfod_0002 1\n",
            "[36, 36, 112, 36, 3, 36, 112, 99, 112, 36, 36, 36, 112, 36, 36, 36, 36, 36, 112, 112, 112, 36, 36, 112, 112, 36, 36]\n",
            "qfod_0003\n",
            "qfod_0003 1\n",
            "[112, 112, 36, 112, 112, 36, 69, 36, 112, 112, 112, 36, 36, 112, 112, 112, 36, 36, 36, 36, 112, 36, 112, 36, 36, 112, 112, 112, 112]\n",
            "qfod_0004\n",
            "qfod_0004 1\n",
            "[112, 112, 36, 36, 112, 36, 36, 112, 36, 36, 112, 36, 36, 36, 36, 36, 112, 36, 112, 36, 36, 36, 112]\n",
            "qfod_0005\n",
            "qfod_0005 1\n",
            "[112, 112, 112, 36, 112, 36, 36, 36, 0, 36, 36, 36, 3, 36, 112, 36, 112, 36, 69, 36, 36, 112, 36, 36, 36, 112, 112, 69, 69, 36, 112]\n",
            "qfod_0006\n",
            "qfod_0006 1\n",
            "[36, 112, 112, 3, 112, 112, 36, 112, 36, 36, 36, 112, 36, 36, 112, 36, 36, 36, 36, 112, 112, 36, 112, 112, 36, 36, 112, 36, 36, 36, 112]\n",
            "qfod_0007\n",
            "qfod_0007 1\n",
            "[112, 112, 112, 36, 112, 112, 36, 112, 36, 36, 112, 36]\n",
            "qfod_0008\n",
            "qfod_0008 1\n",
            "[69, 112, 112, 112, 69, 36, 112, 36, 36, 3, 36, 36, 36, 36, 36, 36]\n",
            "qfod_0009\n",
            "qfod_0009 1\n",
            "[112, 36, 112, 36, 112, 36, 36, 36, 36, 36, 112, 36, 112, 36, 36, 112, 36, 112, 36, 112, 112, 36, 36]\n",
            "qfod_0010\n",
            "qfod_0010 1\n",
            "[36, 112, 36, 112, 112, 36, 36, 36, 36, 112, 3, 112, 36, 36, 112, 36, 36, 36, 36, 36, 36, 36, 112, 112, 36, 112, 36, 36, 36, 36, 112]\n",
            "qfod_0011\n",
            "qfod_0011 1\n",
            "[112, 112, 112, 36, 112, 112, 112, 36, 36, 36, 112, 36, 36, 112, 112, 36, 112, 0]\n",
            "qfod_0012\n",
            "qfod_0012 1\n",
            "[112, 36, 69, 3, 3, 112, 36, 112, 69, 112, 36, 36, 112, 36, 36, 36, 36, 99, 36, 112, 36, 36, 112, 112, 112, 36, 36, 36]\n",
            "qfod_0013\n",
            "qfod_0013 1\n",
            "[112, 112, 36, 69, 112, 36, 36, 36, 112, 112, 112, 36, 36, 36, 112, 69, 112, 112, 36, 112, 69, 36]\n",
            "qfod_0014\n",
            "qfod_0014 1\n",
            "[36, 112, 3, 36, 112, 112, 112, 36, 36, 36, 112, 36, 36, 112, 36, 36, 112, 112, 36, 112, 36, 112, 36]\n",
            "qfod_0015\n",
            "qfod_0015 1\n",
            "[112, 112, 36, 36, 36, 112, 36, 112, 36, 112, 36, 36, 112, 36, 36, 112, 112, 112, 0, 36, 112, 36, 112, 112, 36, 112, 112, 0, 36, 112, 36]\n",
            "qfod_0016\n",
            "qfod_0016 1\n",
            "[112, 36, 36, 36, 112, 36, 36, 36, 112, 112, 36, 112, 36, 112, 112, 36, 112, 99, 112, 112, 36, 36, 112, 69, 36, 36]\n",
            "qfod_0017\n",
            "qfod_0017 1\n",
            "[112, 112, 112, 36, 112, 112, 112, 112, 36, 112, 36, 36, 112, 112, 36, 36, 36, 36, 99, 112, 112, 112, 36, 112, 36, 112]\n",
            "qfod_0018\n",
            "qfod_0018 1\n",
            "[36, 112, 36, 36, 112, 36, 112, 99, 112, 112, 36, 112, 112, 36, 112, 112, 36, 112, 112, 112, 36, 112]\n",
            "qfod_0019\n",
            "qfod_0019 1\n",
            "[69, 112, 112, 112, 36, 36, 36, 36, 112, 69, 36, 36, 36, 112, 112, 3, 36, 112, 112, 112, 36, 36, 0, 36, 36, 112, 112, 112, 112, 112, 69, 36, 112, 112, 112, 112, 112, 36, 36]\n",
            "qfod_0020\n",
            "qfod_0020 1\n",
            "[112, 36, 36, 112, 112, 36, 36, 36, 112, 69, 112, 36, 36, 36, 69, 3, 69, 112, 112, 112, 36, 36, 69, 112, 36, 36, 112, 112, 3]\n",
            "khxv_0000\n",
            "khxv_0000 2\n",
            "[112, 36, 112, 36, 112, 112, 36, 36]\n",
            "khxv_0001\n",
            "khxv_0001 2\n",
            "[36, 36, 112, 36, 36, 36, 3, 112, 36, 36, 112, 36, 112, 36, 112, 36, 112, 36, 112, 36, 36, 36, 112, 112, 112, 112, 36, 36, 36, 36, 36, 36]\n",
            "khxv_0002\n",
            "khxv_0002 2\n",
            "[36, 112, 112, 112, 112, 36, 112, 112, 36, 36, 36]\n",
            "khxv_0003\n",
            "khxv_0003 2\n",
            "[112, 36, 112, 112, 36, 36, 112, 36, 36, 36, 36, 36, 112, 23, 112, 112, 36, 36, 36, 36, 112, 112, 36, 112, 112]\n",
            "khxv_0004\n",
            "khxv_0004 2\n",
            "[36, 36, 36, 36, 36, 112, 112, 112, 112, 36, 112, 112, 112, 112, 36, 112, 112, 112, 112, 36, 112, 112, 3, 112]\n",
            "khxv_0005\n",
            "khxv_0005 2\n",
            "[112, 36, 112, 36, 112, 112, 112, 36, 36, 112, 36, 112, 69, 36, 36, 112, 36, 112, 36, 112, 36, 112, 112, 36, 112, 112, 36]\n",
            "khxv_0006\n",
            "khxv_0006 2\n",
            "[112, 112, 36, 112, 112, 112, 36, 112, 112, 112, 36, 112, 112, 112, 112, 36, 112, 112]\n",
            "khxv_0007\n",
            "khxv_0007 2\n",
            "[112, 36, 112, 36, 3, 36, 3, 36, 112, 112, 112, 112, 112, 112, 112, 112, 3, 112, 112, 112, 36, 112, 112, 36, 112, 36, 36, 36, 112, 3, 112, 112, 112, 36, 3, 69, 112]\n",
            "khxv_0008\n",
            "khxv_0008 2\n",
            "[112, 112, 36, 36, 112, 36, 36, 112, 36, 112, 112, 112, 112, 36, 112, 112, 112, 36, 36, 36, 36, 36, 36, 36, 36, 112, 112, 112, 112, 112, 112]\n",
            "khxv_0009\n",
            "khxv_0009 2\n",
            "[112, 112, 3, 112, 36, 112, 36, 112, 112, 112, 36, 36, 36, 112, 69]\n",
            "khxv_0010\n",
            "khxv_0010 2\n",
            "[3, 112, 112, 112, 36, 36, 36, 112, 36, 112, 36, 3, 36, 36, 112, 112, 112, 112, 112, 36, 112, 36, 112, 112, 36, 36, 112, 112, 36, 36]\n",
            "khxv_0011\n",
            "khxv_0011 2\n",
            "[112, 36, 112, 112, 112, 112, 112, 36, 3, 36, 112, 36, 112, 3, 36, 3, 112, 36]\n",
            "khxv_0012\n",
            "khxv_0012 2\n",
            "[36, 3, 23, 112, 36, 112, 36, 0, 36, 112, 112, 36, 112, 112, 36]\n",
            "khxv_0013\n",
            "khxv_0013 2\n",
            "[112, 36, 36, 112]\n",
            "khxv_0014\n",
            "khxv_0014 2\n",
            "[36, 112, 112, 3, 112, 112, 112, 112]\n",
            "khxv_0015\n",
            "khxv_0015 2\n",
            "[36, 36, 112, 112, 36, 112, 112, 36, 36]\n",
            "khxv_0016\n",
            "khxv_0016 2\n",
            "[112, 36, 112, 112, 112, 112, 112, 112, 36, 112, 112, 36, 112, 3, 36, 3, 112, 36]\n",
            "khxv_0017\n",
            "khxv_0017 2\n",
            "[112, 112, 36, 112, 69, 36, 112, 36, 112, 36, 36]\n",
            "khxv_0018\n",
            "khxv_0018 2\n",
            "[36, 112, 112, 112, 112, 3, 36, 10, 112, 112, 36, 36]\n",
            "khxv_0019\n",
            "khxv_0019 2\n",
            "[36, 112, 36, 36, 36, 0, 36, 36, 112, 112, 36]\n",
            "khxv_0020\n",
            "khxv_0020 2\n",
            "[112, 36, 36, 112, 112, 36, 3, 36, 112, 112, 112, 36, 36, 36, 36, 36, 36, 112]\n",
            "khxv_0021\n",
            "khxv_0021 2\n",
            "[36, 112, 36, 112, 36, 112, 36, 112, 36, 36, 3, 112, 36, 112, 36]\n",
            "khxv_0022\n",
            "khxv_0022 2\n",
            "[110, 36, 112, 112, 112, 36, 36, 36, 36, 112, 36, 36, 36]\n",
            "khxv_0023\n",
            "khxv_0023 2\n",
            "[36, 36, 112, 69, 112, 112, 36, 69, 112, 69, 69]\n",
            "khxv_0024\n",
            "khxv_0024 2\n",
            "[36, 69, 112, 112, 112, 23, 36]\n",
            "khxv_0025\n",
            "khxv_0025 2\n",
            "[36, 112, 36, 112, 36, 112, 36, 36, 112, 10, 36]\n",
            "khxv_0026\n",
            "khxv_0026 2\n",
            "[36, 36, 36, 36, 112, 36, 36, 36, 112, 112, 36, 112]\n",
            "khxv_0027\n",
            "khxv_0027 2\n",
            "[112, 112, 36, 36, 112, 36, 112, 3, 112, 112, 112, 112, 0, 112, 36, 36, 112]\n",
            "khxv_0028\n",
            "khxv_0028 2\n",
            "[36, 112, 36, 112, 112, 36, 36, 36, 112, 36, 36, 36]\n",
            "khxv_0029\n",
            "khxv_0029 2\n",
            "[36, 36, 69, 112, 112, 112, 36, 112, 112, 36, 112, 36, 112, 112, 112, 36]\n",
            "khxv_0030\n",
            "khxv_0030 2\n",
            "[36, 36, 36, 36, 112, 112, 36, 36, 112, 112, 112, 36, 112, 112]\n",
            "khxv_0031\n",
            "khxv_0031 2\n",
            "[112, 36, 112, 3, 112, 36, 112, 36]\n",
            "khxv_0032\n",
            "khxv_0032 2\n",
            "[36, 36, 69, 112, 112, 112, 36, 112, 36, 36, 36, 112, 36, 36, 112]\n",
            "khxv_0033\n",
            "khxv_0033 2\n",
            "[36, 112, 112, 112, 36, 36, 36, 112, 112, 36, 112, 36]\n",
            "khxv_0034\n",
            "khxv_0034 2\n",
            "[112, 36, 112, 112, 112, 36, 112]\n",
            "khxv_0035\n",
            "khxv_0035 2\n",
            "[36, 36, 36, 36, 36, 36, 36, 36, 112, 36, 36]\n",
            "khxv_0036\n",
            "khxv_0036 2\n",
            "[36, 112, 112, 112, 36, 112, 36, 36, 36, 112, 112, 36, 36]\n",
            "khxv_0037\n",
            "khxv_0037 2\n",
            "[36, 36, 36, 36, 36, 69, 36, 69, 36, 112, 36, 112]\n",
            "khxv_0038\n",
            "khxv_0038 2\n",
            "[112, 36, 112, 112, 112, 36, 36]\n",
            "khxv_0039\n",
            "khxv_0039 2\n",
            "[36, 36, 36]\n",
            "khxv_0040\n",
            "khxv_0040 2\n",
            "[36, 36, 36, 36, 112, 36, 36, 112, 36, 112, 36, 112, 36, 112, 36, 112, 112, 36, 36, 36, 36, 112, 36, 112, 36, 36, 36]\n",
            "khxv_0041\n",
            "khxv_0041 2\n",
            "[36, 36, 112, 112, 36, 36, 36, 112, 112, 36, 36, 112, 112, 36, 36, 36, 36, 36]\n",
            "khxv_0042\n",
            "khxv_0042 2\n",
            "[36, 36, 36, 112, 112, 112, 36, 36, 112, 36, 112, 3, 112, 112, 112, 112]\n",
            "khxv_0043\n",
            "khxv_0043 2\n",
            "[36, 36, 112, 36, 112, 36, 36, 112, 36, 112, 36, 69, 69, 110, 112, 36, 112, 112, 36, 112, 112, 36, 36, 36, 112, 112, 112]\n",
            "khxv_0044\n",
            "khxv_0044 2\n",
            "[36, 112, 36, 112, 36, 112, 36, 10, 36, 112, 36, 36, 112, 36, 112, 36, 112]\n",
            "khxv_0045\n",
            "khxv_0045 2\n",
            "[36, 112, 36, 36, 36, 112, 112, 36, 112, 112, 112, 36, 36, 112, 36, 112, 112, 36, 112]\n",
            "khxv_0046\n",
            "khxv_0046 2\n",
            "[112, 36, 36, 112, 36, 36, 112, 36, 36, 112, 112, 112, 112, 112, 112, 112, 112, 112, 36, 36, 112, 112, 112, 112, 36, 36, 36, 36, 112, 112, 112, 36, 112, 112, 112, 69]\n",
            "khxv_0047\n",
            "khxv_0047 2\n",
            "[112, 36, 112, 36, 112, 3, 36, 112, 112, 112, 112, 0, 112, 112, 112, 112, 36, 36, 36, 36, 112, 36, 69, 36, 112, 112, 36, 112, 3]\n",
            "oogw_0000\n",
            "oogw_0000 2\n",
            "[36, 112, 112, 36]\n",
            "oogw_0001\n",
            "oogw_0001 2\n",
            "[112, 112, 112, 112]\n",
            "oogw_0002\n",
            "oogw_0002 2\n",
            "[112, 112, 112, 112, 112, 112]\n",
            "oogw_0003\n",
            "oogw_0003 2\n",
            "[36, 36, 112, 112, 36, 112]\n",
            "oogw_0004\n",
            "oogw_0004 2\n",
            "[112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 36, 112]\n",
            "oogw_0005\n",
            "oogw_0005 2\n",
            "[112, 112, 36, 112, 112, 112]\n",
            "oogw_0006\n",
            "oogw_0006 2\n",
            "[112, 112, 36, 36, 112, 112, 112]\n",
            "oogw_0007\n",
            "oogw_0007 2\n",
            "[112, 36, 112, 36, 112, 112]\n",
            "oogw_0008\n",
            "oogw_0008 2\n",
            "[112, 112, 36, 36, 112, 112, 36, 112, 112, 112, 112, 36, 112, 36, 112, 112, 112]\n",
            "oogw_0009\n",
            "oogw_0009 2\n",
            "[36, 112, 112, 112, 112, 36, 3, 36, 112, 112, 36, 36, 36, 112, 10, 112, 36, 112]\n",
            "oogw_0010\n",
            "oogw_0010 2\n",
            "[112, 3, 112, 112, 36, 112, 36, 36, 112, 36]\n",
            "oogw_0011\n",
            "oogw_0011 2\n",
            "[112, 112, 112, 36, 36]\n",
            "oogw_0012\n",
            "oogw_0012 2\n",
            "[36, 3, 112, 112, 112, 112, 36, 112, 112, 112, 69, 112, 36, 112, 36, 112, 36]\n",
            "oogw_0013\n",
            "oogw_0013 2\n",
            "[112, 36, 36, 112, 112, 112, 112, 112, 36, 36, 36, 112]\n",
            "oogw_0014\n",
            "oogw_0014 2\n",
            "[112, 36, 3, 112, 112, 112, 112, 112, 112, 36, 112, 36, 36, 112, 3, 112, 112]\n",
            "oogw_0015\n",
            "oogw_0015 2\n",
            "[36, 112, 112, 112, 112, 3, 36, 112, 36, 3, 112, 36]\n",
            "oogw_0016\n",
            "oogw_0016 2\n",
            "[112, 112, 36, 112, 112, 112, 112, 112, 36, 36, 112, 69, 112, 36, 10, 112]\n",
            "oogw_0017\n",
            "oogw_0017 2\n",
            "[3, 112, 36, 112, 112, 112, 36, 36, 112, 112]\n",
            "oogw_0018\n",
            "oogw_0018 2\n",
            "[36, 112, 36, 36, 36, 36, 36, 36, 112, 112, 112, 36, 112]\n",
            "oogw_0019\n",
            "oogw_0019 2\n",
            "[36, 112, 36, 112, 36, 36, 112, 36, 36, 112, 112, 36, 112, 36, 112, 112, 112, 3, 112]\n",
            "oogw_0020\n",
            "oogw_0020 2\n",
            "[3, 36, 112, 112, 112, 112, 36, 112, 112, 36, 36, 112, 112, 23, 112, 112, 112, 112]\n",
            "oogw_0021\n",
            "oogw_0021 2\n",
            "[112, 112, 112, 36, 112, 112, 36, 36, 112, 112, 112, 112, 112, 3, 112, 112, 112, 112, 3, 3, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112]\n",
            "oogw_0022\n",
            "oogw_0022 2\n",
            "[112, 112, 36, 36, 36, 112, 112, 36, 36, 36, 112, 112, 112, 3, 3, 112, 36, 36, 112, 3, 112, 112, 36, 112, 36, 3]\n",
            "oogw_0023\n",
            "oogw_0023 2\n",
            "[112, 36, 36, 36, 112, 3, 112, 112, 112, 36, 112, 36, 36, 112, 112, 112, 112, 112, 36, 112, 112, 112]\n",
            "oogw_0024\n",
            "oogw_0024 2\n",
            "[112, 36, 112, 36, 112, 112, 69, 112, 3]\n",
            "oogw_0025\n",
            "oogw_0025 2\n",
            "[36, 3, 112, 112, 36, 36, 36, 112, 112, 36, 112, 36, 36, 112, 112, 36, 36, 36, 36]\n",
            "oogw_0026\n",
            "oogw_0026 2\n",
            "[69, 112, 112, 112, 112, 112, 36, 112, 36, 112, 36, 112, 36, 112, 36, 112, 36, 112, 36, 36, 112, 3, 112, 36, 36, 36, 112, 112, 36, 36]\n",
            "oogw_0027\n",
            "oogw_0027 2\n",
            "[36, 112, 36, 112, 36, 36, 112, 36, 23, 112, 36, 112]\n",
            "oogw_0028\n",
            "oogw_0028 2\n",
            "[112, 112, 112, 112, 36, 112, 112, 36, 3, 36, 36, 112, 36, 36, 112, 36, 112, 112, 36, 112, 36, 112, 112, 112, 36, 112, 36, 112, 36, 112, 3, 36, 112, 112, 36, 112, 112, 112]\n",
            "oogw_0029\n",
            "oogw_0029 2\n",
            "[112, 112, 36, 3, 112, 112, 36, 36, 112, 112, 112, 112, 112, 36, 112, 36, 112, 112, 112, 3, 112, 112, 36]\n",
            "oogw_0030\n",
            "oogw_0030 2\n",
            "[112, 36, 112, 23, 36, 112, 36, 112, 112, 112, 36, 112, 112, 112, 112, 36, 112, 112, 112, 36, 36, 36, 36, 36, 112, 112, 23, 112, 36, 36, 112, 36, 112, 36, 36, 112, 36, 36, 36, 112, 112, 112, 36, 112, 112, 112, 36, 112, 112, 112, 112, 112, 112, 3, 112]\n",
            "oogw_0031\n",
            "oogw_0031 2\n",
            "[112, 112, 112, 112, 112, 112, 112, 112, 112, 3, 36, 36, 112, 112, 36, 112, 3, 36, 112, 112]\n",
            "oogw_0032\n",
            "oogw_0032 2\n",
            "[3, 69, 112, 36, 112, 112, 36, 36, 112, 36, 112, 3, 112, 36, 112, 36, 112, 112]\n",
            "oogw_0033\n",
            "oogw_0033 2\n",
            "[112, 3, 112, 112, 112, 112, 112, 36, 112, 112, 112, 3, 36, 112, 112, 112, 36, 112, 36, 112, 112, 112, 36, 36, 112, 112, 112]\n",
            "oogw_0034\n",
            "oogw_0034 2\n",
            "[36, 112, 36, 36, 112, 3, 112, 112, 36, 112, 36, 112, 112, 36, 112, 36, 36, 0, 112, 36, 112, 112]\n",
            "oogw_0035\n",
            "oogw_0035 2\n",
            "[69, 112, 112, 36, 112, 36, 112, 112, 112, 112, 0, 112, 112, 112, 112, 112, 36, 112]\n",
            "kcos_0000\n",
            "kcos_0000 3\n",
            "[112, 112, 110, 112, 112, 112, 110, 36, 36, 112, 36, 36, 112, 36, 36, 112]\n",
            "kcos_0001\n",
            "kcos_0001 3\n",
            "[112, 36, 112, 112, 112, 112, 112, 112, 112]\n",
            "kcos_0002\n",
            "kcos_0002 3\n",
            "[112, 69, 36, 112, 36, 112, 112, 110, 69, 36, 36, 36, 112, 112, 112]\n",
            "kcos_0003\n",
            "kcos_0003 3\n",
            "[112, 112, 36, 69, 36, 3, 36, 110, 112, 69, 112, 36, 36, 112, 36, 69, 112, 112, 36, 36, 36, 69, 36, 23, 112, 110, 110, 112, 110, 36, 36, 112, 36, 36]\n",
            "kcos_0004\n",
            "kcos_0004 3\n",
            "[3, 110, 36, 112, 36, 112, 110, 36, 69, 36, 112, 36, 69, 110, 69, 69, 112, 112]\n",
            "kcos_0005\n",
            "kcos_0005 3\n",
            "[112, 112, 69, 53, 112, 36, 112, 69, 36, 36, 36, 69, 3, 36, 36, 36, 36, 112, 23, 110, 110, 112, 36, 112, 36, 112, 112, 36, 36, 23, 36, 36, 112, 110, 69, 112, 110, 23, 112, 110]\n",
            "kcos_0006\n",
            "kcos_0006 3\n",
            "[36, 112, 112, 36, 36, 36, 36, 36, 36, 112, 110, 110, 112, 36, 112, 112, 36, 112, 36, 112, 36, 36, 112, 36, 112, 36, 36, 110]\n",
            "kcos_0007\n",
            "kcos_0007 3\n",
            "[112, 110, 36, 36, 36, 36, 110, 36, 112, 112, 112, 112, 112, 112, 112, 112, 36, 36, 36, 36, 112, 112, 36, 112, 36, 110, 112, 69, 36, 36, 36, 110, 112, 112, 36, 36, 36, 3, 36, 36, 112, 110, 69, 3, 36, 36, 36, 110, 69, 112, 36, 110, 36, 110, 110, 36, 112, 36, 69, 36, 112]\n",
            "kcos_0008\n",
            "kcos_0008 3\n",
            "[110, 112, 36, 112, 110, 36, 110, 112, 36, 112, 112, 112, 3, 112, 36, 112, 112, 36, 36, 3, 36, 36, 112, 36, 36, 112, 36]\n",
            "kcos_0009\n",
            "kcos_0009 3\n",
            "[36, 36, 112, 36, 36, 3, 36, 112, 3, 36, 36, 112, 36, 36, 112, 36, 36, 112, 112, 110, 110, 36, 112, 112, 36, 112, 112, 112, 3, 112]\n",
            "kcos_0010\n",
            "kcos_0010 3\n",
            "[36, 112, 36, 112, 36, 112, 112, 36, 112, 36, 36]\n",
            "kcos_0011\n",
            "kcos_0011 3\n",
            "[36, 36, 112, 112, 110, 112, 112, 36, 36]\n",
            "kcos_0012\n",
            "kcos_0012 3\n",
            "[23, 110, 36, 112, 36, 112, 112, 112, 36, 36, 3, 36, 110, 36, 110, 112, 112, 112]\n",
            "kcos_0013\n",
            "kcos_0013 3\n",
            "[112, 110, 36, 36, 110, 36, 112, 36, 112, 36, 36, 112, 36, 36, 112, 69, 110, 36, 36]\n",
            "kcos_0014\n",
            "kcos_0014 3\n",
            "[36, 36, 110, 36, 36, 36, 110, 112, 36, 112, 36, 36, 112, 112, 112, 36, 112, 112, 112, 110, 110, 36, 112, 36, 69, 112, 36, 36, 112]\n",
            "kcos_0015\n",
            "kcos_0015 3\n",
            "[112, 36, 112, 112, 36, 36, 112, 36, 112, 112, 36, 36, 112, 112, 36, 112, 112, 112, 36, 36, 112, 36]\n",
            "kcos_0016\n",
            "kcos_0016 3\n",
            "[36, 112, 112, 112, 36, 112, 112, 36, 112, 36, 110, 112]\n",
            "kcos_0017\n",
            "kcos_0017 3\n",
            "[36, 36, 112, 112, 110, 36, 3, 112, 36, 36, 110, 112, 112, 36, 112, 112, 36, 36, 36, 112, 36, 36, 112, 36, 112]\n",
            "kcos_0018\n",
            "kcos_0018 3\n",
            "[36, 112, 112, 112, 110, 36, 112, 36, 36, 112, 36, 112, 110, 36, 112, 36, 36, 36, 112, 112, 36, 69, 36, 36]\n",
            "kcos_0019\n",
            "kcos_0019 3\n",
            "[36, 3, 36, 36, 3, 110, 110, 112, 112, 36, 36, 112, 36, 112, 36, 112, 110, 3, 112, 3, 112, 36]\n",
            "kcos_0020\n",
            "kcos_0020 3\n",
            "[112, 36, 112, 36, 112, 3, 112, 36, 36, 110, 36, 36, 36, 112, 112, 36, 36, 110, 112, 112]\n",
            "kcos_0021\n",
            "kcos_0021 3\n",
            "[112, 36, 112, 110, 36, 112, 112, 110, 3, 112, 69, 36, 112]\n",
            "kcos_0022\n",
            "kcos_0022 3\n",
            "[36, 36, 112, 112, 36, 112, 112, 110, 36, 36, 3, 36, 23]\n",
            "kcos_0023\n",
            "kcos_0023 3\n",
            "[36, 112, 36, 36, 112, 112, 69, 112, 112, 36, 112, 112, 36, 112, 36, 36, 36, 112, 112, 112, 112, 36, 36]\n",
            "kcos_0024\n",
            "kcos_0024 3\n",
            "[112, 112, 36, 112, 36, 112, 110, 36, 36, 36, 112, 36, 36, 110, 69, 112, 36, 36]\n",
            "kcos_0025\n",
            "kcos_0025 3\n",
            "[36, 112, 36, 112, 110, 112, 112, 36, 36, 36, 36, 36, 112, 112, 110, 36, 36, 112]\n",
            "kcos_0026\n",
            "kcos_0026 3\n",
            "[36, 110, 36, 112, 36, 112, 112, 36, 36, 112, 36, 112, 112, 36, 36, 36, 36]\n",
            "kcos_0027\n",
            "kcos_0027 3\n",
            "[36, 112, 112, 112, 36, 110, 110, 36, 36, 36, 36, 36]\n",
            "kcos_0028\n",
            "kcos_0028 3\n",
            "[112, 36, 112, 112, 36, 112, 112, 36, 36, 112, 112, 36, 36]\n",
            "kcos_0029\n",
            "kcos_0029 3\n",
            "[3, 112, 36, 112, 36, 112, 112, 36, 36, 112, 36, 110, 112, 112, 36, 112, 112]\n",
            "kcos_0030\n",
            "kcos_0030 3\n",
            "[112, 110, 112, 112, 69, 112, 36, 36, 112, 36, 112, 112, 36, 110, 112, 69, 36, 36, 36, 36]\n",
            "tbfk_0000\n",
            "tbfk_0000 4\n",
            "[36, 3, 3, 96, 23, 36, 3, 36, 117, 3, 23, 23, 36, 36, 36, 36]\n",
            "tbfk_0001\n",
            "tbfk_0001 4\n",
            "[36, 36, 3, 36, 36, 36, 36, 62, 36, 23, 36, 36, 23, 23, 36]\n",
            "tbfk_0002\n",
            "tbfk_0002 4\n",
            "[4, 3, 96, 36, 23, 36, 3, 36, 3, 3, 36, 117, 36, 23, 23, 3, 36]\n",
            "tbfk_0003\n",
            "tbfk_0003 4\n",
            "[36, 23, 36, 4, 36, 36, 3, 3, 23, 36, 3, 23, 3, 3, 3, 4, 3, 36]\n",
            "tbfk_0004\n",
            "tbfk_0004 4\n",
            "[3, 36, 3, 36, 96, 3, 117, 23, 36, 23, 36]\n",
            "tbfk_0005\n",
            "tbfk_0005 4\n",
            "[112, 36, 3, 36, 3, 112, 3, 4, 3, 23, 3, 36, 23, 23, 3]\n",
            "tbfk_0006\n",
            "tbfk_0006 4\n",
            "[36, 3, 36, 36, 23, 36, 3, 3, 23, 36, 3, 3, 112]\n",
            "tbfk_0007\n",
            "tbfk_0007 4\n",
            "[36, 3, 96, 4, 3, 36, 3, 23, 3, 36, 3, 117, 4, 23, 23, 36]\n",
            "tbfk_0008\n",
            "tbfk_0008 4\n",
            "[4, 36, 3, 36, 23, 96, 117, 36, 62, 3, 23, 23, 36, 3, 3, 36]\n",
            "tbfk_0009\n",
            "tbfk_0009 4\n",
            "[4, 3, 36, 36, 23, 4, 3, 3, 3, 3, 23, 4, 36, 23, 36, 3, 3, 36]\n",
            "tbfk_0010\n",
            "tbfk_0010 4\n",
            "[3, 23, 23, 112, 3, 3, 3, 4, 3, 4, 3, 3, 96, 23, 3, 36, 23, 96, 96, 110, 3, 3, 23, 36, 23, 4, 3, 3, 4, 3, 96, 62]\n",
            "tbfk_0011\n",
            "tbfk_0011 4\n",
            "[3, 3, 3, 36, 36, 36, 3, 117, 36, 23, 117, 36, 23, 23, 36]\n",
            "tbfk_0012\n",
            "tbfk_0012 4\n",
            "[36, 36, 3, 23, 36, 36, 36, 3, 126, 3, 36, 36, 23, 23, 3, 23, 3, 36, 36, 96]\n",
            "tbfk_0013\n",
            "tbfk_0013 4\n",
            "[36, 36, 3, 23, 23, 96, 23, 3, 4, 96, 96, 23, 23, 36, 96, 36, 36, 36, 117, 36, 36, 96, 96, 3, 36, 3, 96, 96, 3, 62, 3, 4, 36, 36, 96, 96, 3, 3, 3, 23, 3, 4, 36, 96, 4, 53, 36, 36, 36, 62, 96, 36, 36, 112, 117, 3, 23, 23, 3, 36, 36, 3, 96, 96, 36, 23, 36, 36, 36, 3, 112, 96, 36, 36, 36, 23, 3, 23, 4, 36, 96, 96, 3, 36, 62, 23]\n",
            "tbfk_0014\n",
            "tbfk_0014 4\n",
            "[112, 23, 3, 36, 3, 36, 3, 3, 23, 3, 3, 23, 3, 36, 117, 3, 23, 36]\n",
            "tbfk_0015\n",
            "tbfk_0015 4\n",
            "[3, 3, 23, 4, 23, 23, 3, 36, 4, 3, 3, 3]\n",
            "tbfk_0016\n",
            "tbfk_0016 4\n",
            "[36, 3, 3, 36, 3, 36, 112, 23, 23, 53, 3, 3, 23, 3, 117, 3, 36, 23, 36]\n",
            "tbfk_0017\n",
            "tbfk_0017 4\n",
            "[36, 36, 36, 96, 23, 4, 36, 36, 23, 117, 3, 36, 3, 117, 23]\n",
            "tbfk_0018\n",
            "tbfk_0018 4\n",
            "[96, 3, 36, 36, 36, 36, 117, 117, 36, 23, 3, 4, 23, 23, 36]\n",
            "tbfk_0019\n",
            "tbfk_0019 4\n",
            "[36, 3, 36, 36, 36, 36, 4, 3, 23, 117, 36, 23, 3, 36, 36, 3, 36]\n",
            "tbfk_0020\n",
            "tbfk_0020 4\n",
            "[4, 62, 36, 36, 3, 4, 23, 36, 23, 3, 36, 23, 53, 3, 36, 3, 4]\n",
            "tbfk_0021\n",
            "tbfk_0021 4\n",
            "[96, 3, 23, 36, 4, 4, 117, 3, 23, 23, 36, 36, 36, 3, 23, 3, 3, 3, 36, 4, 3]\n",
            "tbfk_0022\n",
            "tbfk_0022 4\n",
            "[4, 36, 36, 36, 36, 36, 3, 23, 36, 23, 3, 36, 23, 23, 36]\n",
            "tbfk_0023\n",
            "tbfk_0023 4\n",
            "[23, 36, 36, 23, 117, 3, 112, 3, 117]\n",
            "tbfk_0024\n",
            "tbfk_0024 4\n",
            "[96, 36, 3, 96, 36, 36, 23, 117, 36, 23, 3, 36, 23, 23, 36]\n",
            "tbfk_0025\n",
            "tbfk_0025 4\n",
            "[36, 4, 3, 3, 53, 3, 23, 23, 36, 3, 36, 36]\n",
            "tbfk_0026\n",
            "tbfk_0026 4\n",
            "[3, 36, 36, 36, 36, 3, 117, 23, 23, 23, 3]\n",
            "tbfk_0027\n",
            "tbfk_0027 4\n",
            "[36, 3, 3, 36, 36, 23, 3, 3, 36, 23, 3, 62, 3, 23, 3]\n",
            "tbfk_0028\n",
            "tbfk_0028 4\n",
            "[112, 3, 23, 36, 23, 112, 53, 3, 3, 3, 23, 23, 36, 3, 36, 112]\n",
            "tbfk_0029\n",
            "tbfk_0029 4\n",
            "[36, 3, 3, 96, 3, 36, 23, 23, 36, 36, 23, 53, 3, 3, 36]\n",
            "tbfk_0030\n",
            "tbfk_0030 4\n",
            "[4, 36, 3, 23, 36, 36, 3, 3, 3, 3, 36, 36, 23, 23, 23, 23, 36, 3, 3, 36]\n",
            "tbfk_0031\n",
            "tbfk_0031 4\n",
            "[36, 3, 36, 23, 36, 23, 3, 23, 36, 23, 3, 3, 3, 23, 23, 23, 3, 3, 36, 23, 23, 112, 36, 3, 3, 23]\n",
            "tbfk_0032\n",
            "tbfk_0032 4\n",
            "[36, 36, 3, 112, 96, 117, 117, 23, 36, 23, 36]\n",
            "tbfk_0033\n",
            "tbfk_0033 4\n",
            "[36, 3, 36, 3, 36, 23, 117, 36, 36, 3, 36, 23, 23, 36]\n",
            "tbfk_0034\n",
            "tbfk_0034 4\n",
            "[69, 23, 36, 23, 36, 36, 23, 62, 36, 36, 36, 112, 36]\n",
            "tbfk_0035\n",
            "tbfk_0035 4\n",
            "[36, 23, 36, 96, 36, 36, 36, 23, 23, 23, 3, 36, 23, 36, 3, 36, 3, 23, 36]\n",
            "tbfk_0036\n",
            "tbfk_0036 4\n",
            "[112, 36, 3, 36, 3, 4, 3, 23, 36, 23, 3, 36, 23, 23, 3]\n",
            "tbfk_0037\n",
            "tbfk_0037 4\n",
            "[36, 36, 3, 23, 36, 112, 36, 3, 117, 3, 36, 96, 23, 23, 117, 23, 36, 36, 3, 36]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BevjWtUJfEyU",
        "outputId": "6c6b85c0-74d8-4c7d-fd53-79c6f72dcbfa"
      },
      "source": [
        "seq_test_accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7984496124031008"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo3eO1FDd5Jc"
      },
      "source": [
        "syllable_df = Pupil_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yF7HSPQw563",
        "outputId": "4a97e063-6c6d-4731-f4a1-c1462658a994"
      },
      "source": [
        "model.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearClassifier(\n",
              "  (linear1): Linear(in_features=1024, out_features=128, bias=True)\n",
              "  (linear3): Linear(in_features=128, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB0xgSqFxWnv",
        "outputId": "b0b0d182-a49f-464d-fc35-bc1e84c34fdd"
      },
      "source": [
        "dir(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T_destination',\n",
              " '__annotations__',\n",
              " '__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattr__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__setstate__',\n",
              " '__sizeof__',\n",
              " '__slotnames__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_apply',\n",
              " '_backward_hooks',\n",
              " '_buffers',\n",
              " '_call_impl',\n",
              " '_forward_hooks',\n",
              " '_forward_pre_hooks',\n",
              " '_get_backward_hooks',\n",
              " '_get_name',\n",
              " '_is_full_backward_hook',\n",
              " '_load_from_state_dict',\n",
              " '_load_state_dict_pre_hooks',\n",
              " '_maybe_warn_non_full_backward_hook',\n",
              " '_modules',\n",
              " '_named_members',\n",
              " '_non_persistent_buffers_set',\n",
              " '_parameters',\n",
              " '_register_load_state_dict_pre_hook',\n",
              " '_register_state_dict_hook',\n",
              " '_replicate_for_data_parallel',\n",
              " '_save_to_state_dict',\n",
              " '_slow_forward',\n",
              " '_state_dict_hooks',\n",
              " '_version',\n",
              " 'add_module',\n",
              " 'apply',\n",
              " 'bfloat16',\n",
              " 'buffers',\n",
              " 'children',\n",
              " 'cpu',\n",
              " 'cuda',\n",
              " 'double',\n",
              " 'dump_patches',\n",
              " 'eval',\n",
              " 'extra_repr',\n",
              " 'float',\n",
              " 'forward',\n",
              " 'half',\n",
              " 'linear1',\n",
              " 'linear3',\n",
              " 'load_state_dict',\n",
              " 'modules',\n",
              " 'named_buffers',\n",
              " 'named_children',\n",
              " 'named_modules',\n",
              " 'named_parameters',\n",
              " 'parameters',\n",
              " 'register_backward_hook',\n",
              " 'register_buffer',\n",
              " 'register_forward_hook',\n",
              " 'register_forward_pre_hook',\n",
              " 'register_full_backward_hook',\n",
              " 'register_parameter',\n",
              " 'requires_grad_',\n",
              " 'share_memory',\n",
              " 'state_dict',\n",
              " 'to',\n",
              " 'train',\n",
              " 'training',\n",
              " 'type',\n",
              " 'xpu',\n",
              " 'zero_grad']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwmPY1zroxoJ",
        "outputId": "24c8bb46-b031-4181-e585-4f494ea3f90e"
      },
      "source": [
        "current_songfile = syllable_df.loc[syllable_df['key']==key]\n",
        "sequence_length = current_songfile['indvi'].values[-1]\n",
        "current_songfile = current_songfile.sample(frac=1, random_state=2021).reset_index(drop=True)\n",
        "sequence_individual_segment = []\n",
        "for i in range(0, sequence_length):\n",
        "  y_pred = model(torch.tensor(current_songfile['densenet121_features'].values[i]).resize_(1, 1024).to(device))\n",
        "  y_label_pred = torch.max(y_pred, 1)[1].to('cpu')\n",
        "  sequence_individual_segment.append(y_label_pred.numpy()[0])\n",
        "  print(y_pred, 1)\n",
        "  print(torch.max(y_pred, 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0000, 0.0000, 0.0000, 2.7909, 4.0111, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.3184, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         9.5615, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.0121,\n",
            "         0.0000, 0.0000, 0.1263, 0.0000, 0.0000, 0.0000, 0.4224, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 2.0825, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4880, 2.6779, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 3.5809, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 2.8426, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.5595, 0.0000, 1.2221, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         3.8517, 0.0000]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([9.5615], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([36], device='cuda:0'))\n",
            "tensor([[1.1688, 0.1338, 0.0000, 6.6032, 3.1111, 0.0000, 1.2138, 0.0000, 0.8164,\n",
            "         0.0000, 4.0234, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.5277, 0.0000, 0.0443, 3.6864, 1.0262, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9519, 0.0000,\n",
            "         7.7174, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.7225, 0.0000, 0.0000, 1.2657, 0.0000, 0.0000, 0.0000, 2.4757,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.4531,\n",
            "         0.0000, 0.0000, 0.0000, 1.6942, 0.0000, 0.0000, 1.0096, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 2.9798, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.9032, 1.5399, 0.0000, 0.0000, 0.0000, 0.0000, 0.1405,\n",
            "         0.4700, 1.2659, 0.0000, 0.0000, 2.3048, 0.0000, 2.9125, 1.6954, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 4.0565, 0.0000, 2.1022, 0.1930, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 1.0606, 0.0000, 0.0000, 0.3467, 0.0000,\n",
            "         4.0216, 0.0778, 1.8435, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         3.3744, 0.0000]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([7.7174], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([36], device='cuda:0'))\n",
            "tensor([[0.7528, 0.0000, 0.0000, 5.9708, 3.9976, 0.0000, 2.4183, 0.0000, 1.6568,\n",
            "         0.0000, 3.0026, 0.0000, 0.0000, 1.0262, 0.0000, 0.0000, 0.0000, 0.0444,\n",
            "         0.0000, 0.0000, 1.9758, 0.0000, 0.2984, 5.5116, 2.3578, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.5754, 0.0000,\n",
            "         4.4258, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1519, 0.0000,\n",
            "         0.0000, 1.6946, 0.0000, 0.0000, 2.8218, 0.0000, 0.0000, 0.0000, 4.4401,\n",
            "         0.0000, 0.0000, 0.0000, 0.8401, 0.0000, 0.0000, 0.0000, 0.0000, 4.8136,\n",
            "         0.6622, 0.0000, 0.0000, 2.3042, 0.0000, 0.0000, 1.2491, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 3.4795, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.6381, 3.9214, 0.0000, 0.0000, 0.0000, 0.0000, 0.4549,\n",
            "         0.8109, 1.1282, 0.0000, 0.0000, 3.3223, 0.0522, 4.0679, 3.3530, 0.1498,\n",
            "         0.0000, 0.0000, 0.0000, 3.7761, 0.0000, 3.2736, 1.8311, 0.0377, 0.0000,\n",
            "         0.0000, 0.0000, 1.5386, 0.3573, 0.0000, 0.0000, 0.6984, 0.5592, 0.0000,\n",
            "         5.2103, 0.7666, 1.1111, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         4.3093, 0.0000]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([5.9708], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([3], device='cuda:0'))\n",
            "tensor([[1.0460, 0.3324, 0.0000, 5.0447, 2.6199, 0.0000, 3.6557, 0.2317, 2.2091,\n",
            "         0.0000, 2.2702, 0.0000, 0.0000, 1.0010, 0.0000, 0.0000, 0.0000, 0.1064,\n",
            "         0.0000, 0.0000, 1.8875, 0.0000, 0.0689, 8.1184, 2.3789, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.5882, 0.0000, 0.0000, 3.1654, 0.0000,\n",
            "         1.9349, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1062, 0.0000,\n",
            "         0.0000, 0.9686, 1.9240, 0.0000, 3.6997, 0.0000, 0.0000, 0.0000, 3.3973,\n",
            "         1.0420, 0.0000, 0.0000, 1.7737, 0.3248, 0.0000, 0.0000, 0.0000, 3.5917,\n",
            "         0.8447, 0.4751, 0.0000, 3.7666, 0.0000, 0.0000, 2.8069, 0.0000, 0.0000,\n",
            "         0.0000, 0.2036, 0.0000, 2.8558, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 2.6153, 6.5446, 0.0000, 0.0000, 0.0000, 0.0000, 0.5568,\n",
            "         1.5577, 1.6507, 0.0000, 0.0000, 4.6757, 0.1654, 2.8906, 5.2211, 0.3764,\n",
            "         0.0000, 0.0000, 0.0000, 3.7742, 0.0000, 4.7031, 2.5844, 0.1167, 0.0000,\n",
            "         0.0000, 0.0000, 3.7949, 0.1458, 0.9141, 0.0000, 2.0371, 1.1806, 0.0000,\n",
            "         6.2879, 1.3613, 0.9086, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         2.1801, 0.0000]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([8.1184], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([23], device='cuda:0'))\n",
            "tensor([[0.0000, 0.2817, 0.0000, 2.8103, 2.3783, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0464, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         8.7619, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.5000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.9890,\n",
            "         0.0000, 0.0000, 0.3093, 0.5799, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 1.7682, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5315, 2.7328, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 2.0924, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 5.3126, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.8702, 0.0000, 0.6551, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         3.4684, 0.0000]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([8.7619], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([36], device='cuda:0'))\n",
            "tensor([[0.0000e+00, 3.9481e-01, 0.0000e+00, 3.0420e+00, 1.7639e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 1.1769e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         5.9282e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4009e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 2.3298e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         6.2583e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6100e-03,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8967e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 3.6297e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.6042e-01,\n",
            "         2.3737e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         2.1681e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 2.0440e-01, 0.0000e+00, 6.0891e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 7.2256e-01, 0.0000e+00, 1.7398e-01,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         2.1114e+00, 0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([6.0891], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([112], device='cuda:0'))\n",
            "tensor([[1.6881, 2.9532, 0.0000, 7.9387, 0.5529, 0.0000, 1.9865, 0.0000, 0.2598,\n",
            "         0.0000, 5.7101, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 3.7947, 0.0000, 0.2070, 5.2015, 2.0361, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.7863, 0.0000,\n",
            "         8.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0812, 0.0000,\n",
            "         0.0000, 1.4058, 0.0000, 0.0000, 2.0017, 0.0000, 0.0000, 0.0000, 5.3275,\n",
            "         0.0000, 0.0000, 0.0000, 0.1099, 0.8874, 0.0000, 0.0000, 0.0000, 4.6039,\n",
            "         3.5690, 0.0000, 0.0000, 6.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.6202,\n",
            "         0.0000, 0.0000, 0.0000, 2.1075, 0.0000, 0.0000, 3.9958, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 5.9620, 3.1677, 0.0000, 0.0000, 0.0000, 0.0000, 0.9422,\n",
            "         1.6816, 1.6182, 0.0000, 0.0000, 3.5975, 0.0000, 4.2792, 2.8162, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 5.8800, 0.0000, 2.7960, 1.3705, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.6130, 0.0000, 0.0000, 1.5880, 0.0000,\n",
            "         4.0886, 3.6218, 0.7904, 0.1631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.6645, 0.0000]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([8.8426], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([36], device='cuda:0'))\n",
            "tensor([[2.2321e+00, 1.1144e-01, 0.0000e+00, 8.0351e+00, 1.6442e+00, 0.0000e+00,\n",
            "         1.9842e+00, 0.0000e+00, 9.3652e-01, 0.0000e+00, 5.6258e+00, 0.0000e+00,\n",
            "         0.0000e+00, 1.3953e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 1.4511e+00, 0.0000e+00, 2.3449e-01, 4.6784e+00,\n",
            "         2.1686e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1768e+00, 0.0000e+00,\n",
            "         7.0701e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 1.0373e-01, 0.0000e+00, 0.0000e+00, 2.3011e+00, 0.0000e+00,\n",
            "         0.0000e+00, 2.0582e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6708e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 8.2687e-01, 6.2725e-01, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 3.1933e+00, 8.8571e-01, 0.0000e+00, 0.0000e+00,\n",
            "         1.1358e+00, 0.0000e+00, 0.0000e+00, 1.6940e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3673e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4833e+00,\n",
            "         3.2634e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.9322e-01,\n",
            "         0.0000e+00, 3.5833e-01, 0.0000e+00, 0.0000e+00, 2.5606e+00, 5.4611e-03,\n",
            "         2.5770e+00, 1.6139e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         5.4628e+00, 0.0000e+00, 2.1616e+00, 1.7095e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5593e-01, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 4.1647e+00, 9.1452e-01, 1.5958e+00,\n",
            "         8.8899e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         3.7526e+00, 0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([8.0351], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([3], device='cuda:0'))\n",
            "tensor([[0.0000, 0.0000, 0.0000, 4.8362, 4.3536, 0.0000, 2.2856, 0.0000, 1.7007,\n",
            "         0.0000, 1.5157, 0.0000, 0.0000, 0.0108, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 2.0389, 0.0000, 0.3004, 5.3568, 2.1169, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3194, 0.0000,\n",
            "         4.8440, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1947, 0.0000,\n",
            "         0.0000, 1.4822, 0.0469, 0.0000, 2.8413, 0.0000, 0.0000, 0.0000, 4.5914,\n",
            "         0.0000, 0.0000, 0.0000, 0.9766, 0.0000, 0.0000, 0.0000, 0.0000, 5.1256,\n",
            "         0.3689, 0.3725, 0.0000, 2.1407, 0.0000, 0.0000, 0.2786, 0.0000, 0.0000,\n",
            "         0.0000, 0.0683, 0.0000, 3.5685, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.6387, 3.9305, 0.0000, 0.0000, 0.0000, 0.0000, 0.4397,\n",
            "         0.7311, 1.0889, 0.0000, 0.0000, 3.2957, 0.1144, 4.5148, 4.2351, 0.6083,\n",
            "         0.0000, 0.0000, 0.0000, 2.4407, 0.0000, 3.5677, 1.2206, 0.7987, 0.0000,\n",
            "         0.0000, 0.0000, 2.1006, 0.0000, 0.0000, 0.0000, 1.7458, 0.6726, 0.0000,\n",
            "         5.7759, 0.6366, 0.8536, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         4.1555, 0.0000]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([5.7759], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([117], device='cuda:0'))\n",
            "tensor([[2.4993, 0.0551, 0.0000, 6.6670, 5.0187, 0.0000, 2.0664, 0.0000, 1.4664,\n",
            "         0.0000, 4.4553, 0.0000, 0.0000, 2.4025, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 2.3614, 0.0000, 0.6143, 5.3106, 2.0575, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4928, 0.0000,\n",
            "         6.0579, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3091, 0.0000,\n",
            "         0.0000, 2.4508, 0.0000, 0.0000, 2.6659, 0.0000, 0.0000, 0.0000, 4.6967,\n",
            "         0.0000, 0.0000, 0.0000, 0.8807, 0.9527, 0.0000, 0.0000, 0.0000, 5.4592,\n",
            "         0.5161, 0.2233, 0.0000, 3.4269, 0.0000, 0.0000, 1.1512, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 4.0544, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 2.4708, 3.8795, 0.0000, 0.0000, 0.0000, 0.0000, 1.3854,\n",
            "         1.1293, 0.9556, 0.0000, 0.0000, 3.5696, 0.0000, 4.6378, 3.4837, 0.4496,\n",
            "         0.1205, 0.0000, 0.0000, 5.1075, 0.0000, 3.4509, 1.1423, 0.5838, 0.0000,\n",
            "         0.0000, 0.0000, 1.9025, 1.4550, 0.0000, 0.0000, 0.7782, 0.6803, 0.0000,\n",
            "         6.5713, 0.9233, 2.4361, 0.5324, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         5.1606, 0.0000]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([6.6670], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([3], device='cuda:0'))\n",
            "tensor([[0.0385, 1.0388, 0.0000, 7.9079, 3.3287, 0.0000, 1.1015, 0.0000, 0.2074,\n",
            "         0.0000, 5.2437, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 2.5332, 0.0000, 0.0489, 3.0398, 1.5577, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2870, 0.0000,\n",
            "         8.1777, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 2.1127, 0.0000, 0.0000, 0.9404, 0.0000, 0.0000, 0.0000, 5.4884,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 4.8456,\n",
            "         1.3423, 0.0000, 0.0000, 3.0086, 0.0000, 0.0000, 0.0000, 0.0000, 0.3613,\n",
            "         0.0000, 0.0000, 0.0000, 3.2141, 0.0000, 0.0000, 1.4423, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 3.2933, 1.0138, 0.0000, 0.0000, 0.0000, 0.0000, 0.2581,\n",
            "         0.0000, 0.8562, 0.0000, 0.0000, 1.7379, 0.0000, 4.4836, 1.0336, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 4.8416, 0.0000, 1.1703, 0.4702, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.2507, 0.0000, 0.0000, 0.3453, 0.0000,\n",
            "         3.6723, 1.3588, 1.7559, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         4.3470, 0.0000]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([8.1777], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([36], device='cuda:0'))\n",
            "tensor([[0.0000, 0.0000, 0.0000, 1.4758, 3.4288, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         3.5888, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.8396,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.7981,\n",
            "         0.0000, 0.0000, 0.0000, 0.5907, 0.0000, 0.0000, 0.2826, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 2.2008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4201, 3.8619, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.8315, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 3.3357, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.9508, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         2.0320, 0.0000]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([3.8619], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([96], device='cuda:0'))\n",
            "tensor([[2.4921, 0.0000, 0.0000, 5.5574, 3.3495, 0.0000, 3.5243, 0.0000, 1.6331,\n",
            "         0.0000, 2.3943, 0.0000, 0.0000, 0.7783, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.3237, 0.0000, 0.0000, 7.8923, 2.0314, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.9637, 0.0000, 0.0000, 3.5769, 0.0000,\n",
            "         3.1195, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1814, 0.0000,\n",
            "         0.0000, 0.9906, 2.0417, 0.0000, 3.7718, 0.0000, 0.0000, 0.0000, 2.9680,\n",
            "         0.9837, 0.0000, 0.0000, 2.0207, 0.1604, 0.0000, 0.0000, 0.1830, 3.7249,\n",
            "         0.1544, 0.3180, 0.0000, 0.9299, 0.0000, 0.0000, 3.4494, 0.0000, 0.0000,\n",
            "         0.0000, 0.1783, 0.0000, 2.9056, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.2515, 6.2615, 0.0000, 0.0000, 0.0000, 0.0000, 0.2192,\n",
            "         1.6630, 1.0585, 0.0000, 0.0000, 4.3927, 0.2572, 3.0793, 4.7432, 0.2963,\n",
            "         0.0000, 0.0000, 0.0000, 4.1120, 0.0000, 4.5670, 2.9915, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0448, 0.5658, 0.0000, 0.0000, 1.7229, 0.9195, 0.0000,\n",
            "         6.2979, 0.8944, 1.4479, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         2.7516, 0.0000]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([7.8923], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([23], device='cuda:0'))\n",
            "tensor([[2.8054e+00, 0.0000e+00, 0.0000e+00, 5.1518e+00, 2.4189e+00, 0.0000e+00,\n",
            "         3.6000e+00, 0.0000e+00, 1.9234e+00, 0.0000e+00, 1.8345e+00, 0.0000e+00,\n",
            "         0.0000e+00, 1.2883e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3111e-03,\n",
            "         0.0000e+00, 0.0000e+00, 1.4011e+00, 0.0000e+00, 0.0000e+00, 7.9889e+00,\n",
            "         2.3428e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 6.6450e-01, 0.0000e+00, 0.0000e+00, 3.3357e+00, 0.0000e+00,\n",
            "         2.3627e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 1.3944e-02, 0.0000e+00, 0.0000e+00, 8.2964e-01, 1.9211e+00,\n",
            "         0.0000e+00, 3.7947e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7287e+00,\n",
            "         8.8278e-01, 0.0000e+00, 0.0000e+00, 2.0043e+00, 2.1806e-01, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 3.0789e+00, 5.5043e-01, 2.8260e-01, 0.0000e+00,\n",
            "         1.0734e+00, 0.0000e+00, 0.0000e+00, 4.0593e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 2.7149e-01, 0.0000e+00, 2.6975e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2089e+00,\n",
            "         6.4953e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0578e-01,\n",
            "         1.4537e+00, 1.1308e+00, 0.0000e+00, 0.0000e+00, 4.6445e+00, 3.2858e-01,\n",
            "         2.4037e+00, 4.8179e+00, 2.9797e-01, 4.5717e-01, 0.0000e+00, 0.0000e+00,\n",
            "         3.7329e+00, 0.0000e+00, 4.7433e+00, 2.8322e+00, 5.1327e-02, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 1.1061e+00, 9.9605e-01, 0.0000e+00, 0.0000e+00,\n",
            "         1.7438e+00, 1.0354e+00, 0.0000e+00, 5.8882e+00, 9.3726e-01, 1.0275e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         2.2211e+00, 0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([7.9889], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([23], device='cuda:0'))\n",
            "tensor([[0.6856, 0.0000, 0.0000, 5.5693, 4.4405, 0.0000, 2.3727, 0.0000, 2.3297,\n",
            "         0.0000, 2.4801, 0.0000, 0.0000, 2.8378, 0.0000, 0.0000, 0.0000, 0.0854,\n",
            "         0.0000, 0.0000, 2.5024, 0.0000, 0.8341, 5.2568, 2.9275, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3638, 0.0000,\n",
            "         1.9947, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2155, 0.0000,\n",
            "         0.0000, 2.1326, 0.0000, 0.0000, 2.4581, 0.0000, 0.0000, 0.0000, 4.8654,\n",
            "         0.0000, 0.0000, 0.0000, 1.1412, 0.3494, 0.0000, 0.0000, 0.0000, 4.6863,\n",
            "         1.1156, 0.2938, 0.0000, 3.4728, 0.0000, 0.0000, 2.4736, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 4.0570, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 2.0003, 4.4195, 0.0000, 0.0000, 0.0000, 0.0000, 1.2047,\n",
            "         0.2245, 1.4093, 0.0000, 0.0000, 3.6730, 0.0000, 4.0038, 3.9918, 0.5980,\n",
            "         0.8828, 0.0000, 0.0000, 3.9829, 0.0000, 3.7259, 1.4478, 0.8680, 0.0000,\n",
            "         0.0000, 0.0000, 5.5859, 1.1922, 1.1433, 0.0000, 1.4002, 0.1175, 0.0000,\n",
            "         5.9084, 0.6514, 1.0614, 0.2880, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         4.5390, 0.0000]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([5.9084], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([117], device='cuda:0'))\n",
            "tensor([[2.8773, 0.0404, 0.0000, 5.6897, 3.0678, 0.0000, 3.3794, 0.0000, 2.1973,\n",
            "         0.0000, 2.7849, 0.0000, 0.0000, 1.3060, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0358, 0.0000, 0.0000, 7.4945, 1.9057, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.6333, 0.0000, 0.0000, 3.2336, 0.0000,\n",
            "         3.2091, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2030, 0.0000,\n",
            "         0.0000, 1.2820, 1.6266, 0.0000, 3.7979, 0.0000, 0.0000, 0.0000, 2.4467,\n",
            "         0.9595, 0.0000, 0.0000, 2.0965, 0.1918, 0.0000, 0.0000, 0.0000, 3.2927,\n",
            "         0.0000, 0.2183, 0.0000, 1.0658, 0.0000, 0.0000, 3.2249, 0.0000, 0.0000,\n",
            "         0.0000, 0.2532, 0.0000, 3.0706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.6747, 5.7869, 0.0000, 0.0000, 0.0000, 0.0000, 0.1976,\n",
            "         1.6664, 1.1508, 0.0000, 0.0000, 4.2680, 0.3263, 2.2330, 4.7503, 0.4275,\n",
            "         0.0479, 0.0000, 0.0000, 4.4466, 0.0000, 4.3152, 2.5355, 0.0913, 0.0000,\n",
            "         0.0000, 0.0000, 1.7028, 1.0333, 0.0000, 0.0000, 1.7272, 1.0676, 0.0000,\n",
            "         6.0929, 0.7052, 1.5900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         2.9303, 0.0000]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([7.4945], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([23], device='cuda:0'))\n",
            "tensor([[0.0000, 0.9292, 0.0000, 7.6754, 3.7571, 0.0000, 0.8722, 0.0000, 0.0317,\n",
            "         0.0000, 5.5732, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 2.2556, 0.0000, 0.0000, 2.5122, 1.4572, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.1235, 0.0000,\n",
            "         8.0784, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 2.1574, 0.0000, 0.0000, 0.5294, 0.0000, 0.0000, 0.0000, 4.9903,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 4.5124,\n",
            "         0.9751, 0.0000, 0.0000, 2.5136, 0.0000, 0.0000, 0.0000, 0.0000, 0.3961,\n",
            "         0.0000, 0.0000, 0.0000, 3.0982, 0.0000, 0.0000, 0.4425, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 2.7362, 0.8132, 0.0000, 0.0000, 0.0000, 0.0000, 0.2686,\n",
            "         0.0000, 0.6358, 0.0000, 0.0000, 1.4428, 0.0000, 4.1142, 0.3811, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 4.7608, 0.0000, 0.7758, 0.4522, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.5311, 0.0000, 0.0000, 0.0729, 0.0000,\n",
            "         3.1682, 0.9024, 1.6677, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         4.7700, 0.0000]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([8.0784], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([36], device='cuda:0'))\n",
            "tensor([[1.2689, 1.2886, 0.0000, 7.6130, 2.1141, 0.0000, 1.5394, 0.0000, 0.6783,\n",
            "         0.0000, 5.6863, 0.0000, 0.0000, 0.7129, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 2.7079, 0.0000, 0.4013, 3.7579, 2.4440, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.1082, 0.0000,\n",
            "         9.2969, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.8338, 0.0000, 0.0000, 1.3986, 0.0000, 0.0000, 0.0000, 4.7998,\n",
            "         0.0000, 0.0000, 0.0000, 0.4157, 0.9729, 0.0000, 0.0000, 0.0000, 3.9421,\n",
            "         2.2252, 0.0000, 0.0000, 3.5696, 0.0000, 0.0000, 0.0000, 0.0000, 0.3365,\n",
            "         0.0000, 0.0000, 0.0000, 2.7446, 0.0000, 0.0000, 1.2050, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 3.2146, 2.8170, 0.0000, 0.0000, 0.0000, 0.0000, 0.9933,\n",
            "         0.3370, 0.6386, 0.0000, 0.0000, 2.3713, 0.0000, 3.5127, 1.4983, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 5.7586, 0.0000, 2.0008, 1.4612, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.1544, 0.0000, 0.0158, 0.0000, 0.0000, 0.5637, 0.0000,\n",
            "         4.0661, 1.9570, 1.5509, 0.3678, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         3.7911, 0.0000]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([9.2969], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([36], device='cuda:0'))\n",
            "tensor([[0.8120, 0.9313, 0.0000, 7.6562, 2.3996, 0.0000, 1.1707, 0.0000, 0.4093,\n",
            "         0.0000, 5.1264, 0.0000, 0.0000, 0.2577, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.9585, 0.0000, 0.0642, 3.2517, 1.6050, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2022, 0.0000,\n",
            "         6.4647, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.9389, 0.0000, 0.0000, 0.7225, 0.0000, 0.0000, 0.0000, 3.8151,\n",
            "         0.0000, 0.0000, 0.0000, 0.1304, 0.0000, 0.0000, 0.0000, 0.0000, 3.5687,\n",
            "         1.0654, 0.0000, 0.0000, 1.8606, 0.0000, 0.0000, 0.3839, 0.0000, 0.3015,\n",
            "         0.0000, 0.0000, 0.0000, 2.7159, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 2.4111, 1.4022, 0.0000, 0.0000, 0.0000, 0.0000, 0.2206,\n",
            "         0.0000, 1.0764, 0.0000, 0.0000, 1.9510, 0.0000, 3.0478, 0.5843, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 5.1016, 0.0000, 1.0412, 0.7343, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 2.1516, 0.0000, 0.0000, 0.1607, 0.0000,\n",
            "         2.9690, 0.9424, 1.1951, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         3.6950, 0.0000]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([7.6562], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([3], device='cuda:0'))\n",
            "tensor([[0.0000, 0.4517, 0.0000, 1.2231, 3.4812, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         7.6108, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5515,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 4.2525,\n",
            "         0.0000, 0.0000, 0.0000, 0.0858, 0.0000, 0.0000, 1.0105, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 1.6319, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0953, 0.0000, 0.1399, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.8830, 0.0000, 0.0000, 0.0000, 0.0000, 0.1623, 4.0872, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 1.7198, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4832, 0.0000,\n",
            "         1.4113, 0.0000, 0.1992, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.8085, 0.0000]], device='cuda:0', grad_fn=<ReluBackward0>) 1\n",
            "torch.return_types.max(\n",
            "values=tensor([7.6108], device='cuda:0', grad_fn=<MaxBackward0>),\n",
            "indices=tensor([36], device='cuda:0'))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyQ__NkrwsB5",
        "outputId": "154ad6a4-6bfe-45a3-b116-d9dcf7313457"
      },
      "source": [
        "i = 1\n",
        "y_pred = model(torch.tensor(current_songfile['densenet121_features'].values[i]).resize_(1, 1024).to(device))\n",
        "y_label_pred = torch.max(y_pred, 1)[1].to('cpu')\n",
        "sequence_individual_segment.append(y_label_pred.numpy()[0])\n",
        "print(y_pred, 1)\n",
        "print(torch.max(y_pred, 1)[1].to('cpu'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5214, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         2.3981, 0.0000, 2.3481, 0.4892, 0.0000, 0.0000, 2.8020, 0.0000, 0.2345,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 4.3205,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.6267, 1.3211, 0.0000, 2.2244, 0.0000,\n",
            "         0.0000, 0.5694, 0.1051, 0.0000, 0.0000, 0.0000, 0.6227, 0.0000, 0.0000,\n",
            "         1.4789, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.7624, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0101, 3.6504, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.3121, 0.0000, 0.0000, 0.0000, 0.5687,\n",
            "         0.0000, 0.0000, 2.7355, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.9918, 0.0000, 0.3908, 1.5499, 0.0000, 0.0000, 0.0000,\n",
            "         2.5221, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6991, 0.5555,\n",
            "         0.1757, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2310, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.7423, 0.0000, 3.8103,\n",
            "         0.0000, 0.0000, 0.0000, 1.9975, 0.0000, 0.0000, 0.0556, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 2.2192, 0.5925, 0.0000, 0.0000, 0.3143,\n",
            "         1.6044, 0.0000, 0.7489, 0.0000, 1.0719, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.8283, 0.0000, 1.8555, 0.0000, 0.0000, 0.0000, 0.5081, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 1.7854, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.7644, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         4.1595, 0.8420, 0.0000, 0.0000, 0.0000, 0.0927, 0.0000, 0.4493, 0.0000,\n",
            "         1.1145, 0.0000, 0.0000, 0.0000, 0.0000, 0.4505, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0231, 2.1074, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 2.8058, 0.0000, 0.3925, 0.0000, 0.0000, 1.7333, 1.7670, 1.1707,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 2.5983, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.3950, 0.0000, 0.0000, 0.0000, 0.0000, 4.4119, 0.5828, 0.0855, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
            "       grad_fn=<ReluBackward0>) 1\n",
            "tensor([248])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taztyIJCHGle"
      },
      "source": [
        "model_new = LinearClassifier(hidden_dim=128, output_dim=len(np.unique(targets))).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jXu9tADHKiE",
        "outputId": "0060d451-2544-4094-ff41-0516521be71d"
      },
      "source": [
        "model_new(torch.tensor(current_songfile['densenet121_features'].values[i]).resize_(1, 1024).to(device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2154, 0.4629, 1.0233, 0.4854, 0.0000]], device='cuda:0',\n",
              "       grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rpqcu95DUWN"
      },
      "source": [
        "x1 = F.relu(model.linear1(torch.tensor(current_songfile['densenet121_features'].values[i]).resize_(1, 1024).to(device)))\n",
        "x2 = model.linear3(x1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ix_ByyiD8QF",
        "outputId": "c0fc8fd3-e558-4cf0-cd5a-953f80c53415"
      },
      "source": [
        "x2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 9.2596, -7.3077, -0.2292, -9.3619,  6.4947]], device='cuda:0',\n",
              "       grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMjqgLJJeAIR"
      },
      "source": [
        "current_songfile = syllable_df.loc[syllable_df['key']==key]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgkp03HneIcw"
      },
      "source": [
        "current_songfile = current_songfile.sample(frac=1, random_state=2021).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "hoNvn871ePgM",
        "outputId": "ad7521b4-9644-4fd5-e194-f98307fcdc24"
      },
      "source": [
        "current_songfile"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>labels</th>\n",
              "      <th>indv</th>\n",
              "      <th>indvi</th>\n",
              "      <th>key</th>\n",
              "      <th>audio</th>\n",
              "      <th>rate</th>\n",
              "      <th>spectrogram</th>\n",
              "      <th>labels_indv</th>\n",
              "      <th>densenet121_features</th>\n",
              "      <th>Nest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.505598</td>\n",
              "      <td>0.552582</td>\n",
              "      <td>i</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>4</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[-0.00019636632, -0.00011087562, 0.00011899666...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_i</td>\n",
              "      <td>[0.00035573693, 0.0059586586, 0.0024591961, 0....</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.535054</td>\n",
              "      <td>1.658591</td>\n",
              "      <td>a</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>10</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[-0.0036366456, -0.0036538602, -0.0037256074, ...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_a</td>\n",
              "      <td>[0.0004193636, 0.004206814, 0.002283146, 0.002...</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.786278</td>\n",
              "      <td>3.976550</td>\n",
              "      <td>c</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>20</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[-0.0018684561, -0.0028802825, -0.0035185874, ...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_c</td>\n",
              "      <td>[0.00047820015, 0.0053550308, 0.0019423104, 0....</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.687298</td>\n",
              "      <td>1.930473</td>\n",
              "      <td>b</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>11</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[0.0025684354, 0.0027094097, 0.0023444323, 0.0...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_b</td>\n",
              "      <td>[0.00040158973, 0.004648357, 0.0024476568, 0.0...</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.408206</td>\n",
              "      <td>0.455666</td>\n",
              "      <td>i</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>3</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[0.00034006516, 0.0004238533, 0.0011047416, 0....</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_i</td>\n",
              "      <td>[0.00035663077, 0.005356712, 0.0025559189, 0.0...</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.584963</td>\n",
              "      <td>0.643262</td>\n",
              "      <td>i</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>5</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[-0.0026012685, -0.0030763852, -0.0028974195, ...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_i</td>\n",
              "      <td>[0.00035347737, 0.00536384, 0.0023823886, 0.00...</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.355280</td>\n",
              "      <td>1.513512</td>\n",
              "      <td>d</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>9</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[0.0033217536, 0.0041787946, 0.0036869275, 0.0...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_d</td>\n",
              "      <td>[0.00044134023, 0.0058528148, 0.0017248018, 0....</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3.156187</td>\n",
              "      <td>3.319385</td>\n",
              "      <td>D</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>17</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[-0.00017250945, -0.0002446745, -0.00015178893...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_D</td>\n",
              "      <td>[0.00049479626, 0.0063569997, 0.0015375959, 0....</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.969815</td>\n",
              "      <td>2.157729</td>\n",
              "      <td>c</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>12</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[-0.0028657683, -0.0021657909, -0.0012662874, ...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_c</td>\n",
              "      <td>[0.0005002575, 0.0046948665, 0.0022767636, 0.0...</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.098274</td>\n",
              "      <td>1.280020</td>\n",
              "      <td>c</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>8</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[0.00032339332, 0.00075202825, 0.0012105576, 0...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_c</td>\n",
              "      <td>[0.00055941334, 0.0055074333, 0.0018285438, 0....</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.668568</td>\n",
              "      <td>0.789702</td>\n",
              "      <td>a</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>6</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[-0.0035794019, -0.0044663996, -0.004429011, -...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_a</td>\n",
              "      <td>[0.00048291424, 0.004363426, 0.0020152628, 0.0...</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.278931</td>\n",
              "      <td>0.319679</td>\n",
              "      <td>i</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>2</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[0.0029296475, 0.004062663, 0.0043832357, 0.00...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_i</td>\n",
              "      <td>[0.00035249168, 0.0062291226, 0.0023399917, 0....</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3.503013</td>\n",
              "      <td>3.741425</td>\n",
              "      <td>b</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>19</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[4.1878673e-05, 0.0005760747, 0.00046885127, -...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_b</td>\n",
              "      <td>[0.0004765579, 0.004761797, 0.0026407782, 0.00...</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2.579634</td>\n",
              "      <td>2.821131</td>\n",
              "      <td>b</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>15</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[-0.0014478292, -0.0012196003, -0.0006711715, ...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_b</td>\n",
              "      <td>[0.00040689958, 0.0040247, 0.0022268854, 0.001...</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2.858999</td>\n",
              "      <td>3.048727</td>\n",
              "      <td>c</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>16</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[0.0013129083, 0.00083894224, 0.00094999745, 0...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_c</td>\n",
              "      <td>[0.0004801523, 0.0050035305, 0.0027258475, 0.0...</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.817094</td>\n",
              "      <td>1.056097</td>\n",
              "      <td>b</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>7</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[-3.6240363e-05, 0.001372807, 0.0025845855, 0....</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_b</td>\n",
              "      <td>[0.0004689582, 0.00443705, 0.0023677256, 0.002...</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3.350133</td>\n",
              "      <td>3.468591</td>\n",
              "      <td>a</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>18</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[0.004827889, 0.0046767294, 0.0039395783, 0.00...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_a</td>\n",
              "      <td>[0.00045768137, 0.003787619, 0.0020910471, 0.0...</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2.243194</td>\n",
              "      <td>2.398614</td>\n",
              "      <td>d</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>13</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[-0.0040601245, -0.004567173, -0.0047555924, -...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_d</td>\n",
              "      <td>[0.00045013093, 0.005651361, 0.0017559798, 0.0...</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2.421970</td>\n",
              "      <td>2.547140</td>\n",
              "      <td>a</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>14</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[0.0028934565, 0.0036356554, 0.0043808385, 0.0...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_a</td>\n",
              "      <td>[0.00043503588, 0.0044180043, 0.0023802228, 0....</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.084441</td>\n",
              "      <td>0.115779</td>\n",
              "      <td>i</td>\n",
              "      <td>tbfk</td>\n",
              "      <td>1</td>\n",
              "      <td>tbfk_0037</td>\n",
              "      <td>[0.002425233, 0.0031203441, 0.0033083023, 0.00...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>tbfk_i</td>\n",
              "      <td>[0.00034361274, 0.0064479127, 0.002264474, 0.0...</td>\n",
              "      <td>Nest5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    start_time  ...   Nest\n",
              "0     0.505598  ...  Nest5\n",
              "1     1.535054  ...  Nest5\n",
              "2     3.786278  ...  Nest5\n",
              "3     1.687298  ...  Nest5\n",
              "4     0.408206  ...  Nest5\n",
              "5     0.584963  ...  Nest5\n",
              "6     1.355280  ...  Nest5\n",
              "7     3.156187  ...  Nest5\n",
              "8     1.969815  ...  Nest5\n",
              "9     1.098274  ...  Nest5\n",
              "10    0.668568  ...  Nest5\n",
              "11    0.278931  ...  Nest5\n",
              "12    3.503013  ...  Nest5\n",
              "13    2.579634  ...  Nest5\n",
              "14    2.858999  ...  Nest5\n",
              "15    0.817094  ...  Nest5\n",
              "16    3.350133  ...  Nest5\n",
              "17    2.243194  ...  Nest5\n",
              "18    2.421970  ...  Nest5\n",
              "19    0.084441  ...  Nest5\n",
              "\n",
              "[20 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_2J6y1sFmdy",
        "outputId": "59bb45f3-1f63-42a4-f232-e377c7d7582a"
      },
      "source": [
        "test_accuracy = test_sequence_eval(model, Pupil_dataset, encoded_targets_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cyea_0000\n",
            "cyea_0001\n",
            "cyea_0002\n",
            "cyea_0003\n",
            "cyea_0004\n",
            "cyea_0005\n",
            "cyea_0006\n",
            "cyea_0007\n",
            "cyea_0008\n",
            "cyea_0009\n",
            "cyea_0010\n",
            "cyea_0011\n",
            "cyea_0012\n",
            "cyea_0013\n",
            "cyea_0014\n",
            "cyea_0015\n",
            "cyea_0016\n",
            "cyea_0017\n",
            "cyea_0018\n",
            "cyea_0019\n",
            "cyea_0020\n",
            "cyea_0021\n",
            "cyea_0022\n",
            "cyea_0023\n",
            "cyea_0024\n",
            "cyea_0025\n",
            "cyea_0026\n",
            "cyea_0027\n",
            "cyea_0028\n",
            "cyea_0029\n",
            "cyea_0030\n",
            "cyea_0031\n",
            "cyea_0032\n",
            "cyea_0033\n",
            "cyea_0034\n",
            "cyea_0035\n",
            "cyea_0036\n",
            "cyea_0037\n",
            "cyea_0038\n",
            "cyea_0039\n",
            "cyea_0040\n",
            "hphi_0000\n",
            "hphi_0001\n",
            "hphi_0002\n",
            "hphi_0003\n",
            "hphi_0004\n",
            "hphi_0005\n",
            "hphi_0006\n",
            "hphi_0007\n",
            "hphi_0008\n",
            "hphi_0009\n",
            "hphi_0010\n",
            "hphi_0011\n",
            "hphi_0012\n",
            "hphi_0013\n",
            "hphi_0014\n",
            "hphi_0015\n",
            "hphi_0016\n",
            "hphi_0017\n",
            "hphi_0018\n",
            "hphi_0019\n",
            "hphi_0020\n",
            "hphi_0021\n",
            "hphi_0022\n",
            "hphi_0023\n",
            "hphi_0024\n",
            "hphi_0025\n",
            "hphi_0026\n",
            "hphi_0027\n",
            "hphi_0028\n",
            "hphi_0029\n",
            "hphi_0030\n",
            "hphi_0031\n",
            "hphi_0032\n",
            "hphi_0033\n",
            "hphi_0034\n",
            "hphi_0035\n",
            "hphi_0036\n",
            "hphi_0037\n",
            "hphi_0038\n",
            "hphi_0039\n",
            "hphi_0040\n",
            "hphi_0041\n",
            "hphi_0042\n",
            "hphi_0043\n",
            "phpd_0000\n",
            "phpd_0001\n",
            "phpd_0002\n",
            "phpd_0003\n",
            "phpd_0004\n",
            "phpd_0005\n",
            "phpd_0006\n",
            "phpd_0007\n",
            "phpd_0008\n",
            "phpd_0009\n",
            "phpd_0010\n",
            "phpd_0011\n",
            "phpd_0012\n",
            "phpd_0013\n",
            "phpd_0014\n",
            "phpd_0015\n",
            "phpd_0016\n",
            "phpd_0017\n",
            "phpd_0018\n",
            "phpd_0019\n",
            "phpd_0020\n",
            "phpd_0021\n",
            "phpd_0022\n",
            "phpd_0023\n",
            "phpd_0024\n",
            "phpd_0025\n",
            "phpd_0026\n",
            "phpd_0027\n",
            "phpd_0028\n",
            "phpd_0029\n",
            "phpd_0030\n",
            "phpd_0031\n",
            "phpd_0032\n",
            "phpd_0033\n",
            "phpd_0034\n",
            "phpd_0035\n",
            "phpd_0036\n",
            "phpd_0037\n",
            "phpd_0038\n",
            "phpd_0039\n",
            "phpd_0040\n",
            "phpd_0041\n",
            "phpd_0042\n",
            "phpd_0043\n",
            "phpd_0044\n",
            "phpd_0045\n",
            "phpd_0046\n",
            "phpd_0047\n",
            "phpd_0048\n",
            "phpd_0049\n",
            "phpd_0050\n",
            "phpd_0051\n",
            "phpd_0052\n",
            "phpd_0053\n",
            "phpd_0054\n",
            "phpd_0055\n",
            "phpd_0056\n",
            "phpd_0057\n",
            "phpd_0058\n",
            "phpd_0059\n",
            "phpd_0060\n",
            "phpd_0061\n",
            "phpd_0062\n",
            "cxyc_0000\n",
            "cxyc_0001\n",
            "cxyc_0002\n",
            "cxyc_0003\n",
            "cxyc_0004\n",
            "cxyc_0005\n",
            "cxyc_0006\n",
            "cxyc_0007\n",
            "cxyc_0008\n",
            "cxyc_0009\n",
            "cxyc_0010\n",
            "cxyc_0011\n",
            "cxyc_0012\n",
            "cxyc_0013\n",
            "cxyc_0014\n",
            "cxyc_0015\n",
            "cxyc_0016\n",
            "cxyc_0017\n",
            "cxyc_0018\n",
            "cxyc_0019\n",
            "cxyc_0020\n",
            "cxyc_0021\n",
            "cxyc_0022\n",
            "cxyc_0023\n",
            "cxyc_0024\n",
            "cxyc_0025\n",
            "cxyc_0026\n",
            "nsrn_0000\n",
            "nsrn_0001\n",
            "nsrn_0002\n",
            "nsrn_0003\n",
            "nsrn_0004\n",
            "nsrn_0005\n",
            "nsrn_0006\n",
            "nsrn_0007\n",
            "nsrn_0008\n",
            "nsrn_0009\n",
            "nsrn_0010\n",
            "nsrn_0011\n",
            "nsrn_0012\n",
            "nsrn_0013\n",
            "nsrn_0014\n",
            "nsrn_0015\n",
            "nsrn_0016\n",
            "nsrn_0017\n",
            "nsrn_0018\n",
            "nsrn_0019\n",
            "nsrn_0020\n",
            "nsrn_0021\n",
            "nsrn_0022\n",
            "nsrn_0023\n",
            "nsrn_0024\n",
            "nsrn_0025\n",
            "nsrn_0026\n",
            "nsrn_0027\n",
            "nsrn_0028\n",
            "nsrn_0029\n",
            "nsrn_0030\n",
            "nsrn_0031\n",
            "nsrn_0032\n",
            "nsrn_0033\n",
            "nsrn_0034\n",
            "nsrn_0035\n",
            "nsrn_0036\n",
            "nsrn_0037\n",
            "qfod_0000\n",
            "qfod_0001\n",
            "qfod_0002\n",
            "qfod_0003\n",
            "qfod_0004\n",
            "qfod_0005\n",
            "qfod_0006\n",
            "qfod_0007\n",
            "qfod_0008\n",
            "qfod_0009\n",
            "qfod_0010\n",
            "qfod_0011\n",
            "qfod_0012\n",
            "qfod_0013\n",
            "qfod_0014\n",
            "qfod_0015\n",
            "qfod_0016\n",
            "qfod_0017\n",
            "qfod_0018\n",
            "qfod_0019\n",
            "qfod_0020\n",
            "khxv_0000\n",
            "khxv_0001\n",
            "khxv_0002\n",
            "khxv_0003\n",
            "khxv_0004\n",
            "khxv_0005\n",
            "khxv_0006\n",
            "khxv_0007\n",
            "khxv_0008\n",
            "khxv_0009\n",
            "khxv_0010\n",
            "khxv_0011\n",
            "khxv_0012\n",
            "khxv_0013\n",
            "khxv_0014\n",
            "khxv_0015\n",
            "khxv_0016\n",
            "khxv_0017\n",
            "khxv_0018\n",
            "khxv_0019\n",
            "khxv_0020\n",
            "khxv_0021\n",
            "khxv_0022\n",
            "khxv_0023\n",
            "khxv_0024\n",
            "khxv_0025\n",
            "khxv_0026\n",
            "khxv_0027\n",
            "khxv_0028\n",
            "khxv_0029\n",
            "khxv_0030\n",
            "khxv_0031\n",
            "khxv_0032\n",
            "khxv_0033\n",
            "khxv_0034\n",
            "khxv_0035\n",
            "khxv_0036\n",
            "khxv_0037\n",
            "khxv_0038\n",
            "khxv_0039\n",
            "khxv_0040\n",
            "khxv_0041\n",
            "khxv_0042\n",
            "khxv_0043\n",
            "khxv_0044\n",
            "khxv_0045\n",
            "khxv_0046\n",
            "khxv_0047\n",
            "oogw_0000\n",
            "oogw_0001\n",
            "oogw_0002\n",
            "oogw_0003\n",
            "oogw_0004\n",
            "oogw_0005\n",
            "oogw_0006\n",
            "oogw_0007\n",
            "oogw_0008\n",
            "oogw_0009\n",
            "oogw_0010\n",
            "oogw_0011\n",
            "oogw_0012\n",
            "oogw_0013\n",
            "oogw_0014\n",
            "oogw_0015\n",
            "oogw_0016\n",
            "oogw_0017\n",
            "oogw_0018\n",
            "oogw_0019\n",
            "oogw_0020\n",
            "oogw_0021\n",
            "oogw_0022\n",
            "oogw_0023\n",
            "oogw_0024\n",
            "oogw_0025\n",
            "oogw_0026\n",
            "oogw_0027\n",
            "oogw_0028\n",
            "oogw_0029\n",
            "oogw_0030\n",
            "oogw_0031\n",
            "oogw_0032\n",
            "oogw_0033\n",
            "oogw_0034\n",
            "oogw_0035\n",
            "kcos_0000\n",
            "kcos_0001\n",
            "kcos_0002\n",
            "kcos_0003\n",
            "kcos_0004\n",
            "kcos_0005\n",
            "kcos_0006\n",
            "kcos_0007\n",
            "kcos_0008\n",
            "kcos_0009\n",
            "kcos_0010\n",
            "kcos_0011\n",
            "kcos_0012\n",
            "kcos_0013\n",
            "kcos_0014\n",
            "kcos_0015\n",
            "kcos_0016\n",
            "kcos_0017\n",
            "kcos_0018\n",
            "kcos_0019\n",
            "kcos_0020\n",
            "kcos_0021\n",
            "kcos_0022\n",
            "kcos_0023\n",
            "kcos_0024\n",
            "kcos_0025\n",
            "kcos_0026\n",
            "kcos_0027\n",
            "kcos_0028\n",
            "kcos_0029\n",
            "kcos_0030\n",
            "tbfk_0000\n",
            "tbfk_0001\n",
            "tbfk_0002\n",
            "tbfk_0003\n",
            "tbfk_0004\n",
            "tbfk_0005\n",
            "tbfk_0006\n",
            "tbfk_0007\n",
            "tbfk_0008\n",
            "tbfk_0009\n",
            "tbfk_0010\n",
            "tbfk_0011\n",
            "tbfk_0012\n",
            "tbfk_0013\n",
            "tbfk_0014\n",
            "tbfk_0015\n",
            "tbfk_0016\n",
            "tbfk_0017\n",
            "tbfk_0018\n",
            "tbfk_0019\n",
            "tbfk_0020\n",
            "tbfk_0021\n",
            "tbfk_0022\n",
            "tbfk_0023\n",
            "tbfk_0024\n",
            "tbfk_0025\n",
            "tbfk_0026\n",
            "tbfk_0027\n",
            "tbfk_0028\n",
            "tbfk_0029\n",
            "tbfk_0030\n",
            "tbfk_0031\n",
            "tbfk_0032\n",
            "tbfk_0033\n",
            "tbfk_0034\n",
            "tbfk_0035\n",
            "tbfk_0036\n",
            "tbfk_0037\n",
            "[4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuNOhwUDOqFa",
        "outputId": "bbf8f937-41bb-46ae-b479-bebc0fd89aa6"
      },
      "source": [
        "test_accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1834625322997416"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypvpKvCLGDyx"
      },
      "source": [
        "confusion_matrix(sequence_prediction, actual_prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qz70Ld8oGFbi",
        "outputId": "e2bcf02a-356f-405d-f3cf-97a37050d027"
      },
      "source": [
        "accuracy_score(sequence_prediction, actual_prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8036175710594315"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0I50OjRUIJd",
        "outputId": "4dec94fb-e583-4da7-b4f8-9ba5ee5d04ea"
      },
      "source": [
        "def validation_metrics(model_test, X_test, test_targets):\n",
        "  model_test.eval()\n",
        "  sum_loss_test = 0.0\n",
        "  total_test = 0.0\n",
        "  correct_test = 0.0\n",
        "  for i in range(len(X_test)):\n",
        "    x = X_test[i]\n",
        "    x.resize_(1, 1024)\n",
        "    y_pred = model_test(x)\n",
        "    y = torch.tensor(test_targets[i]).long().resize_((1))\n",
        "    loss = criterion(y_pred, y)\n",
        "    sum_loss_test += loss.item()*y.shape[0]\n",
        "    y_label_pred = torch.max(y_pred, 1)[1]\n",
        "    correct_test += (y_label_pred == y).float().sum()\n",
        "    total_test += y.shape[0]\n",
        "    # print(y, y_label_pred,correct_test, total_test)\n",
        "  return (sum_loss_test/total_test), (correct_test/total_test)\n",
        "\n",
        "for epoch in range(100):\n",
        "  model.train()\n",
        "  sum_loss = 0.0\n",
        "  total = 0.0\n",
        "  correct = 0.0\n",
        "  for i in range(len(X_total)):\n",
        "    x = X_total[i]\n",
        "    x.resize_(1, 1024)\n",
        "    y_pred = model(x)\n",
        "    y = torch.tensor(targets[i]).long().resize_((1))\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(y_pred, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    sum_loss += loss.item()*y.shape[0]\n",
        "    y_label_pred = torch.max(y_pred, 1)[1]\n",
        "    correct += (y_label_pred == y).float().sum()\n",
        "    total += y.shape[0]\n",
        "  val_loss, val_accuracy = validation_metrics(model, X_test, test_targets)\n",
        "  print(\"Epoch %d, train loss %.3f train accuracy %.3f test loss %.3f test accuracy %.3f\" %(epoch, (sum_loss/total), (correct/total), val_loss, val_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, train loss 0.003 train accuracy 1.000 test loss 1.239 test accuracy 0.878\n",
            "Epoch 1, train loss 0.004 train accuracy 0.998 test loss 1.829 test accuracy 0.808\n",
            "Epoch 2, train loss 0.008 train accuracy 0.998 test loss 1.662 test accuracy 0.786\n",
            "Epoch 3, train loss 0.003 train accuracy 1.000 test loss 1.016 test accuracy 0.908\n",
            "Epoch 4, train loss 0.005 train accuracy 0.998 test loss 1.497 test accuracy 0.859\n",
            "Epoch 5, train loss 0.019 train accuracy 0.997 test loss 1.129 test accuracy 0.878\n",
            "Epoch 6, train loss 0.004 train accuracy 0.999 test loss 0.818 test accuracy 0.905\n",
            "Epoch 7, train loss 0.003 train accuracy 0.999 test loss 0.987 test accuracy 0.888\n",
            "Epoch 8, train loss 0.013 train accuracy 0.996 test loss 1.268 test accuracy 0.808\n",
            "Epoch 9, train loss 0.009 train accuracy 0.998 test loss 0.686 test accuracy 0.927\n",
            "Epoch 10, train loss 0.002 train accuracy 1.000 test loss 0.746 test accuracy 0.917\n",
            "Epoch 11, train loss 0.002 train accuracy 1.000 test loss 0.734 test accuracy 0.929\n",
            "Epoch 12, train loss 0.002 train accuracy 1.000 test loss 3.033 test accuracy 0.747\n",
            "Epoch 13, train loss 0.003 train accuracy 0.999 test loss 1.070 test accuracy 0.883\n",
            "Epoch 14, train loss 0.007 train accuracy 0.998 test loss 0.829 test accuracy 0.900\n",
            "Epoch 15, train loss 0.010 train accuracy 0.998 test loss 0.839 test accuracy 0.888\n",
            "Epoch 16, train loss 0.004 train accuracy 0.999 test loss 1.002 test accuracy 0.910\n",
            "Epoch 17, train loss 0.009 train accuracy 0.995 test loss 4.833 test accuracy 0.650\n",
            "Epoch 18, train loss 0.013 train accuracy 0.997 test loss 3.014 test accuracy 0.710\n",
            "Epoch 19, train loss 0.007 train accuracy 0.999 test loss 1.058 test accuracy 0.895\n",
            "Epoch 20, train loss 0.003 train accuracy 0.999 test loss 1.038 test accuracy 0.903\n",
            "Epoch 21, train loss 0.014 train accuracy 0.998 test loss 1.032 test accuracy 0.883\n",
            "Epoch 22, train loss 0.012 train accuracy 0.996 test loss 0.718 test accuracy 0.912\n",
            "Epoch 23, train loss 0.006 train accuracy 0.997 test loss 2.221 test accuracy 0.774\n",
            "Epoch 24, train loss 0.004 train accuracy 0.999 test loss 0.914 test accuracy 0.900\n",
            "Epoch 25, train loss 0.004 train accuracy 0.999 test loss 1.355 test accuracy 0.825\n",
            "Epoch 26, train loss 0.004 train accuracy 0.999 test loss 1.355 test accuracy 0.876\n",
            "Epoch 27, train loss 0.009 train accuracy 0.997 test loss 1.013 test accuracy 0.912\n",
            "Epoch 28, train loss 0.006 train accuracy 0.999 test loss 1.135 test accuracy 0.888\n",
            "Epoch 29, train loss 0.015 train accuracy 0.996 test loss 7.074 test accuracy 0.603\n",
            "Epoch 30, train loss 0.009 train accuracy 0.997 test loss 7.003 test accuracy 0.637\n",
            "Epoch 31, train loss 0.020 train accuracy 0.995 test loss 2.349 test accuracy 0.725\n",
            "Epoch 32, train loss 0.007 train accuracy 0.998 test loss 1.851 test accuracy 0.776\n",
            "Epoch 33, train loss 0.003 train accuracy 1.000 test loss 1.005 test accuracy 0.903\n",
            "Epoch 34, train loss 0.003 train accuracy 1.000 test loss 3.059 test accuracy 0.783\n",
            "Epoch 35, train loss 0.010 train accuracy 0.998 test loss 5.022 test accuracy 0.727\n",
            "Epoch 36, train loss 0.008 train accuracy 0.998 test loss 1.469 test accuracy 0.832\n",
            "Epoch 37, train loss 0.005 train accuracy 0.998 test loss 1.001 test accuracy 0.871\n",
            "Epoch 38, train loss 0.005 train accuracy 0.998 test loss 4.323 test accuracy 0.727\n",
            "Epoch 39, train loss 0.015 train accuracy 0.998 test loss 1.337 test accuracy 0.813\n",
            "Epoch 40, train loss 0.008 train accuracy 0.998 test loss 6.351 test accuracy 0.655\n",
            "Epoch 41, train loss 0.010 train accuracy 0.995 test loss 3.844 test accuracy 0.706\n",
            "Epoch 42, train loss 0.006 train accuracy 0.998 test loss 1.041 test accuracy 0.891\n",
            "Epoch 43, train loss 0.014 train accuracy 0.998 test loss 1.451 test accuracy 0.815\n",
            "Epoch 44, train loss 0.008 train accuracy 0.998 test loss 6.485 test accuracy 0.633\n",
            "Epoch 45, train loss 0.006 train accuracy 0.998 test loss 5.758 test accuracy 0.606\n",
            "Epoch 46, train loss 0.017 train accuracy 0.996 test loss 1.803 test accuracy 0.808\n",
            "Epoch 47, train loss 0.006 train accuracy 0.998 test loss 2.081 test accuracy 0.805\n",
            "Epoch 48, train loss 0.006 train accuracy 0.999 test loss 1.437 test accuracy 0.881\n",
            "Epoch 49, train loss 0.006 train accuracy 0.999 test loss 1.981 test accuracy 0.808\n",
            "Epoch 50, train loss 0.005 train accuracy 0.998 test loss 4.029 test accuracy 0.742\n",
            "Epoch 51, train loss 0.005 train accuracy 0.998 test loss 3.281 test accuracy 0.779\n",
            "Epoch 52, train loss 0.008 train accuracy 0.997 test loss 1.771 test accuracy 0.854\n",
            "Epoch 53, train loss 0.006 train accuracy 0.998 test loss 0.876 test accuracy 0.905\n",
            "Epoch 54, train loss 0.003 train accuracy 1.000 test loss 1.445 test accuracy 0.881\n",
            "Epoch 55, train loss 0.003 train accuracy 0.999 test loss 1.454 test accuracy 0.883\n",
            "Epoch 56, train loss 0.003 train accuracy 0.999 test loss 1.706 test accuracy 0.876\n",
            "Epoch 57, train loss 0.004 train accuracy 0.999 test loss 3.586 test accuracy 0.830\n",
            "Epoch 58, train loss 0.021 train accuracy 0.997 test loss 1.031 test accuracy 0.864\n",
            "Epoch 59, train loss 0.003 train accuracy 0.999 test loss 0.988 test accuracy 0.891\n",
            "Epoch 60, train loss 0.004 train accuracy 0.998 test loss 1.983 test accuracy 0.852\n",
            "Epoch 61, train loss 0.018 train accuracy 0.996 test loss 2.683 test accuracy 0.764\n",
            "Epoch 62, train loss 0.009 train accuracy 0.998 test loss 1.202 test accuracy 0.888\n",
            "Epoch 63, train loss 0.003 train accuracy 0.999 test loss 1.210 test accuracy 0.905\n",
            "Epoch 64, train loss 0.006 train accuracy 0.998 test loss 1.142 test accuracy 0.883\n",
            "Epoch 65, train loss 0.005 train accuracy 0.997 test loss 2.178 test accuracy 0.832\n",
            "Epoch 66, train loss 0.008 train accuracy 0.998 test loss 1.313 test accuracy 0.864\n",
            "Epoch 67, train loss 0.004 train accuracy 0.999 test loss 1.040 test accuracy 0.903\n",
            "Epoch 68, train loss 0.005 train accuracy 0.999 test loss 4.252 test accuracy 0.757\n",
            "Epoch 69, train loss 0.009 train accuracy 0.998 test loss 1.399 test accuracy 0.844\n",
            "Epoch 70, train loss 0.003 train accuracy 0.999 test loss 0.901 test accuracy 0.883\n",
            "Epoch 71, train loss 0.004 train accuracy 1.000 test loss 3.208 test accuracy 0.791\n",
            "Epoch 72, train loss 0.005 train accuracy 0.998 test loss 2.074 test accuracy 0.786\n",
            "Epoch 73, train loss 0.010 train accuracy 0.997 test loss 3.982 test accuracy 0.735\n",
            "Epoch 74, train loss 0.015 train accuracy 0.997 test loss 3.200 test accuracy 0.691\n",
            "Epoch 75, train loss 0.008 train accuracy 0.997 test loss 1.120 test accuracy 0.876\n",
            "Epoch 76, train loss 0.005 train accuracy 0.998 test loss 1.494 test accuracy 0.839\n",
            "Epoch 77, train loss 0.007 train accuracy 0.999 test loss 1.421 test accuracy 0.849\n",
            "Epoch 78, train loss 0.007 train accuracy 0.999 test loss 1.086 test accuracy 0.873\n",
            "Epoch 79, train loss 0.003 train accuracy 0.999 test loss 0.836 test accuracy 0.927\n",
            "Epoch 80, train loss 0.006 train accuracy 0.998 test loss 0.983 test accuracy 0.886\n",
            "Epoch 81, train loss 0.004 train accuracy 0.998 test loss 0.868 test accuracy 0.912\n",
            "Epoch 82, train loss 0.003 train accuracy 0.999 test loss 0.918 test accuracy 0.908\n",
            "Epoch 83, train loss 0.004 train accuracy 0.998 test loss 2.451 test accuracy 0.788\n",
            "Epoch 84, train loss 0.010 train accuracy 0.998 test loss 1.638 test accuracy 0.873\n",
            "Epoch 85, train loss 0.009 train accuracy 0.998 test loss 1.062 test accuracy 0.883\n",
            "Epoch 86, train loss 0.005 train accuracy 0.998 test loss 0.986 test accuracy 0.917\n",
            "Epoch 87, train loss 0.004 train accuracy 0.998 test loss 1.316 test accuracy 0.878\n",
            "Epoch 88, train loss 0.005 train accuracy 0.998 test loss 0.713 test accuracy 0.927\n",
            "Epoch 89, train loss 0.015 train accuracy 0.996 test loss 0.823 test accuracy 0.912\n",
            "Epoch 90, train loss 0.003 train accuracy 0.999 test loss 0.686 test accuracy 0.929\n",
            "Epoch 91, train loss 0.003 train accuracy 1.000 test loss 1.175 test accuracy 0.895\n",
            "Epoch 92, train loss 0.004 train accuracy 0.998 test loss 0.805 test accuracy 0.925\n",
            "Epoch 93, train loss 0.011 train accuracy 0.997 test loss 3.992 test accuracy 0.798\n",
            "Epoch 94, train loss 0.021 train accuracy 0.996 test loss 0.776 test accuracy 0.915\n",
            "Epoch 95, train loss 0.004 train accuracy 1.000 test loss 0.828 test accuracy 0.912\n",
            "Epoch 96, train loss 0.002 train accuracy 1.000 test loss 0.857 test accuracy 0.922\n",
            "Epoch 97, train loss 0.004 train accuracy 0.998 test loss 1.303 test accuracy 0.878\n",
            "Epoch 98, train loss 0.007 train accuracy 0.997 test loss 3.315 test accuracy 0.776\n",
            "Epoch 99, train loss 0.007 train accuracy 0.998 test loss 1.131 test accuracy 0.893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrewQMerLFRw"
      },
      "source": [
        "torch.save(model, 'Baseline_Nest1')\n",
        "\n",
        "# # Model class must be defined somewhere\n",
        "# model = torch.load(PATH)\n",
        "# model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXGgmzSf5sPN",
        "outputId": "fd374eab-a0b2-4020-b8b7-134490ccc829"
      },
      "source": [
        "y_actual_total = []\n",
        "y_pred_total = []\n",
        "for i in range(len(X_test)):\n",
        "    x = X_test[i]\n",
        "    x.resize_(1, 1024)\n",
        "    y_pred = model(x)\n",
        "    y = torch.tensor(test_targets[i]).long().resize_((1))\n",
        "    y_label_pred = torch.max(y_pred, 1)[1]\n",
        "    y_actual_total.append(y.numpy())\n",
        "    y_pred_total.append(y_label_pred.numpy())\n",
        "    print (i, y, y_label_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor([0]) tensor([0])\n",
            "1 tensor([0]) tensor([0])\n",
            "2 tensor([0]) tensor([0])\n",
            "3 tensor([0]) tensor([0])\n",
            "4 tensor([0]) tensor([0])\n",
            "5 tensor([0]) tensor([1])\n",
            "6 tensor([0]) tensor([1])\n",
            "7 tensor([0]) tensor([0])\n",
            "8 tensor([0]) tensor([0])\n",
            "9 tensor([0]) tensor([0])\n",
            "10 tensor([0]) tensor([0])\n",
            "11 tensor([0]) tensor([0])\n",
            "12 tensor([0]) tensor([0])\n",
            "13 tensor([0]) tensor([0])\n",
            "14 tensor([0]) tensor([0])\n",
            "15 tensor([0]) tensor([0])\n",
            "16 tensor([0]) tensor([0])\n",
            "17 tensor([0]) tensor([1])\n",
            "18 tensor([0]) tensor([0])\n",
            "19 tensor([0]) tensor([0])\n",
            "20 tensor([0]) tensor([0])\n",
            "21 tensor([0]) tensor([0])\n",
            "22 tensor([0]) tensor([0])\n",
            "23 tensor([0]) tensor([0])\n",
            "24 tensor([0]) tensor([1])\n",
            "25 tensor([0]) tensor([0])\n",
            "26 tensor([0]) tensor([0])\n",
            "27 tensor([0]) tensor([0])\n",
            "28 tensor([0]) tensor([1])\n",
            "29 tensor([0]) tensor([0])\n",
            "30 tensor([0]) tensor([1])\n",
            "31 tensor([0]) tensor([0])\n",
            "32 tensor([0]) tensor([0])\n",
            "33 tensor([0]) tensor([0])\n",
            "34 tensor([0]) tensor([0])\n",
            "35 tensor([0]) tensor([0])\n",
            "36 tensor([0]) tensor([0])\n",
            "37 tensor([0]) tensor([0])\n",
            "38 tensor([0]) tensor([0])\n",
            "39 tensor([0]) tensor([1])\n",
            "40 tensor([0]) tensor([1])\n",
            "41 tensor([0]) tensor([1])\n",
            "42 tensor([0]) tensor([0])\n",
            "43 tensor([0]) tensor([0])\n",
            "44 tensor([0]) tensor([0])\n",
            "45 tensor([0]) tensor([0])\n",
            "46 tensor([0]) tensor([0])\n",
            "47 tensor([0]) tensor([1])\n",
            "48 tensor([0]) tensor([1])\n",
            "49 tensor([0]) tensor([1])\n",
            "50 tensor([0]) tensor([0])\n",
            "51 tensor([0]) tensor([0])\n",
            "52 tensor([0]) tensor([0])\n",
            "53 tensor([0]) tensor([0])\n",
            "54 tensor([0]) tensor([0])\n",
            "55 tensor([0]) tensor([0])\n",
            "56 tensor([0]) tensor([0])\n",
            "57 tensor([0]) tensor([0])\n",
            "58 tensor([0]) tensor([0])\n",
            "59 tensor([0]) tensor([1])\n",
            "60 tensor([0]) tensor([0])\n",
            "61 tensor([0]) tensor([1])\n",
            "62 tensor([0]) tensor([0])\n",
            "63 tensor([0]) tensor([0])\n",
            "64 tensor([0]) tensor([0])\n",
            "65 tensor([0]) tensor([0])\n",
            "66 tensor([0]) tensor([0])\n",
            "67 tensor([0]) tensor([1])\n",
            "68 tensor([0]) tensor([1])\n",
            "69 tensor([0]) tensor([0])\n",
            "70 tensor([0]) tensor([0])\n",
            "71 tensor([0]) tensor([0])\n",
            "72 tensor([0]) tensor([0])\n",
            "73 tensor([0]) tensor([0])\n",
            "74 tensor([0]) tensor([1])\n",
            "75 tensor([0]) tensor([1])\n",
            "76 tensor([0]) tensor([1])\n",
            "77 tensor([0]) tensor([0])\n",
            "78 tensor([0]) tensor([1])\n",
            "79 tensor([0]) tensor([0])\n",
            "80 tensor([1]) tensor([1])\n",
            "81 tensor([1]) tensor([1])\n",
            "82 tensor([1]) tensor([1])\n",
            "83 tensor([1]) tensor([1])\n",
            "84 tensor([1]) tensor([1])\n",
            "85 tensor([1]) tensor([1])\n",
            "86 tensor([1]) tensor([1])\n",
            "87 tensor([1]) tensor([1])\n",
            "88 tensor([1]) tensor([1])\n",
            "89 tensor([1]) tensor([1])\n",
            "90 tensor([1]) tensor([1])\n",
            "91 tensor([1]) tensor([1])\n",
            "92 tensor([1]) tensor([1])\n",
            "93 tensor([1]) tensor([1])\n",
            "94 tensor([1]) tensor([1])\n",
            "95 tensor([1]) tensor([1])\n",
            "96 tensor([1]) tensor([1])\n",
            "97 tensor([1]) tensor([1])\n",
            "98 tensor([1]) tensor([1])\n",
            "99 tensor([1]) tensor([1])\n",
            "100 tensor([1]) tensor([1])\n",
            "101 tensor([1]) tensor([1])\n",
            "102 tensor([1]) tensor([1])\n",
            "103 tensor([1]) tensor([1])\n",
            "104 tensor([1]) tensor([1])\n",
            "105 tensor([1]) tensor([1])\n",
            "106 tensor([1]) tensor([1])\n",
            "107 tensor([1]) tensor([1])\n",
            "108 tensor([1]) tensor([1])\n",
            "109 tensor([1]) tensor([1])\n",
            "110 tensor([1]) tensor([1])\n",
            "111 tensor([1]) tensor([1])\n",
            "112 tensor([1]) tensor([1])\n",
            "113 tensor([1]) tensor([1])\n",
            "114 tensor([1]) tensor([1])\n",
            "115 tensor([1]) tensor([1])\n",
            "116 tensor([1]) tensor([1])\n",
            "117 tensor([1]) tensor([1])\n",
            "118 tensor([1]) tensor([1])\n",
            "119 tensor([1]) tensor([1])\n",
            "120 tensor([1]) tensor([1])\n",
            "121 tensor([1]) tensor([1])\n",
            "122 tensor([1]) tensor([1])\n",
            "123 tensor([1]) tensor([1])\n",
            "124 tensor([1]) tensor([1])\n",
            "125 tensor([1]) tensor([1])\n",
            "126 tensor([1]) tensor([1])\n",
            "127 tensor([1]) tensor([1])\n",
            "128 tensor([1]) tensor([1])\n",
            "129 tensor([1]) tensor([1])\n",
            "130 tensor([1]) tensor([1])\n",
            "131 tensor([1]) tensor([1])\n",
            "132 tensor([1]) tensor([1])\n",
            "133 tensor([1]) tensor([1])\n",
            "134 tensor([1]) tensor([1])\n",
            "135 tensor([1]) tensor([1])\n",
            "136 tensor([1]) tensor([1])\n",
            "137 tensor([1]) tensor([1])\n",
            "138 tensor([1]) tensor([1])\n",
            "139 tensor([1]) tensor([1])\n",
            "140 tensor([1]) tensor([1])\n",
            "141 tensor([1]) tensor([1])\n",
            "142 tensor([1]) tensor([1])\n",
            "143 tensor([1]) tensor([1])\n",
            "144 tensor([1]) tensor([1])\n",
            "145 tensor([1]) tensor([1])\n",
            "146 tensor([1]) tensor([1])\n",
            "147 tensor([1]) tensor([1])\n",
            "148 tensor([1]) tensor([1])\n",
            "149 tensor([1]) tensor([1])\n",
            "150 tensor([1]) tensor([1])\n",
            "151 tensor([1]) tensor([1])\n",
            "152 tensor([1]) tensor([1])\n",
            "153 tensor([1]) tensor([1])\n",
            "154 tensor([1]) tensor([1])\n",
            "155 tensor([1]) tensor([1])\n",
            "156 tensor([1]) tensor([1])\n",
            "157 tensor([1]) tensor([1])\n",
            "158 tensor([1]) tensor([1])\n",
            "159 tensor([1]) tensor([1])\n",
            "160 tensor([1]) tensor([1])\n",
            "161 tensor([1]) tensor([1])\n",
            "162 tensor([1]) tensor([1])\n",
            "163 tensor([1]) tensor([1])\n",
            "164 tensor([1]) tensor([1])\n",
            "165 tensor([1]) tensor([1])\n",
            "166 tensor([1]) tensor([1])\n",
            "167 tensor([1]) tensor([1])\n",
            "168 tensor([1]) tensor([1])\n",
            "169 tensor([1]) tensor([1])\n",
            "170 tensor([1]) tensor([1])\n",
            "171 tensor([1]) tensor([1])\n",
            "172 tensor([1]) tensor([1])\n",
            "173 tensor([1]) tensor([1])\n",
            "174 tensor([2]) tensor([2])\n",
            "175 tensor([2]) tensor([2])\n",
            "176 tensor([2]) tensor([2])\n",
            "177 tensor([2]) tensor([2])\n",
            "178 tensor([2]) tensor([2])\n",
            "179 tensor([2]) tensor([2])\n",
            "180 tensor([2]) tensor([2])\n",
            "181 tensor([2]) tensor([2])\n",
            "182 tensor([2]) tensor([2])\n",
            "183 tensor([2]) tensor([2])\n",
            "184 tensor([2]) tensor([2])\n",
            "185 tensor([2]) tensor([2])\n",
            "186 tensor([2]) tensor([2])\n",
            "187 tensor([2]) tensor([2])\n",
            "188 tensor([2]) tensor([2])\n",
            "189 tensor([2]) tensor([2])\n",
            "190 tensor([2]) tensor([2])\n",
            "191 tensor([2]) tensor([2])\n",
            "192 tensor([2]) tensor([2])\n",
            "193 tensor([2]) tensor([2])\n",
            "194 tensor([2]) tensor([2])\n",
            "195 tensor([2]) tensor([2])\n",
            "196 tensor([2]) tensor([2])\n",
            "197 tensor([2]) tensor([2])\n",
            "198 tensor([2]) tensor([2])\n",
            "199 tensor([2]) tensor([2])\n",
            "200 tensor([2]) tensor([2])\n",
            "201 tensor([2]) tensor([2])\n",
            "202 tensor([2]) tensor([2])\n",
            "203 tensor([2]) tensor([2])\n",
            "204 tensor([2]) tensor([2])\n",
            "205 tensor([2]) tensor([2])\n",
            "206 tensor([2]) tensor([2])\n",
            "207 tensor([2]) tensor([2])\n",
            "208 tensor([2]) tensor([2])\n",
            "209 tensor([2]) tensor([2])\n",
            "210 tensor([2]) tensor([3])\n",
            "211 tensor([2]) tensor([2])\n",
            "212 tensor([2]) tensor([2])\n",
            "213 tensor([2]) tensor([2])\n",
            "214 tensor([2]) tensor([2])\n",
            "215 tensor([2]) tensor([2])\n",
            "216 tensor([2]) tensor([2])\n",
            "217 tensor([2]) tensor([2])\n",
            "218 tensor([2]) tensor([2])\n",
            "219 tensor([2]) tensor([2])\n",
            "220 tensor([2]) tensor([3])\n",
            "221 tensor([2]) tensor([2])\n",
            "222 tensor([2]) tensor([2])\n",
            "223 tensor([2]) tensor([2])\n",
            "224 tensor([2]) tensor([2])\n",
            "225 tensor([2]) tensor([2])\n",
            "226 tensor([2]) tensor([2])\n",
            "227 tensor([2]) tensor([2])\n",
            "228 tensor([2]) tensor([2])\n",
            "229 tensor([2]) tensor([1])\n",
            "230 tensor([2]) tensor([2])\n",
            "231 tensor([2]) tensor([2])\n",
            "232 tensor([2]) tensor([2])\n",
            "233 tensor([2]) tensor([2])\n",
            "234 tensor([2]) tensor([2])\n",
            "235 tensor([2]) tensor([2])\n",
            "236 tensor([2]) tensor([2])\n",
            "237 tensor([2]) tensor([2])\n",
            "238 tensor([2]) tensor([2])\n",
            "239 tensor([2]) tensor([3])\n",
            "240 tensor([2]) tensor([2])\n",
            "241 tensor([2]) tensor([2])\n",
            "242 tensor([2]) tensor([2])\n",
            "243 tensor([2]) tensor([3])\n",
            "244 tensor([2]) tensor([2])\n",
            "245 tensor([2]) tensor([2])\n",
            "246 tensor([2]) tensor([2])\n",
            "247 tensor([2]) tensor([2])\n",
            "248 tensor([2]) tensor([2])\n",
            "249 tensor([2]) tensor([2])\n",
            "250 tensor([2]) tensor([2])\n",
            "251 tensor([2]) tensor([2])\n",
            "252 tensor([2]) tensor([2])\n",
            "253 tensor([2]) tensor([2])\n",
            "254 tensor([2]) tensor([3])\n",
            "255 tensor([2]) tensor([2])\n",
            "256 tensor([2]) tensor([2])\n",
            "257 tensor([2]) tensor([2])\n",
            "258 tensor([2]) tensor([2])\n",
            "259 tensor([2]) tensor([2])\n",
            "260 tensor([2]) tensor([3])\n",
            "261 tensor([2]) tensor([2])\n",
            "262 tensor([2]) tensor([2])\n",
            "263 tensor([2]) tensor([2])\n",
            "264 tensor([2]) tensor([3])\n",
            "265 tensor([2]) tensor([2])\n",
            "266 tensor([2]) tensor([2])\n",
            "267 tensor([2]) tensor([2])\n",
            "268 tensor([2]) tensor([2])\n",
            "269 tensor([2]) tensor([3])\n",
            "270 tensor([2]) tensor([2])\n",
            "271 tensor([2]) tensor([2])\n",
            "272 tensor([2]) tensor([2])\n",
            "273 tensor([2]) tensor([3])\n",
            "274 tensor([2]) tensor([2])\n",
            "275 tensor([2]) tensor([2])\n",
            "276 tensor([2]) tensor([2])\n",
            "277 tensor([2]) tensor([1])\n",
            "278 tensor([2]) tensor([3])\n",
            "279 tensor([2]) tensor([3])\n",
            "280 tensor([2]) tensor([2])\n",
            "281 tensor([2]) tensor([2])\n",
            "282 tensor([2]) tensor([2])\n",
            "283 tensor([2]) tensor([2])\n",
            "284 tensor([2]) tensor([3])\n",
            "285 tensor([2]) tensor([2])\n",
            "286 tensor([2]) tensor([2])\n",
            "287 tensor([2]) tensor([3])\n",
            "288 tensor([2]) tensor([3])\n",
            "289 tensor([3]) tensor([3])\n",
            "290 tensor([3]) tensor([3])\n",
            "291 tensor([3]) tensor([3])\n",
            "292 tensor([3]) tensor([3])\n",
            "293 tensor([3]) tensor([3])\n",
            "294 tensor([3]) tensor([3])\n",
            "295 tensor([3]) tensor([3])\n",
            "296 tensor([3]) tensor([3])\n",
            "297 tensor([3]) tensor([3])\n",
            "298 tensor([3]) tensor([3])\n",
            "299 tensor([3]) tensor([3])\n",
            "300 tensor([3]) tensor([3])\n",
            "301 tensor([3]) tensor([3])\n",
            "302 tensor([3]) tensor([3])\n",
            "303 tensor([3]) tensor([3])\n",
            "304 tensor([3]) tensor([3])\n",
            "305 tensor([3]) tensor([3])\n",
            "306 tensor([3]) tensor([3])\n",
            "307 tensor([3]) tensor([3])\n",
            "308 tensor([3]) tensor([3])\n",
            "309 tensor([3]) tensor([3])\n",
            "310 tensor([3]) tensor([3])\n",
            "311 tensor([3]) tensor([3])\n",
            "312 tensor([3]) tensor([3])\n",
            "313 tensor([3]) tensor([3])\n",
            "314 tensor([3]) tensor([3])\n",
            "315 tensor([3]) tensor([3])\n",
            "316 tensor([3]) tensor([3])\n",
            "317 tensor([3]) tensor([3])\n",
            "318 tensor([3]) tensor([3])\n",
            "319 tensor([3]) tensor([3])\n",
            "320 tensor([3]) tensor([3])\n",
            "321 tensor([3]) tensor([3])\n",
            "322 tensor([3]) tensor([3])\n",
            "323 tensor([3]) tensor([3])\n",
            "324 tensor([3]) tensor([3])\n",
            "325 tensor([3]) tensor([3])\n",
            "326 tensor([3]) tensor([3])\n",
            "327 tensor([3]) tensor([3])\n",
            "328 tensor([3]) tensor([3])\n",
            "329 tensor([3]) tensor([3])\n",
            "330 tensor([3]) tensor([3])\n",
            "331 tensor([3]) tensor([3])\n",
            "332 tensor([3]) tensor([3])\n",
            "333 tensor([3]) tensor([3])\n",
            "334 tensor([3]) tensor([3])\n",
            "335 tensor([3]) tensor([3])\n",
            "336 tensor([3]) tensor([3])\n",
            "337 tensor([3]) tensor([3])\n",
            "338 tensor([3]) tensor([3])\n",
            "339 tensor([3]) tensor([3])\n",
            "340 tensor([3]) tensor([3])\n",
            "341 tensor([3]) tensor([3])\n",
            "342 tensor([3]) tensor([2])\n",
            "343 tensor([3]) tensor([3])\n",
            "344 tensor([3]) tensor([3])\n",
            "345 tensor([3]) tensor([3])\n",
            "346 tensor([3]) tensor([3])\n",
            "347 tensor([3]) tensor([3])\n",
            "348 tensor([3]) tensor([2])\n",
            "349 tensor([3]) tensor([3])\n",
            "350 tensor([3]) tensor([3])\n",
            "351 tensor([3]) tensor([3])\n",
            "352 tensor([3]) tensor([3])\n",
            "353 tensor([3]) tensor([1])\n",
            "354 tensor([3]) tensor([2])\n",
            "355 tensor([3]) tensor([3])\n",
            "356 tensor([3]) tensor([3])\n",
            "357 tensor([3]) tensor([3])\n",
            "358 tensor([3]) tensor([3])\n",
            "359 tensor([3]) tensor([2])\n",
            "360 tensor([3]) tensor([2])\n",
            "361 tensor([3]) tensor([3])\n",
            "362 tensor([3]) tensor([3])\n",
            "363 tensor([3]) tensor([3])\n",
            "364 tensor([3]) tensor([3])\n",
            "365 tensor([3]) tensor([3])\n",
            "366 tensor([3]) tensor([3])\n",
            "367 tensor([3]) tensor([3])\n",
            "368 tensor([3]) tensor([2])\n",
            "369 tensor([3]) tensor([3])\n",
            "370 tensor([3]) tensor([3])\n",
            "371 tensor([3]) tensor([3])\n",
            "372 tensor([3]) tensor([3])\n",
            "373 tensor([3]) tensor([3])\n",
            "374 tensor([3]) tensor([3])\n",
            "375 tensor([3]) tensor([3])\n",
            "376 tensor([3]) tensor([3])\n",
            "377 tensor([3]) tensor([3])\n",
            "378 tensor([3]) tensor([3])\n",
            "379 tensor([3]) tensor([3])\n",
            "380 tensor([3]) tensor([3])\n",
            "381 tensor([3]) tensor([3])\n",
            "382 tensor([3]) tensor([3])\n",
            "383 tensor([3]) tensor([3])\n",
            "384 tensor([3]) tensor([3])\n",
            "385 tensor([3]) tensor([3])\n",
            "386 tensor([3]) tensor([3])\n",
            "387 tensor([3]) tensor([3])\n",
            "388 tensor([3]) tensor([3])\n",
            "389 tensor([3]) tensor([3])\n",
            "390 tensor([3]) tensor([3])\n",
            "391 tensor([3]) tensor([3])\n",
            "392 tensor([3]) tensor([3])\n",
            "393 tensor([3]) tensor([3])\n",
            "394 tensor([3]) tensor([3])\n",
            "395 tensor([3]) tensor([3])\n",
            "396 tensor([3]) tensor([3])\n",
            "397 tensor([3]) tensor([3])\n",
            "398 tensor([3]) tensor([3])\n",
            "399 tensor([3]) tensor([3])\n",
            "400 tensor([3]) tensor([3])\n",
            "401 tensor([3]) tensor([3])\n",
            "402 tensor([3]) tensor([3])\n",
            "403 tensor([3]) tensor([3])\n",
            "404 tensor([3]) tensor([3])\n",
            "405 tensor([3]) tensor([3])\n",
            "406 tensor([3]) tensor([3])\n",
            "407 tensor([3]) tensor([3])\n",
            "408 tensor([3]) tensor([2])\n",
            "409 tensor([3]) tensor([3])\n",
            "410 tensor([3]) tensor([3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df46NTOnb3Pm"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI_HkKfWcO-c",
        "outputId": "678e91fc-4d10-4ad4-ff47-0439cd59276e"
      },
      "source": [
        "confusion_matrix(y_actual_total, y_pred_total)\n",
        "# Confusion_Matrix.print_stats()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 60,  20,   0,   0],\n",
              "       [  0,  94,   0,   0],\n",
              "       [  0,   2,  99,  14],\n",
              "       [  0,   1,   7, 114]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUDCOhAiL7q5",
        "outputId": "8b0dc842-ff12-4f1b-fcea-4dfef241eedd"
      },
      "source": [
        "sequence_prediction = []\n",
        "actual_prediction = []\n",
        "for key in encoded_targets_test['key'].unique():# [:10]:\n",
        "  print(key)\n",
        "  current_songfile = syllable_df_Nest_Total.loc[syllable_df_Nest_Total['key']==key]\n",
        "  sequence_length = current_songfile['indvi'].values[-1]\n",
        "  sequence_individual_segment = []\n",
        "  if key in test_keys:\n",
        "    for i in range(0, sequence_length):\n",
        "      y_pred = model(torch.tensor(current_songfile['densenet121_features'].values[i]).resize_(1, 1024))\n",
        "      y_label_pred = torch.max(y_pred, 1)[1]\n",
        "      sequence_individual_segment.append(y_label_pred.numpy()[0])\n",
        "    sequence_prediction.append(np.bincount(sequence_individual_segment).argmax())\n",
        "    actual_prediction.append(le.transform(current_songfile['indv'].values)[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cyea_0000\n",
            "cyea_0001\n",
            "cyea_0002\n",
            "cyea_0003\n",
            "cyea_0004\n",
            "cyea_0005\n",
            "cyea_0006\n",
            "cyea_0007\n",
            "cyea_0008\n",
            "cyea_0009\n",
            "cyea_0010\n",
            "cyea_0011\n",
            "cyea_0012\n",
            "cyea_0013\n",
            "cyea_0014\n",
            "cyea_0015\n",
            "cyea_0016\n",
            "cyea_0017\n",
            "cyea_0018\n",
            "cyea_0019\n",
            "cyea_0020\n",
            "cyea_0021\n",
            "cyea_0022\n",
            "cyea_0023\n",
            "cyea_0024\n",
            "cyea_0025\n",
            "cyea_0026\n",
            "cyea_0027\n",
            "cyea_0028\n",
            "cyea_0029\n",
            "cyea_0030\n",
            "cyea_0031\n",
            "cyea_0032\n",
            "cyea_0033\n",
            "cyea_0034\n",
            "cyea_0035\n",
            "cyea_0036\n",
            "cyea_0037\n",
            "cyea_0038\n",
            "cyea_0039\n",
            "cyea_0040\n",
            "hphi_0000\n",
            "hphi_0001\n",
            "hphi_0002\n",
            "hphi_0003\n",
            "hphi_0004\n",
            "hphi_0005\n",
            "hphi_0006\n",
            "hphi_0007\n",
            "hphi_0008\n",
            "hphi_0009\n",
            "hphi_0010\n",
            "hphi_0011\n",
            "hphi_0012\n",
            "hphi_0013\n",
            "hphi_0014\n",
            "hphi_0015\n",
            "hphi_0016\n",
            "hphi_0017\n",
            "hphi_0018\n",
            "hphi_0019\n",
            "hphi_0020\n",
            "hphi_0021\n",
            "hphi_0022\n",
            "hphi_0023\n",
            "hphi_0024\n",
            "hphi_0025\n",
            "hphi_0026\n",
            "hphi_0027\n",
            "hphi_0028\n",
            "hphi_0029\n",
            "hphi_0030\n",
            "hphi_0031\n",
            "hphi_0032\n",
            "hphi_0033\n",
            "hphi_0034\n",
            "hphi_0035\n",
            "hphi_0036\n",
            "hphi_0037\n",
            "hphi_0038\n",
            "hphi_0039\n",
            "hphi_0040\n",
            "hphi_0041\n",
            "hphi_0042\n",
            "hphi_0043\n",
            "ivoj_0000\n",
            "ivoj_0001\n",
            "ivoj_0002\n",
            "ivoj_0003\n",
            "ivoj_0004\n",
            "ivoj_0005\n",
            "ivoj_0006\n",
            "phpd_0000\n",
            "phpd_0001\n",
            "phpd_0002\n",
            "phpd_0003\n",
            "phpd_0004\n",
            "phpd_0005\n",
            "phpd_0006\n",
            "phpd_0007\n",
            "phpd_0008\n",
            "phpd_0009\n",
            "phpd_0010\n",
            "phpd_0011\n",
            "phpd_0012\n",
            "phpd_0013\n",
            "phpd_0014\n",
            "phpd_0015\n",
            "phpd_0016\n",
            "phpd_0017\n",
            "phpd_0018\n",
            "phpd_0019\n",
            "phpd_0020\n",
            "phpd_0021\n",
            "phpd_0022\n",
            "phpd_0023\n",
            "phpd_0024\n",
            "phpd_0025\n",
            "phpd_0026\n",
            "phpd_0027\n",
            "phpd_0028\n",
            "phpd_0029\n",
            "phpd_0030\n",
            "phpd_0031\n",
            "phpd_0032\n",
            "phpd_0033\n",
            "phpd_0034\n",
            "phpd_0035\n",
            "phpd_0036\n",
            "phpd_0037\n",
            "phpd_0038\n",
            "phpd_0039\n",
            "phpd_0040\n",
            "phpd_0041\n",
            "phpd_0042\n",
            "phpd_0043\n",
            "phpd_0044\n",
            "phpd_0045\n",
            "phpd_0046\n",
            "phpd_0047\n",
            "phpd_0048\n",
            "phpd_0049\n",
            "phpd_0050\n",
            "phpd_0051\n",
            "phpd_0052\n",
            "phpd_0053\n",
            "phpd_0054\n",
            "phpd_0055\n",
            "phpd_0056\n",
            "phpd_0057\n",
            "phpd_0058\n",
            "phpd_0059\n",
            "phpd_0060\n",
            "phpd_0061\n",
            "phpd_0062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt_gDsqD0Jpz",
        "outputId": "81821cff-420e-4f93-98a6-51380667e248"
      },
      "source": [
        "print(sequence_prediction, actual_prediction )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6frLJ-oC3ZLB",
        "outputId": "534ea6cd-d422-486a-ae4d-ce1cbfc3c9b0"
      },
      "source": [
        "print(\"Sequence Accuracy :\", sum(1 for x,y in zip(sequence_prediction,actual_prediction) if x == y) / len(actual_prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequence Accuracy : 0.2903225806451613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVzE487JMLmh"
      },
      "source": [
        "current_songfile = syllable_df_Nest_Total.loc[syllable_df_Nest_Total['key']==key]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbJok5YMMOUz"
      },
      "source": [
        "sequence_length = current_songfile['indvi'].values[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iX-vBZ-MPwh",
        "outputId": "bd41df2c-b563-4e09-ea91-1e99b04722cb"
      },
      "source": [
        "sequence_length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P9cAt6w5ghb"
      },
      "source": [
        "sequence_individual_segment = []\n",
        "for i in range(0, sequence_length):\n",
        "  y_pred = model(torch.tensor(current_songfile['densenet121_features'].values[i]).resize_(1, 1024))\n",
        "  y_label_pred = torch.max(y_pred, 1)[1]\n",
        "  sequence_individual_segment.append(y_label_pred.numpy()[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXx1zziiMdy0",
        "outputId": "6bd2bdee-01d3-4884-f2b2-0bab5705383e"
      },
      "source": [
        "sequence_individual_segment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 3, 3, 3, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43M0ODviNjNJ",
        "outputId": "aea2e9a6-5cc6-4c0b-af7e-4cf9980a71ba"
      },
      "source": [
        "y_label_pred.numpy()[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNwQ97l1Nabj",
        "outputId": "3db90dad-34ba-4e35-b7f2-c221da40e95a"
      },
      "source": [
        "np.bincount(sequence_individual_segment).argmax()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "jVrG7E7NMiAp",
        "outputId": "086c5ae6-da73-47b1-d6df-abd8f95a45e5"
      },
      "source": [
        "torch.max(y_pred, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-569ec691e51b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXJG5JDDCgF7"
      },
      "source": [
        "le = LabelEncoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CNNQDMpCk1v",
        "outputId": "493eed06-79d1-4377-ff9b-b42e35697201"
      },
      "source": [
        "le.fit(syllable_df_Nest_Total['indv'].to_list())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAVsMN6cCwQJ"
      },
      "source": [
        "targets = le.transform(syllable_df_Nest_Total['indv'].to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui06X-t2C313",
        "outputId": "f91bee99-f2bb-46a1-f7ed-82fb2e3a7722"
      },
      "source": [
        "targets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 3, 3, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNZ_knrXG6D5",
        "outputId": "3264d745-38db-490b-edb5-d176d5ea1742"
      },
      "source": [
        "list(le.classes_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cyea', 'hphi', 'ivoj', 'phpd']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSdI03XNDB9s"
      },
      "source": [
        "syllable_df_Nest_Total['indv_encoded'] = targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "5slyQftJDHQq",
        "outputId": "f16d6f12-bd0d-4932-8003-38a23d197343"
      },
      "source": [
        "syllable_df_Nest_Total"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>labels</th>\n",
              "      <th>indv</th>\n",
              "      <th>indvi</th>\n",
              "      <th>key</th>\n",
              "      <th>audio</th>\n",
              "      <th>rate</th>\n",
              "      <th>spectrogram</th>\n",
              "      <th>labels_indv</th>\n",
              "      <th>densenet121_features</th>\n",
              "      <th>indv_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.103084</td>\n",
              "      <td>0.133787</td>\n",
              "      <td>-</td>\n",
              "      <td>cyea</td>\n",
              "      <td>1</td>\n",
              "      <td>cyea_0000</td>\n",
              "      <td>[-0.0037507678, -0.0037730576, -0.0033396403, ...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>cyea_-</td>\n",
              "      <td>[0.00036964213, 0.0061388216, 0.0023623325, 0....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.197664</td>\n",
              "      <td>0.342063</td>\n",
              "      <td>a</td>\n",
              "      <td>cyea</td>\n",
              "      <td>2</td>\n",
              "      <td>cyea_0000</td>\n",
              "      <td>[-0.0016009295, -0.00080086064, 0.00024397256,...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>cyea_a</td>\n",
              "      <td>[0.00039546576, 0.0042689457, 0.0021646945, 0....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.369546</td>\n",
              "      <td>0.536848</td>\n",
              "      <td>b</td>\n",
              "      <td>cyea</td>\n",
              "      <td>3</td>\n",
              "      <td>cyea_0000</td>\n",
              "      <td>[0.0028250518, 0.0025573175, 0.002130959, 0.00...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>cyea_b</td>\n",
              "      <td>[0.00043923515, 0.0035873966, 0.0023025582, 0....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.559955</td>\n",
              "      <td>0.687823</td>\n",
              "      <td>c</td>\n",
              "      <td>cyea</td>\n",
              "      <td>4</td>\n",
              "      <td>cyea_0000</td>\n",
              "      <td>[0.00036472746, 0.00013401418, -0.00052369735,...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>cyea_c</td>\n",
              "      <td>[0.0004885812, 0.0055127405, 0.0017675881, 0.0...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.726644</td>\n",
              "      <td>0.832925</td>\n",
              "      <td>d</td>\n",
              "      <td>cyea</td>\n",
              "      <td>5</td>\n",
              "      <td>cyea_0000</td>\n",
              "      <td>[0.0033374317, 0.004109387, 0.003555804, 0.002...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>cyea_d</td>\n",
              "      <td>[0.00040706727, 0.004342597, 0.0018088806, 0.0...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2934</th>\n",
              "      <td>0.102356</td>\n",
              "      <td>0.135123</td>\n",
              "      <td>0</td>\n",
              "      <td>phpd</td>\n",
              "      <td>1</td>\n",
              "      <td>phpd_0062</td>\n",
              "      <td>[-0.0035796678, 0.00066409475, 0.0039508683, 0...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>phpd_0</td>\n",
              "      <td>[0.00035229328, 0.0069653825, 0.00236547, 0.00...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2935</th>\n",
              "      <td>0.228433</td>\n",
              "      <td>0.287435</td>\n",
              "      <td>0</td>\n",
              "      <td>phpd</td>\n",
              "      <td>2</td>\n",
              "      <td>phpd_0062</td>\n",
              "      <td>[0.0010511333, 0.00033233757, -0.00028101055, ...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>phpd_0</td>\n",
              "      <td>[0.00036833377, 0.006149904, 0.0025585291, 0.0...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2936</th>\n",
              "      <td>0.323399</td>\n",
              "      <td>0.392424</td>\n",
              "      <td>0</td>\n",
              "      <td>phpd</td>\n",
              "      <td>3</td>\n",
              "      <td>phpd_0062</td>\n",
              "      <td>[0.0036439702, 0.001985288, 0.00042269952, -0....</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>phpd_0</td>\n",
              "      <td>[0.00036655215, 0.005553442, 0.0027189297, 0.0...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2937</th>\n",
              "      <td>0.426528</td>\n",
              "      <td>0.540746</td>\n",
              "      <td>0</td>\n",
              "      <td>phpd</td>\n",
              "      <td>4</td>\n",
              "      <td>phpd_0062</td>\n",
              "      <td>[0.007070427, 0.0066078743, 0.0063827583, 0.00...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>phpd_0</td>\n",
              "      <td>[0.00043264066, 0.004336921, 0.002199093, 0.00...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2938</th>\n",
              "      <td>0.560996</td>\n",
              "      <td>0.606415</td>\n",
              "      <td>0</td>\n",
              "      <td>phpd</td>\n",
              "      <td>5</td>\n",
              "      <td>phpd_0062</td>\n",
              "      <td>[0.00012365187, 9.9252546e-05, 6.6792077e-06, ...</td>\n",
              "      <td>44100</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>phpd_0</td>\n",
              "      <td>[0.00036569627, 0.0069330796, 0.0024534285, 0....</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2939 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      start_time  ...  indv_encoded\n",
              "0       0.103084  ...             0\n",
              "1       0.197664  ...             0\n",
              "2       0.369546  ...             0\n",
              "3       0.559955  ...             0\n",
              "4       0.726644  ...             0\n",
              "...          ...  ...           ...\n",
              "2934    0.102356  ...             3\n",
              "2935    0.228433  ...             3\n",
              "2936    0.323399  ...             3\n",
              "2937    0.426528  ...             3\n",
              "2938    0.560996  ...             3\n",
              "\n",
              "[2939 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5JgKLYqwQEs"
      },
      "source": [
        "max_sequence_length = syllable_df_Nest_Total['indvi'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yrVOqmPm09Y"
      },
      "source": [
        "def create_sequence_feature(syllable_df_Nest_Total, given_key):\n",
        "    current_songfile = syllable_df_Nest_Total.loc[syllable_df_Nest_Total['key']==given_key]\n",
        "    sequence_length = current_songfile['indvi'].values[-1]\n",
        "    temp_list = current_songfile['densenet121_features'].to_list()\n",
        "    label = current_songfile['indv_encoded'].values[-1]\n",
        "    # for k in range(sequence_length,max_sequence_length):\n",
        "    #   temp_list.append(np.zeros(len(temp_list[0])))\n",
        "    return [temp_list, sequence_length], label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGCWBUpGEo4L"
      },
      "source": [
        "# current_songfile = syllable_df_Nest_Total.loc[syllable_df_Nest_Total['key']=='cyea_0000']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbREeNijRKi9",
        "outputId": "d92d2381-5f4f-4b21-94c6-97bc03869da4"
      },
      "source": [
        "# F.cross_entropy(ypred, ytorch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.2292, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTtmBRetSxv_",
        "outputId": "8deb1288-2fd1-4ae7-d721-f9a47175e7c5"
      },
      "source": [
        "# ypred.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Cp1D5-1S4LM"
      },
      "source": [
        "# ytorch = torch.tensor(y).long().resize_((1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeRlr5YLS9_b",
        "outputId": "48fe6872-f743-4f62-bebc-194073297a51"
      },
      "source": [
        "# ytorch.resize_((1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EOiIebjSzv1",
        "outputId": "3bd3b2c4-b74f-443e-f0e4-6d5e971d81ac"
      },
      "source": [
        "# ytorch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKC1ct11iR5F"
      },
      "source": [
        "Hidden_layer =[]\n",
        "Hidden_layer_labels = []\n",
        "\n",
        "for x, y, l in train_ds:\n",
        "  x = x.float()\n",
        "  x.resize_((1,x.shape[0],x.shape[1]))\n",
        "  lstm_out, (ht, ct) = model_fixed.lstm(x)\n",
        "  Hidden_layer.append(ht[1].detach().numpy().reshape(10, ))\n",
        "  Hidden_layer_labels.append(y)\n",
        "\n",
        "for x, y, l in test_ds:\n",
        "  x = x.float()\n",
        "  x.resize_((1,x.shape[0],x.shape[1]))\n",
        "  lstm_out, (ht, ct) = model_fixed.lstm(x)\n",
        "  Hidden_layer.append(ht[1].detach().numpy().reshape(10, ))\n",
        "  Hidden_layer_labels.append(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7zRwHr3_gJ2",
        "outputId": "8b80e583-2b21-4e78-e7e3-e85a2ded2bcf"
      },
      "source": [
        "Hidden_layer[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.79127705, -0.9375417 , -0.77980804,  0.5639782 ,  0.07360452,\n",
              "        0.8828154 ,  0.0929956 , -0.13487111, -0.08837527,  0.48622584],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5kXdYyPlbB-",
        "outputId": "fec90111-79bb-4081-879e-9c67defd6e59"
      },
      "source": [
        "len(Hidden_layer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "155"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwx-OODw5nIE",
        "outputId": "084f928c-6648-41f0-89b2-3b9c40b3b4e6"
      },
      "source": [
        "len(Hidden_layer_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "155"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgwoLiEAMRzM"
      },
      "source": [
        "current_songfile = syllable_df_Nest_Total.loc[syllable_df_Nest_Total['key']=='cyea_0000']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsm7XG-w1VMV"
      },
      "source": [
        "sequence_length = current_songfile['indvi'].values[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5TV6CGW1cTH"
      },
      "source": [
        "temp_list = current_songfile['densenet121_features'].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5y4BWxQ35Rv",
        "outputId": "7e6be2bf-a4d3-49a6-f88f-c4dc68b86cd8"
      },
      "source": [
        "temp_list.append()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([3.6964213e-04, 6.1388216e-03, 2.3623325e-03, ..., 5.9952104e-01,\n",
              "        4.7096276e-01, 6.7699389e-03], dtype=float32),\n",
              " array([3.9546576e-04, 4.2689457e-03, 2.1646945e-03, ..., 1.0832235e+00,\n",
              "        5.0408190e-01, 8.0454396e-04], dtype=float32),\n",
              " array([4.3923515e-04, 3.5873966e-03, 2.3025582e-03, ..., 4.5419684e-01,\n",
              "        9.9826270e-01, 5.8748936e-03], dtype=float32),\n",
              " array([4.8858119e-04, 5.5127405e-03, 1.7675881e-03, ..., 2.6316303e-01,\n",
              "        6.5507305e-01, 0.0000000e+00], dtype=float32),\n",
              " array([4.0706727e-04, 4.3425970e-03, 1.8088806e-03, ..., 6.8255633e-02,\n",
              "        9.7876644e-01, 0.0000000e+00], dtype=float32),\n",
              " array([3.6909830e-04, 7.3469197e-03, 2.3746886e-03, ..., 5.4688144e-01,\n",
              "        7.2352040e-01, 7.7279314e-02], dtype=float32),\n",
              " array([4.8456003e-04, 3.6201791e-03, 2.1676929e-03, ..., 1.2960277e-01,\n",
              "        9.3385494e-01, 4.3036998e-03], dtype=float32),\n",
              " array([0.00049007, 0.00513859, 0.00200676, ..., 0.47218606, 0.48447147,\n",
              "        0.        ], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwska5uM2vvR"
      },
      "source": [
        "for k in range(sequence_length,20):\n",
        "  temp_list.append(np.zeros(len(temp_list[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmRNB9Yd3TLx",
        "outputId": "d8dd4725-5a56-4ad2-c200-a27985ddfaac"
      },
      "source": [
        "torch.tensor(temp_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3.6964e-04, 6.1388e-03, 2.3623e-03,  ..., 5.9952e-01, 4.7096e-01,\n",
              "         6.7699e-03],\n",
              "        [3.9547e-04, 4.2689e-03, 2.1647e-03,  ..., 1.0832e+00, 5.0408e-01,\n",
              "         8.0454e-04],\n",
              "        [4.3924e-04, 3.5874e-03, 2.3026e-03,  ..., 4.5420e-01, 9.9826e-01,\n",
              "         5.8749e-03],\n",
              "        ...,\n",
              "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "         0.0000e+00],\n",
              "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "         0.0000e+00],\n",
              "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "         0.0000e+00]], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-jzjSqSEzhb"
      },
      "source": [
        "feature_set = []\n",
        "for feat in current_songfile['densenet121_features']:\n",
        "  feature_set.append(create_sequence_feature(feat))\n",
        "# current_songfile['densenet121_features'].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpakzniXE0w7",
        "outputId": "7a4f0fa6-e667-4628-e2bb-2e80ae9f593c"
      },
      "source": [
        "torch.tensor(current_songfile['densenet121_features'].to_list())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3.6964e-04, 6.1388e-03, 2.3623e-03,  ..., 5.9952e-01, 4.7096e-01,\n",
              "         6.7699e-03],\n",
              "        [3.9547e-04, 4.2689e-03, 2.1647e-03,  ..., 1.0832e+00, 5.0408e-01,\n",
              "         8.0454e-04],\n",
              "        [4.3924e-04, 3.5874e-03, 2.3026e-03,  ..., 4.5420e-01, 9.9826e-01,\n",
              "         5.8749e-03],\n",
              "        ...,\n",
              "        [3.6910e-04, 7.3469e-03, 2.3747e-03,  ..., 5.4688e-01, 7.2352e-01,\n",
              "         7.7279e-02],\n",
              "        [4.8456e-04, 3.6202e-03, 2.1677e-03,  ..., 1.2960e-01, 9.3385e-01,\n",
              "         4.3037e-03],\n",
              "        [4.9007e-04, 5.1386e-03, 2.0068e-03,  ..., 4.7219e-01, 4.8447e-01,\n",
              "         0.0000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    }
  ]
}